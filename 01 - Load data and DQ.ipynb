{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade google-cloud-dataplex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "id": "FC0hS7WbJY3k",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758943001403,
          "user_tz": 240,
          "elapsed": 34706,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "fd4b2b5d-d933-4268-f3fb-09ccf02a6429"
      },
      "id": "FC0hS7WbJY3k",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-cloud-dataplex\n",
            "  Downloading google_cloud_dataplex-2.12.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-dataplex) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-dataplex) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-dataplex) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-dataplex) (6.32.1)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-dataplex) (0.14.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-dataplex) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-dataplex) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-dataplex) (1.75.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-dataplex) (1.75.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-dataplex) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-dataplex) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-dataplex) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.11/dist-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-dataplex) (4.14.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-dataplex) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-dataplex) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-dataplex) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-dataplex) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-dataplex) (2025.8.3)\n",
            "Downloading google_cloud_dataplex-2.12.0-py3-none-any.whl (576 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m576.2/576.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-cloud-dataplex\n",
            "Successfully installed google-cloud-dataplex-2.12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "634000ea479547468afc622449893665"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "id": "HnC976mmyOwLab0YMz0K3TPj",
      "metadata": {
        "tags": [],
        "id": "HnC976mmyOwLab0YMz0K3TPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758943935277,
          "user_tz": 240,
          "elapsed": 120683,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "4e84a031-78e8-41c4-994c-68e8464c5368"
      },
      "source": [
        "import csv\n",
        "import io\n",
        "import random\n",
        "from google.cloud import bigquery, storage, dataplex_v1\n",
        "from google.api_core import exceptions\n",
        "from google.api_core import exceptions as dataplex_exceptions\n",
        "\n",
        "# --- Configuration ---\n",
        "# Your GCP Project ID and Dataset (as requested)\n",
        "PROJECT_ID = \"bq-sme-governance-build\"\n",
        "DATASET_ID = \"sme_raw_layer\"\n",
        "\n",
        "# Set a single region. Dataplex DQ scans are not supported in multi-regions like \"US\".\n",
        "LOCATION = \"us-central1\"\n",
        "\n",
        "# GCS Bucket for external table data.\n",
        "# This script will try to create it if it doesn't exist.\n",
        "# !- UPDATE THIS to a globally unique name -!\n",
        "BUCKET_NAME = f\"{PROJECT_ID}-lab-data-source\"\n",
        "# ---------------------\n",
        "\n",
        "# Initialize clients\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "\n",
        "# Global list to track created scans for cleanup\n",
        "CREATED_SCAN_IDS = []\n",
        "\n",
        "\n",
        "# --- Mock Data Generation ---\n",
        "\n",
        "def get_mock_customers():\n",
        "    \"\"\"Generates mock data for the 'customers' table.\"\"\"\n",
        "    return [\n",
        "        {\"customer_id\": \"C1001\", \"first_name\": \"Alice\", \"last_name\": \"Smith\", \"email\": \"alice@example.com\", \"join_date\": \"2023-01-15\"},\n",
        "        {\"customer_id\": \"C1002\", \"first_name\": \"Bob\", \"last_name\": \"Johnson\", \"email\": \"bob@example.com\", \"join_date\": \"2023-02-10\"},\n",
        "        {\"customer_id\": \"C1003\", \"first_name\": \"Charlie\", \"last_name\": \"Brown\", \"email\": \"charlie@example.com\", \"join_date\": \"2023-03-05\"},\n",
        "        {\"customer_id\": \"C1004\", \"first_name\": \"David\", \"last_name\": \"Lee\", \"email\": \"david@example.com\", \"join_date\": \"2023-04-20\"},\n",
        "        {\"customer_id\": \"C1005\", \"first_name\": \"Eve\", \"last_name\": \"Davis\", \"email\": \"eve@example.com\", \"join_date\": \"2023-05-15\"},\n",
        "    ]\n",
        "\n",
        "def get_mock_products():\n",
        "    \"\"\"Generates mock data for the 'products' table.\"\"\"\n",
        "    return [\n",
        "        {\"product_id\": \"P2001\", \"product_name\": \"Laptop\", \"category\": \"Electronics\", \"unit_price\": 1200.00},\n",
        "        {\"product_id\": \"P2002\", \"product_name\": \"Mouse\", \"category\": \"Electronics\", \"unit_price\": 25.50},\n",
        "        {\"product_id\": \"P2003\", \"product_name\": \"Coffee Mug\", \"category\": \"Homeware\", \"unit_price\": 15.00},\n",
        "        {\"product_id\": \"P2004\", \"product_name\": \"Notebook\", \"category\": \"Stationery\", \"unit_price\": 5.75},\n",
        "        {\"product_id\": \"P9999\", \"product_name\": \"Test Item\", \"category\": \"UNKNOWN\", \"unit_price\": -1.00},\n",
        "    ]\n",
        "\n",
        "def get_mock_orders():\n",
        "    \"\"\"Generates mock data for 'orders' as a list of dicts.\"\"\"\n",
        "    return [\n",
        "        {\"order_id\": \"E101\", \"customer_id\": \"C1001\", \"order_date\": \"2024-05-01\", \"status\": \"Shipped\"},\n",
        "        {\"order_id\": \"E102\", \"customer_id\": \"C1002\", \"order_date\": \"2024-05-03\", \"status\": \"Processing\"},\n",
        "        {\"order_id\": \"E103\", \"customer_id\": \"C1001\", \"order_date\": \"2024-05-04\", \"status\": \"Shipped\"},\n",
        "        {\"order_id\": \"E104\", \"customer_id\": \"C1003\", \"order_date\": \"2024-05-05\", \"status\": \"Delivered\"},\n",
        "        {\"order_id\": \"E105\", \"customer_id\": \"C1004\", \"order_date\": \"2024-05-06\", \"status\": \"Shipped\"},\n",
        "        {\"order_id\": \"E106\", \"customer_id\": \"C9999\", \"order_date\": \"2024-05-07\", \"status\": \"Pending\"},\n",
        "    ]\n",
        "\n",
        "\n",
        "def get_mock_order_items_csv():\n",
        "    \"\"\"Generates mock data for 'order_items' as a CSV string.\"\"\"\n",
        "    data = [\n",
        "        [\"item_id\", \"order_id\", \"product_id\", \"quantity\"],\n",
        "        [\"OI301\", \"E101\", \"P2001\", 1],\n",
        "        [\"OI302\", \"E101\", \"P2002\", 1],\n",
        "        [\"OI303\", \"E102\", \"P2003\", 2],\n",
        "        [\"OI304\", \"E103\", \"P2004\", 5],\n",
        "        [\"OI305\", \"E104\", \"P2001\", 1],\n",
        "        [\"OI306\", \"E105\", \"P2003\", 1],\n",
        "        [\"OI307\", \"E106\", \"P9999\", 99],\n",
        "    ]\n",
        "    output = io.StringIO()\n",
        "    writer = csv.writer(output)\n",
        "    writer.writerows(data)\n",
        "    return output.getvalue()\n",
        "\n",
        "# --- Cloud Resource Setup ---\n",
        "\n",
        "def ensure_gcs_bucket_exists():\n",
        "    \"\"\"Checks for GCS bucket and creates it if not found.\"\"\"\n",
        "    print(f\"Checking for GCS bucket: {BUCKET_NAME}...\")\n",
        "    try:\n",
        "        bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "        print(\"...bucket already exists.\")\n",
        "    except exceptions.NotFound:\n",
        "        print(\"...bucket not found, creating new bucket.\")\n",
        "        bucket = storage_client.create_bucket(BUCKET_NAME, location=LOCATION)\n",
        "        print(f\"...created bucket {bucket.name} in {bucket.location}\")\n",
        "    return bucket\n",
        "\n",
        "def ensure_bq_dataset_exists():\n",
        "    \"\"\"Checks for BQ dataset and creates it if not found.\"\"\"\n",
        "    dataset_ref = bq_client.dataset(DATASET_ID)\n",
        "    print(f\"Checking for BigQuery dataset: {DATASET_ID}...\")\n",
        "    try:\n",
        "        bq_client.get_dataset(dataset_ref)\n",
        "        print(\"...dataset already exists.\")\n",
        "    except exceptions.NotFound:\n",
        "        print(\"...dataset not found, creating new dataset.\")\n",
        "        dataset = bigquery.Dataset(dataset_ref)\n",
        "        dataset.location = LOCATION\n",
        "        bq_client.create_dataset(dataset, timeout=30)\n",
        "        print(\"...created dataset.\")\n",
        "\n",
        "def upload_to_gcs(bucket, blob_name, data_string):\n",
        "    \"\"\"Uploads a string as a file to GCS.\"\"\"\n",
        "    print(f\"Uploading {blob_name} to GCS bucket {bucket.name}...\")\n",
        "    blob = bucket.blob(blob_name)\n",
        "    blob.upload_from_string(data_string, content_type=\"text/csv\")\n",
        "    print(\"...upload complete.\")\n",
        "    return f\"gs://{bucket.name}/{blob_name}\"\n",
        "\n",
        "# --- BigQuery Table Creation ---\n",
        "\n",
        "def load_table_from_memory(table_id, data, schema):\n",
        "    \"\"\"Loads data from a list of dicts into a new BQ table.\"\"\"\n",
        "    full_table_id = f\"{PROJECT_ID}.{DATASET_ID}.{table_id}\"\n",
        "    print(f\"Starting load job for table: {full_table_id}...\")\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "    )\n",
        "    try:\n",
        "        load_job = bq_client.load_table_from_json(\n",
        "            data, full_table_id, job_config=job_config\n",
        "        )\n",
        "        load_job.result()\n",
        "        print(\"...load job finished.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading table {full_table_id}: {e}\")\n",
        "\n",
        "def load_iceberg_table_from_memory(table_id, data, schema):\n",
        "    \"\"\"Creates a BQ-managed Iceberg table and loads data from memory.\"\"\"\n",
        "    full_table_id = f\"{PROJECT_ID}.{DATASET_ID}.{table_id}\"\n",
        "    print(f\"Creating and loading BQ-managed Iceberg table: {full_table_id}...\")\n",
        "    try:\n",
        "        bq_client.delete_table(full_table_id, not_found_ok=True)\n",
        "        print(\"...deleted existing table (if any).\")\n",
        "        table = bigquery.Table(full_table_id, schema=schema)\n",
        "        table.table_format = \"ICEBERG\"\n",
        "        bq_client.create_table(table)\n",
        "        print(\"...Iceberg table definition created.\")\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            schema=schema,\n",
        "            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
        "        )\n",
        "        load_job = bq_client.load_table_from_json(\n",
        "            data, full_table_id, job_config=job_config\n",
        "        )\n",
        "        load_job.result()\n",
        "        print(\"...load job finished.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating or loading Iceberg table {full_table_id}: {e}\")\n",
        "\n",
        "\n",
        "def create_external_table(table_id, schema, gcs_uri):\n",
        "    \"\"\"Creates a new BQ external table pointing to a GCS file.\"\"\"\n",
        "    full_table_id = f\"{PROJECT_ID}.{DATASET_ID}.{table_id}\"\n",
        "    print(f\"Creating external table: {full_table_id}...\")\n",
        "    external_config = bigquery.ExternalConfig(\"CSV\")\n",
        "    external_config.source_uris = [gcs_uri]\n",
        "    external_config.schema = schema\n",
        "    external_config.csv_options.skip_leading_rows = 1\n",
        "    table = bigquery.Table(full_table_id)\n",
        "    table.external_data_configuration = external_config\n",
        "    try:\n",
        "        bq_client.delete_table(full_table_id, not_found_ok=True)\n",
        "        print(f\"...deleted existing table (if any).\")\n",
        "        bq_client.create_table(table)\n",
        "        print(\"...external table created.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating external table {full_table_id}: {e}\")\n",
        "\n",
        "# --- Dataplex Data Quality Functions ---\n",
        "\n",
        "def create_data_quality_scan(project_id, table_name, rules):\n",
        "    \"\"\"Creates and runs a Dataplex AutoDQ scan for a given table.\"\"\"\n",
        "    clean_table_name = table_name.replace('_', '-')\n",
        "    scan_id = f\"dq-{clean_table_name}-{random.randint(1000, 9999)}\"\n",
        "\n",
        "    # CORRECTED: The resource string must be fully qualified with the service name.\n",
        "    table_resource_string = f\"//bigquery.googleapis.com/projects/{project_id}/datasets/{DATASET_ID}/tables/{table_name}\"\n",
        "\n",
        "    print(f\"\\nCreating Dataplex DQ scan for table: {table_name}...\")\n",
        "\n",
        "    try:\n",
        "        dq_client = dataplex_v1.DataScanServiceClient()\n",
        "        parent = f\"projects/{project_id}/locations/{LOCATION}\"\n",
        "\n",
        "        data_scan = dataplex_v1.types.DataScan(\n",
        "            data={\"resource\": table_resource_string},\n",
        "            data_quality_spec={\"rules\": rules},\n",
        "            execution_spec={\n",
        "                \"trigger\": {\"on_demand\": {}}\n",
        "            },\n",
        "        )\n",
        "\n",
        "        request = dataplex_v1.CreateDataScanRequest(\n",
        "            parent=parent,\n",
        "            data_scan=data_scan,\n",
        "            data_scan_id=scan_id,\n",
        "        )\n",
        "\n",
        "        operation = dq_client.create_data_scan(request=request)\n",
        "        result = operation.result()\n",
        "\n",
        "        print(f\"...Successfully created DQ scan: {result.name}\")\n",
        "        CREATED_SCAN_IDS.append(result.name)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!!! An error occurred creating the DQ scan for {table_name}: {e}\")\n",
        "        print(\"Please ensure the Dataplex API is enabled in your project.\")\n",
        "\n",
        "\n",
        "def get_customers_dq_rules():\n",
        "    \"\"\"Returns a list of appropriate DQ rules for the 'customers' table.\"\"\"\n",
        "    return [\n",
        "        dataplex_v1.types.DataQualityRule(\n",
        "            column=\"customer_id\",\n",
        "            non_null_expectation={},\n",
        "            dimension=\"VALIDITY\",\n",
        "            description=\"Customer ID must not be empty.\"\n",
        "        ),\n",
        "        dataplex_v1.types.DataQualityRule(\n",
        "            column=\"customer_id\",\n",
        "            uniqueness_expectation={},\n",
        "            dimension=\"UNIQUENESS\",\n",
        "            description=\"Each Customer ID must be unique.\"\n",
        "        ),\n",
        "        dataplex_v1.types.DataQualityRule(\n",
        "            column=\"email\",\n",
        "            regex_expectation={\"regex\": r\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"},\n",
        "            dimension=\"VALIDITY\",\n",
        "            description=\"Email must be in a valid format.\"\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "def get_products_dq_rules():\n",
        "    \"\"\"Returns a list of appropriate DQ rules for the 'products' table.\"\"\"\n",
        "    return [\n",
        "        dataplex_v1.types.DataQualityRule(\n",
        "            column=\"product_id\",\n",
        "            non_null_expectation={},\n",
        "            dimension=\"VALIDITY\",\n",
        "            description=\"Product ID must not be empty.\"\n",
        "        ),\n",
        "        dataplex_v1.types.DataQualityRule(\n",
        "            column=\"unit_price\",\n",
        "            range_expectation={\"min_value\": \"0.01\"},\n",
        "            dimension=\"VALIDITY\",\n",
        "            description=\"Unit price must be a positive value.\"\n",
        "        ),\n",
        "        dataplex_v1.types.DataQualityRule(\n",
        "            column=\"category\",\n",
        "            set_expectation={\"values\": [\"Electronics\", \"Homeware\", \"Stationery\"]},\n",
        "            dimension=\"VALIDITY\",\n",
        "            description=\"Category must be one of the allowed values.\"\n",
        "        )\n",
        "    ]\n",
        "\n",
        "def get_orders_dq_rules():\n",
        "    \"\"\"Returns a list of appropriate DQ rules for the 'orders' table.\"\"\"\n",
        "    customers_table_fqn = f\"`{PROJECT_ID}.{DATASET_ID}.customers`\"\n",
        "    return [\n",
        "        dataplex_v1.types.DataQualityRule(\n",
        "            column=\"order_id\",\n",
        "            uniqueness_expectation={},\n",
        "            dimension=\"UNIQUENESS\",\n",
        "            description=\"Each Order ID must be unique.\"\n",
        "        ),\n",
        "        dataplex_v1.types.DataQualityRule(\n",
        "            row_condition_expectation={\n",
        "                \"sql_expression\": f\"customer_id IS NULL OR customer_id IN (SELECT customer_id FROM {customers_table_fqn})\"\n",
        "            },\n",
        "            dimension=\"CONSISTENCY\",\n",
        "            description=\"Customer ID must exist in the customers table.\"\n",
        "        )\n",
        "    ]\n",
        "\n",
        "def get_order_items_dq_rules():\n",
        "    \"\"\"Returns a list of appropriate DQ rules for the 'order_items' table.\"\"\"\n",
        "    orders_table_fqn = f\"`{PROJECT_ID}.{DATASET_ID}.orders`\"\n",
        "    products_table_fqn = f\"`{PROJECT_ID}.{DATASET_ID}.products`\"\n",
        "    return [\n",
        "        dataplex_v1.types.DataQualityRule(\n",
        "            column=\"quantity\",\n",
        "            range_expectation={\"min_value\": \"1\"},\n",
        "            dimension=\"VALIDITY\",\n",
        "            description=\"Quantity must be at least 1.\"\n",
        "        ),\n",
        "        dataplex_v1.types.DataQualityRule(\n",
        "            row_condition_expectation={\n",
        "                 \"sql_expression\": f\"order_id IS NULL OR order_id IN (SELECT order_id FROM {orders_table_fqn})\"\n",
        "            },\n",
        "            dimension=\"CONSISTENCY\",\n",
        "            description=\"Order ID must exist in the orders table.\"\n",
        "        ),\n",
        "        dataplex_v1.types.DataQualityRule(\n",
        "            row_condition_expectation={\n",
        "                 \"sql_expression\": f\"product_id IS NULL OR product_id IN (SELECT product_id FROM {products_table_fqn})\"\n",
        "            },\n",
        "            dimension=\"CONSISTENCY\",\n",
        "            description=\"Product ID must exist in the products table.\"\n",
        "        )\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(f\"Starting data setup for project: {PROJECT_ID}\\n\")\n",
        "\n",
        "    try:\n",
        "        bucket = ensure_gcs_bucket_exists()\n",
        "        ensure_bq_dataset_exists()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create cloud resources: {e}\")\n",
        "        print(\"Please check your permissions and project configuration.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- Step 1: Handling Internal Tables (Load Jobs) ---\")\n",
        "\n",
        "    customers_schema = [\n",
        "        bigquery.SchemaField(\"customer_id\", \"STRING\", \"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"first_name\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"last_name\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"email\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"join_date\", \"DATE\"),\n",
        "    ]\n",
        "    customers_data = get_mock_customers()\n",
        "    load_table_from_memory(\"customers\", customers_data, customers_schema)\n",
        "\n",
        "    products_schema = [\n",
        "        bigquery.SchemaField(\"product_id\", \"STRING\", \"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"product_name\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"category\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"unit_price\", \"FLOAT\"),\n",
        "    ]\n",
        "    products_data = get_mock_products()\n",
        "    load_table_from_memory(\"products\", products_data, products_schema)\n",
        "\n",
        "    orders_schema = [\n",
        "        bigquery.SchemaField(\"order_id\", \"STRING\", \"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"customer_id\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"order_date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"status\", \"STRING\"),\n",
        "    ]\n",
        "    orders_data = get_mock_orders()\n",
        "    load_iceberg_table_from_memory(\"orders\", orders_data, orders_schema)\n",
        "\n",
        "    print(\"\\n--- Step 2: Handling External Tables (GCS) ---\")\n",
        "\n",
        "    order_items_csv_data = get_mock_order_items_csv()\n",
        "    order_items_gcs_uri = upload_to_gcs(bucket, \"raw/order_items/order_items.csv\", order_items_csv_data)\n",
        "\n",
        "    order_items_schema = [\n",
        "        bigquery.SchemaField(\"item_id\", \"STRING\", \"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"order_id\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"product_id\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"quantity\", \"INTEGER\"),\n",
        "    ]\n",
        "    create_external_table(\"order_items\", order_items_schema, order_items_gcs_uri)\n",
        "\n",
        "    print(\"\\n--- Step 3: Creating Dataplex Data Quality Scans ---\")\n",
        "\n",
        "    create_data_quality_scan(PROJECT_ID, \"customers\", get_customers_dq_rules())\n",
        "    create_data_quality_scan(PROJECT_ID, \"products\", get_products_dq_rules())\n",
        "    create_data_quality_scan(PROJECT_ID, \"orders\", get_orders_dq_rules())\n",
        "    create_data_quality_scan(PROJECT_ID, \"order_items\", get_order_items_dq_rules())\n",
        "\n",
        "    print(\"\\n--- Setup Complete! ---\")\n",
        "    print(f\"Project: {PROJECT_ID}\")\n",
        "    print(f\"Dataset: {DATASET_ID}\")\n",
        "    print(\"Tables created:\")\n",
        "    print(\" - customers (Standard BQ Table)\")\n",
        "    print(\" - products (Standard BQ Table)\")\n",
        "    print(\" - orders (BQ-Managed Iceberg Table)\")\n",
        "    print(\" - order_items (External CSV Table)\")\n",
        "    print(\"Dataplex DQ Scans created for all tables.\")\n",
        "\n",
        "\n",
        "\n",
        "def cleanup_dataplex_scans():\n",
        "    \"\"\"Deletes all Dataplex scans created by this script.\"\"\"\n",
        "    print(\"\\nAttempting to delete Dataplex Data Quality Scans...\")\n",
        "    if not CREATED_SCAN_IDS:\n",
        "        print(\"...no scans were created, skipping.\")\n",
        "        return\n",
        "\n",
        "    dq_client = dataplex_v1.DataScanServiceClient()\n",
        "    deleted_count = 0\n",
        "    for scan_name in CREATED_SCAN_IDS:\n",
        "        print(f\"...deleting scan: {scan_name}\")\n",
        "        try:\n",
        "            request = dataplex_v1.DeleteDataScanRequest(name=scan_name)\n",
        "            operation = dq_client.delete_data_scan(request=request)\n",
        "            operation.result()\n",
        "            print(\"...scan deleted successfully.\")\n",
        "            deleted_count += 1\n",
        "        except dataplex_exceptions.NotFound:\n",
        "            print(\"...scan not found, may have already been deleted.\")\n",
        "        except Exception as e:\n",
        "            print(f\"!!! Error deleting scan {scan_name}: {e}\")\n",
        "    print(f\"...deleted {deleted_count} Dataplex scans.\")\n",
        "\n",
        "\n",
        "def cleanup_resources():\n",
        "    \"\"\"\n",
        "    Deletes all created resources for a clean teardown.\n",
        "    WARNING: This is destructive and irreversible.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- STARTING RESOURCE CLEANUP ---\")\n",
        "\n",
        "    cleanup_dataplex_scans()\n",
        "\n",
        "    print(f\"\\nAttempting to delete GCS Bucket: {BUCKET_NAME}...\")\n",
        "    try:\n",
        "        bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "        blobs = list(bucket.list_blobs())\n",
        "        for blob in blobs:\n",
        "            blob.delete()\n",
        "        bucket.delete()\n",
        "        print(f\"...bucket {BUCKET_NAME} deleted successfully.\")\n",
        "    except exceptions.NotFound:\n",
        "        print(f\"...bucket {BUCKET_NAME} not found, skipping.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting bucket {BUCKET_NAME}: {e}\")\n",
        "\n",
        "    print(f\"\\nAttempting to delete BigQuery Dataset: {DATASET_ID}...\")\n",
        "    try:\n",
        "        bq_client.delete_dataset(\n",
        "            DATASET_ID, delete_contents=True, not_found_ok=True\n",
        "        )\n",
        "        print(f\"...dataset {DATASET_ID} deleted successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting dataset {DATASET_ID}: {e}\")\n",
        "\n",
        "    print(\"\\n--- CLEANUP COMPLETE ---\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    # Uncomment the line below to automatically delete all created resources after the script runs.\n",
        "    #cleanup_resources()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data setup for project: bq-sme-governance-build\n",
            "\n",
            "Checking for GCS bucket: bq-sme-governance-build-lab-data-source...\n",
            "...bucket not found, creating new bucket.\n",
            "...created bucket bq-sme-governance-build-lab-data-source in US-CENTRAL1\n",
            "Checking for BigQuery dataset: sme_raw_layer...\n",
            "...dataset not found, creating new dataset.\n",
            "...created dataset.\n",
            "\n",
            "--- Step 1: Handling Internal Tables (Load Jobs) ---\n",
            "Starting load job for table: bq-sme-governance-build.sme_raw_layer.customers...\n",
            "...load job finished.\n",
            "Starting load job for table: bq-sme-governance-build.sme_raw_layer.products...\n",
            "...load job finished.\n",
            "Creating and loading BQ-managed Iceberg table: bq-sme-governance-build.sme_raw_layer.orders...\n",
            "...deleted existing table (if any).\n",
            "...Iceberg table definition created.\n",
            "...load job finished.\n",
            "\n",
            "--- Step 2: Handling External Tables (GCS) ---\n",
            "Uploading raw/order_items/order_items.csv to GCS bucket bq-sme-governance-build-lab-data-source...\n",
            "...upload complete.\n",
            "Creating external table: bq-sme-governance-build.sme_raw_layer.order_items...\n",
            "...deleted existing table (if any).\n",
            "...external table created.\n",
            "\n",
            "--- Step 3: Creating Dataplex Data Quality Scans ---\n",
            "\n",
            "Creating Dataplex DQ scan for table: customers...\n",
            "...Successfully created DQ scan: projects/bq-sme-governance-build/locations/us-central1/dataScans/dq-customers-2124\n",
            "\n",
            "Creating Dataplex DQ scan for table: products...\n",
            "...Successfully created DQ scan: projects/bq-sme-governance-build/locations/us-central1/dataScans/dq-products-1311\n",
            "\n",
            "Creating Dataplex DQ scan for table: orders...\n",
            "...Successfully created DQ scan: projects/bq-sme-governance-build/locations/us-central1/dataScans/dq-orders-2712\n",
            "\n",
            "Creating Dataplex DQ scan for table: order_items...\n",
            "...Successfully created DQ scan: projects/bq-sme-governance-build/locations/us-central1/dataScans/dq-order-items-1768\n",
            "\n",
            "--- Setup Complete! ---\n",
            "Project: bq-sme-governance-build\n",
            "Dataset: sme_raw_layer\n",
            "Tables created:\n",
            " - customers (Standard BQ Table)\n",
            " - products (Standard BQ Table)\n",
            " - orders (BQ-Managed Iceberg Table)\n",
            " - order_items (External CSV Table)\n",
            "Dataplex DQ Scans created for all tables.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "02-Lineage and DQ"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}