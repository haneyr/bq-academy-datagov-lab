{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataplex Data Quality and BigQuery ETL Demo\n",
    "\n",
    "This notebook demonstrates a common data engineering workflow on Google Cloud. It covers:\n",
    "\n",
    "1.  **Infrastructure Setup**: Creating a Google Cloud Storage (GCS) bucket and a BigQuery dataset.\n",
    "2.  **Data Ingestion**: Loading raw data into BigQuery from both in-memory sources (managed tables) and GCS (external tables).\n",
    "3.  **Data Quality**: Defining and creating Dataplex Data Quality scans to profile and validate the raw data.\n",
    "4.  **ETL/ELT**: Transforming the raw data into a simple star-schema dimensional model.\n",
    "5.  **Cleanup**: Providing a utility to tear down all created resources.\n",
    "\n",
    "The data used is a mock dataset representing customers, products, and sales orders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Configuration\n",
    "\n",
    "First, we'll import the necessary libraries, configure our project variables, and initialize the Google Cloud clients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication (If running locally)\n",
    "If you are running this notebook from a local environment (not a GCP Vertex AI Notebook), you will need to authenticate. Uncomment and run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "import random\n",
    "import time\n",
    "\n",
    "from google.cloud import bigquery, storage, dataplex_v1\n",
    "from google.api_core import exceptions\n",
    "from google.api_core import exceptions as dataplex_exceptions\n",
    "\n",
    "# ---  Configuration  ---\n",
    "# TODO: Update these values with your project details\n",
    "PROJECT_ID = \"bq-sme-governance-build\" # Your Google Cloud project ID\n",
    "LOCATION = \"us-central1\"             # The region for your resources\n",
    "DATASET_ID = \"sme_raw_layer\"         # The BigQuery dataset to create\n",
    "\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-lab-data-source\"\n",
    "# ---------------------------\n",
    "\n",
    "# Initialize clients\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "dataplex_client = dataplex_v1.DataScanServiceClient()\n",
    "\n",
    "# Global list to track created scans for later execution and cleanup\n",
    "CREATED_SCAN_NAMES = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Helper Functions\n",
    "\n",
    "Here we define all the helper functions that will be used throughout the notebook. They are grouped by purpose: data generation, resource management, data loading, data quality, and cleanup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation Functions\n",
    "These functions generate mock data for our tables. Note that some data is intentionally malformed to demonstrate the effectiveness of the data quality scans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mock_customers():\n",
    "    \"\"\"Generates mock data for the 'customers' table.\"\"\"\n",
    "    return [\n",
    "        {\"customer_id\": \"C1001\", \"first_name\": \"Alice\", \"last_name\": \"Smith\", \"email\": \"alice@example.com\", \"join_date\": \"2023-01-15\"},\n",
    "        {\"customer_id\": \"C1002\", \"first_name\": \"Bob\", \"last_name\": \"Johnson\", \"email\": \"bob@example.com\", \"join_date\": \"2023-02-10\"},\n",
    "        {\"customer_id\": \"C1003\", \"first_name\": \"Charlie\", \"last_name\": \"Brown\", \"email\": \"charlie@example.com\", \"join_date\": \"2023-03-05\"},\n",
    "        {\"customer_id\": \"C1004\", \"first_name\": \"David\", \"last_name\": \"Lee\", \"email\": \"david@example.com\", \"join_date\": \"2023-04-20\"},\n",
    "        {\"customer_id\": \"C1005\", \"first_name\": \"Eve\", \"last_name\": \"Davis\", \"email\": \"eve@example.com\", \"join_date\": \"2023-05-15\"},\n",
    "    ]\n",
    "\n",
    "def get_mock_products():\n",
    "    \"\"\"Generates mock data for the 'products' table.\"\"\"\n",
    "    return [\n",
    "        {\"product_id\": \"P2001\", \"product_name\": \"Laptop\", \"category\": \"Electronics\", \"unit_price\": 1200.00},\n",
    "        {\"product_id\": \"P2002\", \"product_name\": \"Mouse\", \"category\": \"Electronics\", \"unit_price\": 25.50},\n",
    "        {\"product_id\": \"P2003\", \"product_name\": \"Coffee Mug\", \"category\": \"Homeware\", \"unit_price\": 15.00},\n",
    "        {\"product_id\": \"P2004\", \"product_name\": \"Notebook\", \"category\": \"Stationery\", \"unit_price\": 5.75},\n",
    "        {\"product_id\": \"P9999\", \"product_name\": \"Test Item\", \"category\": \"\", \"unit_price\": -1.00}, # Intentional bad data\n",
    "    ]\n",
    "\n",
    "def get_mock_orders():\n",
    "    \"\"\"Generates mock data for 'orders' as a list of dicts.\"\"\"\n",
    "    return [\n",
    "        {\"order_id\": \"E101\", \"customer_id\": \"C1001\", \"order_date\": \"2024-05-01\", \"status\": \"Shipped\"},\n",
    "        {\"order_id\": \"E102\", \"customer_id\": \"C1002\", \"order_date\": \"2024-05-03\", \"status\": \"Processing\"},\n",
    "        {\"order_id\": \"E103\", \"customer_id\": \"C1001\", \"order_date\": \"2024-05-04\", \"status\": \"Shipped\"},\n",
    "        {\"order_id\": \"E104\", \"customer_id\": \"C1003\", \"order_date\": \"2024-05-05\", \"status\": \"Delivered\"},\n",
    "        {\"order_id\": \"E105\", \"customer_id\": \"C1004\", \"order_date\": \"2024-05-06\", \"status\": \"Shipped\"},\n",
    "        {\"order_id\": \"E106\", \"customer_id\": \"C9999\", \"order_date\": \"2024-05-07\", \"status\": \"Pending\"}, # Intentional bad data (FK violation)\n",
    "    ]\n",
    "\n",
    "def get_mock_order_items_csv():\n",
    "    \"\"\"Generates mock data for 'order_items' as a CSV string.\"\"\"\n",
    "    data = [\n",
    "        [\"item_id\", \"order_id\", \"product_id\", \"quantity\"],\n",
    "        [\"OI301\", \"E101\", \"P2001\", 1],\n",
    "        [\"OI302\", \"E101\", \"P2002\", 1],\n",
    "        [\"OI303\", \"E102\", \"P2003\", 2],\n",
    "        [\"OI304\", \"E103\", \"P2004\", 5],\n",
    "        [\"OI305\", \"E104\", \"P2001\", 1],\n",
    "        [\"OI306\", \"E105\", \"P2003\", 1],\n",
    "        [\"OI307\", \"E106\", \"P9999\", 99],\n",
    "    ]\n",
    "    output = io.StringIO()\n",
    "    writer = csv.writer(output)\n",
    "    writer.writerows(data)\n",
    "    return output.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_gcs_bucket_exists():\n",
    "    \"\"\"Checks for GCS bucket and creates it if not found.\"\"\"\n",
    "    print(f\"Checking for GCS bucket: {BUCKET_NAME}...\")\n",
    "    try:\n",
    "        bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "        print(\"...bucket already exists.\")\n",
    "    except exceptions.NotFound:\n",
    "        print(\"...bucket not found, creating new bucket.\")\n",
    "        bucket = storage_client.create_bucket(BUCKET_NAME, location=LOCATION)\n",
    "        print(f\"...created bucket {bucket.name} in {bucket.location}\")\n",
    "    return bucket\n",
    "\n",
    "def ensure_bq_dataset_exists():\n",
    "    \"\"\"Checks for BQ dataset and creates it if not found.\"\"\"\n",
    "    dataset_ref = bq_client.dataset(DATASET_ID)\n",
    "    print(f\"Checking for BigQuery dataset: {DATASET_ID}...\")\n",
    "    try:\n",
    "        bq_client.get_dataset(dataset_ref)\n",
    "        print(\"...dataset already exists.\")\n",
    "    except exceptions.NotFound:\n",
    "        print(\"...dataset not found, creating new dataset.\")\n",
    "        dataset = bigquery.Dataset(dataset_ref)\n",
    "        dataset.location = LOCATION\n",
    "        bq_client.create_dataset(dataset, timeout=30)\n",
    "        print(\"...created dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BigQuery Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(bucket, blob_name, data_string):\n",
    "    \"\"\"Uploads a string as a file to GCS.\"\"\"\n",
    "    print(f\"Uploading {blob_name} to GCS bucket {bucket.name}...\")\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_string(data_string, content_type=\"text/csv\")\n",
    "    print(\"...upload complete.\")\n",
    "    return f\"gs://{bucket.name}/{blob_name}\"\n",
    "\n",
    "def load_table_from_memory(table_id, data, schema):\n",
    "    \"\"\"Loads data from a list of dicts into a new BQ table.\"\"\"\n",
    "    full_table_id = f\"{PROJECT_ID}.{DATASET_ID}.{table_id}\"\n",
    "    print(f\"Starting load job for table: {full_table_id}...\")\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=schema,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "    )\n",
    "    try:\n",
    "        load_job = bq_client.load_table_from_json(\n",
    "            data, full_table_id, job_config=job_config\n",
    "        )\n",
    "        load_job.result()\n",
    "        print(\"...load job finished.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading table {full_table_id}: {e}\")\n",
    "\n",
    "def create_external_table(table_id, schema, gcs_uri):\n",
    "    \"\"\"Creates a new BQ external table pointing to a GCS file.\"\"\"\n",
    "    full_table_id = f\"{PROJECT_ID}.{DATASET_ID}.{table_id}\"\n",
    "    print(f\"Creating external table: {full_table_id}...\")\n",
    "    external_config = bigquery.ExternalConfig(\"CSV\")\n",
    "    external_config.source_uris = [gcs_uri]\n",
    "    external_config.schema = schema\n",
    "    external_config.csv_options.skip_leading_rows = 1\n",
    "    table = bigquery.Table(full_table_id)\n",
    "    table.external_data_configuration = external_config\n",
    "    try:\n",
    "        bq_client.delete_table(full_table_id, not_found_ok=True)\n",
    "        bq_client.create_table(table)\n",
    "        print(\"...external table created.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating external table {full_table_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataplex Data Quality Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_customers_dq_rules():\n",
    "    \"\"\"Returns a list of appropriate DQ rules for the 'customers' table.\"\"\"\n",
    "    return [\n",
    "        {\"column\": \"customer_id\", \"non_null_expectation\": {}, \"dimension\": \"VALIDITY\", \"description\": \"Customer ID must not be empty.\"},\n",
    "        {\"column\": \"customer_id\", \"uniqueness_expectation\": {}, \"dimension\": \"UNIQUENESS\", \"description\": \"Each Customer ID must be unique.\"},\n",
    "        {\"column\": \"email\", \"regex_expectation\": {\"regex\": r\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"}, \"dimension\": \"VALIDITY\", \"description\": \"Email must be in a valid format.\"},\n",
    "    ]\n",
    "\n",
    "def get_products_dq_rules():\n",
    "    \"\"\"Returns a list of appropriate DQ rules for the 'products' table.\"\"\"\n",
    "    return [\n",
    "        {\"column\": \"product_id\", \"non_null_expectation\": {}, \"dimension\": \"VALIDITY\", \"description\": \"Product ID must not be empty.\"},\n",
    "        {\"column\": \"unit_price\", \"range_expectation\": {\"min_value\": \"0.01\"}, \"dimension\": \"VALIDITY\", \"description\": \"Unit price must be a positive value.\"},\n",
    "        {\"column\": \"category\", \"set_expectation\": {\"values\": [\"Electronics\", \"Homeware\", \"Stationery\"]}, \"dimension\": \"VALIDITY\", \"description\": \"Category must be one of the allowed values.\"},\n",
    "    ]\n",
    "\n",
    "def get_orders_dq_rules():\n",
    "    \"\"\"Returns a list of appropriate DQ rules for the 'orders' table.\"\"\"\n",
    "    customers_table_fqn = f\"`{PROJECT_ID}.{DATASET_ID}.customers`\"\n",
    "    return [\n",
    "        {\"column\": \"order_id\", \"uniqueness_expectation\": {}, \"dimension\": \"UNIQUENESS\", \"description\": \"Each Order ID must be unique.\"},\n",
    "        {\"row_condition_expectation\": {\"sql_expression\": f\"customer_id IS NULL OR customer_id IN (SELECT customer_id FROM {customers_table_fqn})\"}, \"dimension\": \"CONSISTENCY\", \"description\": \"Customer ID must exist in the customers table.\"},\n",
    "    ]\n",
    "\n",
    "def get_order_items_dq_rules():\n",
    "    \"\"\"Returns a list of appropriate DQ rules for the 'order_items' table.\"\"\"\n",
    "    orders_table_fqn = f\"`{PROJECT_ID}.{DATASET_ID}.orders`\"\n",
    "    products_table_fqn = f\"`{PROJECT_ID}.{DATASET_ID}.products`\"\n",
    "    return [\n",
    "        {\"column\": \"quantity\", \"range_expectation\": {\"min_value\": \"1\"}, \"dimension\": \"VALIDITY\", \"description\": \"Quantity must be at least 1.\"},\n",
    "        {\"row_condition_expectation\": {\"sql_expression\": f\"order_id IS NULL OR order_id IN (SELECT order_id FROM {orders_table_fqn})\"}, \"dimension\": \"CONSISTENCY\", \"description\": \"Order ID must exist in the orders table.\"},\n",
    "        {\"row_condition_expectation\": {\"sql_expression\": f\"product_id IS NULL OR product_id IN (SELECT product_id FROM {products_table_fqn})\"}, \"dimension\": \"CONSISTENCY\", \"description\": \"Product ID must exist in the products table.\"},\n",
    "    ]\n",
    "\n",
    "def create_data_quality_scan(project_id, table_name, rules):\n",
    "    \"\"\"Creates a Dataplex DQ scan for a given BigQuery table.\"\"\"\n",
    "    clean_table_name = table_name.replace('_', '-')\n",
    "    scan_id = f\"dq-{clean_table_name}-{random.randint(1000, 9999)}\"\n",
    "    table_resource_string = f\"//bigquery.googleapis.com/projects/{project_id}/datasets/{DATASET_ID}/tables/{table_name}\"\n",
    "    \n",
    "    print(f\"\\nCreating Dataplex DQ scan '{scan_id}' for table: {table_name}...\")\n",
    "\n",
    "    try:\n",
    "        parent = f\"projects/{project_id}/locations/{LOCATION}\"\n",
    "\n",
    "        data_scan = dataplex_v1.types.DataScan(\n",
    "            data={\"resource\": table_resource_string},\n",
    "            data_quality_spec={\"rules\": rules},\n",
    "            execution_spec={\"trigger\": {\"on_demand\": {}}},\n",
    "        )\n",
    "\n",
    "        request = dataplex_v1.CreateDataScanRequest(\n",
    "            parent=parent,\n",
    "            data_scan=data_scan,\n",
    "            data_scan_id=scan_id,\n",
    "        )\n",
    "\n",
    "        operation = dataplex_client.create_data_scan(request=request)\n",
    "        result = operation.result()\n",
    "\n",
    "        print(f\"...Successfully created DQ scan: {result.name}\")\n",
    "        CREATED_SCAN_NAMES.append(result.name) # Store the full name for running/cleanup\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"!!! An error occurred creating the DQ scan for {table_name}: {e}\")\n",
    "\n",
    "def run_data_quality_scans():\n",
    "    \"\"\"Runs all the Dataplex DQ scans created by this script.\"\"\"\n",
    "    if not CREATED_SCAN_NAMES:\n",
    "        print(\"...no scans were created, skipping.\")\n",
    "        return\n",
    "\n",
    "    for scan_name in CREATED_SCAN_NAMES:\n",
    "        print(f\"...triggering scan: {scan_name.split('/')[-1]}\")\n",
    "        try:\n",
    "            request = dataplex_v1.RunDataScanRequest(name=scan_name)\n",
    "            response = dataplex_client.run_data_scan(request=request)\n",
    "            print(f\"  -> Scan run initiated. Job name: {response.job.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"!!! Error running scan {scan_name}: {e}\")\n",
    "    print(\"\\nScans are running in the background. It may take a few minutes for results to appear in the Cloud Console.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation (ETL) Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dimensional_tables():\n",
    "    \"\"\"Creates the dimension and fact tables from the base tables.\"\"\"\n",
    "    # dim_customers\n",
    "    print(\"Creating dim_customers...\")\n",
    "    sql = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.dim_customers` AS\n",
    "    SELECT\n",
    "      customer_id AS customer_key,\n",
    "      first_name,\n",
    "      last_name,\n",
    "      email,\n",
    "      join_date\n",
    "    FROM\n",
    "      `{PROJECT_ID}.{DATASET_ID}.customers`\n",
    "    \"\"\"\n",
    "    bq_client.query(sql).result()\n",
    "\n",
    "    # dim_products\n",
    "    print(\"Creating dim_products...\")\n",
    "    sql = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.dim_products` AS\n",
    "    SELECT\n",
    "      product_id AS product_key,\n",
    "      product_name,\n",
    "      category,\n",
    "      unit_price\n",
    "    FROM\n",
    "      `{PROJECT_ID}.{DATASET_ID}.products`\n",
    "    \"\"\"\n",
    "    bq_client.query(sql).result()\n",
    "\n",
    "    # dim_date\n",
    "    print(\"Creating dim_date...\")\n",
    "    sql = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.dim_date` AS\n",
    "    SELECT\n",
    "      order_date AS date_key,\n",
    "      EXTRACT(YEAR FROM order_date) AS year,\n",
    "      EXTRACT(MONTH FROM order_date) AS month,\n",
    "      EXTRACT(DAY FROM order_date) AS day,\n",
    "      EXTRACT(DAYOFWEEK FROM order_date) AS day_of_week\n",
    "    FROM (\n",
    "      SELECT DISTINCT order_date FROM `{PROJECT_ID}.{DATASET_ID}.orders`\n",
    "    )\n",
    "    \"\"\"\n",
    "    bq_client.query(sql).result()\n",
    "\n",
    "    # fct_sales\n",
    "    print(\"Creating fct_sales...\")\n",
    "    sql = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.fct_sales` AS\n",
    "    SELECT\n",
    "      oi.item_id AS sales_key,\n",
    "      o.order_id,\n",
    "      o.customer_id AS customer_key,\n",
    "      oi.product_id AS product_key,\n",
    "      o.order_date AS order_date_key,\n",
    "      oi.quantity,\n",
    "      p.unit_price,\n",
    "      oi.quantity * p.unit_price AS total_price\n",
    "    FROM\n",
    "      `{PROJECT_ID}.{DATASET_ID}.order_items` AS oi\n",
    "    JOIN\n",
    "      `{PROJECT_ID}.{DATASET_ID}.orders` AS o ON oi.order_id = o.order_id\n",
    "    JOIN\n",
    "      `{PROJECT_ID}.{DATASET_ID}.products` AS p ON oi.product_id = p.product_id\n",
    "    \"\"\"\n",
    "    bq_client.query(sql).result()\n",
    "\n",
    "    print(\"...dimensional tables created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_dataplex_scans():\n",
    "    \"\"\"Deletes all Dataplex scans created by this script.\"\"\"\n",
    "    print(\"\\nAttempting to delete Dataplex Data Quality Scans...\")\n",
    "    if not CREATED_SCAN_NAMES:\n",
    "        print(\"...no scans were created, skipping.\")\n",
    "        return\n",
    "\n",
    "    deleted_count = 0\n",
    "    for scan_name in CREATED_SCAN_NAMES:\n",
    "        print(f\"...deleting scan: {scan_name.split('/')[-1]}\")\n",
    "        try:\n",
    "            request = dataplex_v1.DeleteDataScanRequest(name=scan_name)\n",
    "            operation = dataplex_client.delete_data_scan(request=request)\n",
    "            operation.result(timeout=120)\n",
    "            print(\"  -> Scan deleted successfully.\")\n",
    "            deleted_count += 1\n",
    "        except dataplex_exceptions.NotFound:\n",
    "            print(\"  -> Scan not found, may have already been deleted.\")\n",
    "        except Exception as e:\n",
    "            print(f\"!!! Error deleting scan {scan_name}: {e}\")\n",
    "    print(f\"...deleted {deleted_count} Dataplex scans.\")\n",
    "\n",
    "def cleanup_resources():\n",
    "    \"\"\"Deletes all created resources for a clean teardown.\"\"\"\n",
    "    print(\"\\n--- STARTING RESOURCE CLEANUP ---\")\n",
    "    cleanup_dataplex_scans()\n",
    "    print(f\"\\nAttempting to delete GCS Bucket: {BUCKET_NAME}...\")\n",
    "    try:\n",
    "        bucket = storage_client.bucket(BUCKET_NAME)\n",
    "        bucket.delete(force=True)\n",
    "        print(f\"...bucket {BUCKET_NAME} deleted successfully.\")\n",
    "    except exceptions.NotFound:\n",
    "        print(f\"...bucket {BUCKET_NAME} not found, skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting bucket {BUCKET_NAME}: {e}\")\n",
    "\n",
    "    print(f\"\\nAttempting to delete BigQuery Dataset: {DATASET_ID}...\")\n",
    "    try:\n",
    "        bq_client.delete_dataset(DATASET_ID, delete_contents=True, not_found_ok=True)\n",
    "        print(f\"...dataset {DATASET_ID} deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting dataset {DATASET_ID}: {e}\")\n",
    "    print(\"\\n--- CLEANUP COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution Workflow\n",
    "\n",
    "Now we will execute the helper functions in a logical sequence. You can run the cells one by one to see the process unfold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create GCS and BigQuery Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting data setup for project: {PROJECT_ID}\\n\")\n",
    "try:\n",
    "    bucket = ensure_gcs_bucket_exists()\n",
    "    ensure_bq_dataset_exists()\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create cloud resources: {e}\\nCheck permissions and config.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Internal (Managed) Tables\n",
    "We load `customers`, `products`, and `orders` data directly from memory into standard BigQuery tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Handling Internal Tables (Load Jobs) ---\")\n",
    "customers_schema = [bigquery.SchemaField(\"customer_id\", \"STRING\"), bigquery.SchemaField(\"first_name\", \"STRING\"), bigquery.SchemaField(\"last_name\", \"STRING\"), bigquery.SchemaField(\"email\", \"STRING\"), bigquery.SchemaField(\"join_date\", \"DATE\")]\n",
    "load_table_from_memory(\"customers\", get_mock_customers(), customers_schema)\n",
    "\n",
    "products_schema = [bigquery.SchemaField(\"product_id\", \"STRING\"), bigquery.SchemaField(\"product_name\", \"STRING\"), bigquery.SchemaField(\"category\", \"STRING\"), bigquery.SchemaField(\"unit_price\", \"FLOAT64\")]\n",
    "load_table_from_memory(\"products\", get_mock_products(), products_schema)\n",
    "\n",
    "orders_schema = [bigquery.SchemaField(\"order_id\", \"STRING\"), bigquery.SchemaField(\"customer_id\", \"STRING\"), bigquery.SchemaField(\"order_date\", \"DATE\"), bigquery.SchemaField(\"status\", \"STRING\")]\n",
    "load_table_from_memory(\"orders\", get_mock_orders(), orders_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load External Table\n",
    "For the `order_items` table, we first upload the data as a CSV to our GCS bucket and then create an external BigQuery table that reads directly from that GCS file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Handling External Tables (GCS) ---\")\n",
    "order_items_gcs_uri = upload_to_gcs(bucket, \"raw/order_items/order_items.csv\", get_mock_order_items_csv())\n",
    "order_items_schema = [bigquery.SchemaField(\"item_id\", \"STRING\"), bigquery.SchemaField(\"order_id\", \"STRING\"), bigquery.SchemaField(\"product_id\", \"STRING\"), bigquery.SchemaField(\"quantity\", \"INTEGER\")]\n",
    "create_external_table(\"order_items\", order_items_schema, order_items_gcs_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create and Run Dataplex Data Quality Scans\n",
    "\n",
    "Now that our raw data tables exist, we can define and create on-demand Dataplex scans to check for issues. After creating the scans, we trigger them to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Creating Dataplex Data Quality Scans ---\")\n",
    "create_data_quality_scan(PROJECT_ID, \"customers\", get_customers_dq_rules())\n",
    "create_data_quality_scan(PROJECT_ID, \"products\", get_products_dq_rules())\n",
    "create_data_quality_scan(PROJECT_ID, \"orders\", get_orders_dq_rules())\n",
    "create_data_quality_scan(PROJECT_ID, \"order_items\", get_order_items_dq_rules())\n",
    "\n",
    "# Wait a moment for scan creation to propagate before running\n",
    "print(\"\\nWaiting 5 seconds before triggering scans...\")\n",
    "time.sleep(5)\n",
    "\n",
    "print(\"\\n--- Running Dataplex Data Quality Scans ---\")\n",
    "run_data_quality_scans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create Dimensional Model\n",
    "Finally, we run SQL queries to transform our raw tables into a star schema consisting of dimension tables (`dim_customers`, `dim_products`, `dim_date`) and a fact table (`fct_sales`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Creating Dimensional Tables ---\")\n",
    "create_dimensional_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Setup Complete ---\")\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Dataset: {DATASET_ID}\")\n",
    "print(\"Resources created:\")\n",
    "print(\" - Tables: customers, products, orders, order_items, dim_customers, dim_products, dim_date, fct_sales\")\n",
    "print(\" - Dataplex DQ Scans for all raw tables.\")\n",
    "print(\" - GCS Bucket for external data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resource Cleanup\n",
    "\n",
    "**Warning:** The following cell is destructive. It will delete all the resources created by this notebook, including the GCS bucket (and its contents), the BigQuery dataset (and all its tables), and the Dataplex data scans.\n",
    "\n",
    "Run this cell only when you are finished and want to clean up your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To automatically delete all created resources, uncomment and run the line below.\n",
    "\n",
    "#cleanup_resources()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
