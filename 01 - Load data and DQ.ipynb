{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "xqsmbci203c",
   "source": [
    "# BigQuery Data Pipeline: Bronze → Silver → Gold with Data Quality Issues\n",
    "\n",
    "This notebook demonstrates a complete data pipeline with intentional data quality issues that flow through all layers for detection by Dataplex Data Quality.\n",
    "\n",
    "## Pipeline Stages\n",
    "\n",
    "**1. Bronze Layer**: Raw data with injected quality issues\n",
    "   - Native BigQuery table (customers)\n",
    "   - BigLake external table with CSV (products)\n",
    "   - BigLake Iceberg table (orders)\n",
    "   - Issues injected: duplicates, nulls, inconsistent casing, future dates\n",
    "\n",
    "**2. Silver Layer**: Structured data with issues preserved\n",
    "   - Parse JSON fields\n",
    "   - Convert data types\n",
    "   - Does NOT clean quality issues - they flow forward\n",
    "\n",
    "**3. Gold Layer**: Dimensional model with issues still present\n",
    "   - Fact table: fct_sales\n",
    "   - Dimensions: dim_customers, dim_products, dim_date\n",
    "   - Wide view: vw_sales_wide (One Big Table pattern)\n",
    "   - All quality issues preserved for Dataplex detection\n",
    "\n",
    "## Data Quality Issues Included\n",
    "\n",
    "This pipeline intentionally includes:\n",
    "1. Duplicate records - customer_id = 20 appears twice\n",
    "2. NULL values - ~5% of customers have NULL emails\n",
    "3. Inconsistent naming - Categories with mixed case ('Electronics' vs 'electronics')\n",
    "4. Invalid dates - ~5% of orders have future timestamps\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Run this notebook to create Bronze → Silver → Gold with quality issues\n",
    "2. Use Dataplex Data Quality to detect issues in Gold layer tables\n",
    "3. Trace issues back from Gold → Silver → Bronze\n",
    "4. Implement fixes in Silver layer based on Dataplex findings\n",
    "\n",
    "## How to Use\n",
    "\n",
    "1. Run the installation cell to install required packages\n",
    "2. Configure parameters using the form fields in the configuration cell (in Google Colab, click the cell to reveal the form)\n",
    "3. Run all subsequent cells in order\n",
    "4. Observe quality issues in the query results\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "At the end of the notebook, you'll find a cleanup cell to delete all created resources."
   ],
   "metadata": {
    "id": "xqsmbci203c"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! pip install --upgrade google-cloud-bigquery google-cloud-storage google-cloud-dataplex faker"
   ],
   "metadata": {
    "id": "tZVBDV8w2uEs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759982318293,
     "user_tz": 240,
     "elapsed": 4500,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "12cbd80c-d3c4-4fef-8387-5e5f64b72b48"
   },
   "id": "tZVBDV8w2uEs",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "90w99svjsan",
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "The following cells handle authentication and configuration. If running in Google Colab, uncomment the authentication cell."
   ],
   "metadata": {
    "id": "90w99svjsan"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#from google.colab import auth\n",
    "#auth.authenticate_user()"
   ],
   "metadata": {
    "id": "Zd1SMPthTh6J",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759982318293,
     "user_tz": 240,
     "elapsed": 3,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "Zd1SMPthTh6J",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict\n",
    "import os\n",
    "import subprocess\n",
    "from google.cloud import bigquery, storage\n",
    "from google.api_core.exceptions import NotFound, Conflict\n",
    "\n",
    "# --- Configuration ---\n",
    "# @title Pipeline Configuration { display-mode: \"form\" }\n",
    "\n",
    "# GCP Settings\n",
    "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"your-gcp-project-id\") #@param {type:\"string\"}\n",
    "REGION = \"us-central1\" #@param [\"us-central1\", \"us-east1\", \"us-west1\", \"europe-west1\", \"asia-southeast1\"]\n",
    "BIGQUERY_CONNECTION_ID = \"gcs-biglake-connection\" #@param {type:\"string\"}\n",
    "\n",
    "# Dataset Names\n",
    "BRONZE_DATASET = \"bronze\" #@param {type:\"string\"}\n",
    "SILVER_DATASET = \"silver\" #@param {type:\"string\"}\n",
    "GOLD_DATASET = \"gold\" #@param {type:\"string\"}\n",
    "\n",
    "# Data Generation Parameters\n",
    "NUM_CUSTOMERS = 200 #@param {type:\"slider\", min:10, max:1000, step:10}\n",
    "NUM_PRODUCTS = 50 #@param {type:\"slider\", min:10, max:500, step:10}\n",
    "NUM_ORDERS = 500 #@param {type:\"slider\", min:50, max:5000, step:50}\n",
    "MAX_ITEMS_PER_ORDER = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
    "\n",
    "# Auto-generated\n",
    "BIGQUERY_DATASET = BRONZE_DATASET\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-lab-data-source\"\n",
    "CONNECTION_NAME = f\"{PROJECT_ID}.{REGION}.{BIGQUERY_CONNECTION_ID}\"\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"   Project: {PROJECT_ID}\")\n",
    "print(f\"   Region: {REGION}\")\n",
    "print(f\"   Datasets: {BRONZE_DATASET}, {SILVER_DATASET}, {GOLD_DATASET}\")\n",
    "print(f\"   Data: {NUM_CUSTOMERS} customers, {NUM_PRODUCTS} products, {NUM_ORDERS} orders\")"
   ],
   "metadata": {
    "id": "OF-kSzMlTls1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759982563539,
     "user_tz": 240,
     "elapsed": 153,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "e2a9f935-b671-48e7-f4f2-dfc400e635e0"
   },
   "id": "OF-kSzMlTls1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper, generation, and cleanup functions\n",
    "\n",
    "Utility functions for infrastructure setup and data generation."
   ],
   "metadata": {
    "id": "BKn6PhO4GMFk"
   },
   "id": "BKn6PhO4GMFk"
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Helper Functions ---\n",
    "\n",
    "def validate_config():\n",
    "    \"\"\"Validates that required configuration is set.\"\"\"\n",
    "    if PROJECT_ID == \"your-gcp-project-id\":\n",
    "        raise ValueError(\"Please update the PROJECT_ID variable before running.\")\n",
    "\n",
    "def ensure_dataset(bq_client: bigquery.Client, dataset_name: str) -> str:\n",
    "    \"\"\"Creates a BigQuery dataset if it doesn't exist.\"\"\"\n",
    "    dataset_id = f\"{PROJECT_ID}.{dataset_name}\"\n",
    "    dataset = bigquery.Dataset(dataset_id)\n",
    "    dataset.location = REGION\n",
    "    bq_client.create_dataset(dataset, exists_ok=True)\n",
    "    print(f\"Ensured dataset '{dataset_id}' exists in location '{REGION}'.\")\n",
    "    return dataset_id\n",
    "\n",
    "def setup_gcs_bucket(storage_client: storage.Client) -> storage.Bucket:\n",
    "    \"\"\"Checks for a GCS bucket and creates it if it doesn't exist.\"\"\"\n",
    "    print(f\"--- Checking for GCS Bucket: {BUCKET_NAME} ---\")\n",
    "    try:\n",
    "        bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "        print(f\"Bucket '{bucket.name}' already exists.\")\n",
    "    except NotFound:\n",
    "        print(f\"Bucket '{BUCKET_NAME}' not found. Creating it...\")\n",
    "        bucket = storage_client.create_bucket(BUCKET_NAME, location=REGION)\n",
    "        print(f\"Bucket '{bucket.name}' created successfully in region {REGION}.\")\n",
    "    return bucket\n",
    "\n",
    "def setup_bigquery_connection(bq_client: bigquery.Client, storage_client: storage.Client):\n",
    "    \"\"\"Creates a BigQuery external connection and grants GCS permissions.\"\"\"\n",
    "    print(f\"--- Setting up BigQuery Connection: {CONNECTION_NAME} ---\")\n",
    "\n",
    "    full_connection_id = f\"{PROJECT_ID}.{REGION}.{BIGQUERY_CONNECTION_ID}\"\n",
    "\n",
    "    # Check if connection exists\n",
    "    show_command = [\"bq\", \"show\", \"--connection\", full_connection_id]\n",
    "    result = subprocess.run(show_command, capture_output=True, text=True)\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        print(\"Connection not found. Creating it...\")\n",
    "        create_command = [\n",
    "            \"bq\", \"mk\", \"--connection\",\n",
    "            f\"--location={REGION}\",\n",
    "            f\"--project_id={PROJECT_ID}\",\n",
    "            \"--connection_type=CLOUD_RESOURCE\",\n",
    "            BIGQUERY_CONNECTION_ID\n",
    "        ]\n",
    "        subprocess.run(create_command, capture_output=True, text=True, check=True)\n",
    "        print(\"Connection created successfully.\")\n",
    "    else:\n",
    "        print(\"BigQuery connection already exists.\")\n",
    "\n",
    "    # Get service account and grant permissions\n",
    "    show_json_command = [\"bq\", \"show\", \"--connection\", \"--format=json\", full_connection_id]\n",
    "    result = subprocess.run(show_json_command, capture_output=True, text=True, check=True)\n",
    "    connection_info = json.loads(result.stdout)\n",
    "    service_account = connection_info[\"cloudResource\"][\"serviceAccountId\"]\n",
    "    print(f\"Found service account: {service_account}\")\n",
    "\n",
    "    # Grant IAM permissions\n",
    "    bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "    policy = bucket.get_iam_policy(requested_policy_version=3)\n",
    "    role = \"roles/storage.admin\"\n",
    "    member = f\"serviceAccount:{service_account}\"\n",
    "\n",
    "    binding_exists = any(\n",
    "        b[\"role\"] == role and member in b.get(\"members\", set())\n",
    "        for b in policy.bindings\n",
    "    )\n",
    "\n",
    "    if not binding_exists:\n",
    "        policy.bindings.append({\"role\": role, \"members\": {member}})\n",
    "        bucket.set_iam_policy(policy)\n",
    "        print(f\"Granted '{role}' to service account on bucket '{BUCKET_NAME}'.\")\n",
    "    else:\n",
    "        print(f\"Service account already has '{role}' on bucket '{BUCKET_NAME}'.\")\n"
   ],
   "metadata": {
    "id": "t95c9L_RT0R8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759982319281,
     "user_tz": 240,
     "elapsed": 2,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "t95c9L_RT0R8",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Data Generation Functions ---\n",
    "\n",
    "def generate_customers(num_customers: int) -> list:\n",
    "    \"\"\"Generates fake customer records with intentional data quality issues.\"\"\"\n",
    "    customers = []\n",
    "    customer_tiers = ['Bronze', 'Silver', 'Gold', 'Platinum']\n",
    "    signup_channels = ['Web', 'Mobile App', 'In-Store', 'Phone', 'Social Media']\n",
    "\n",
    "    for i in range(1, num_customers + 1):\n",
    "        tier = random.choices(customer_tiers, weights=[40, 30, 20, 10])[0]  # Most are Bronze\n",
    "\n",
    "        # 5% chance of missing email\n",
    "        profile_details = {\n",
    "            \"name\": fake.name(),\n",
    "            \"email\": fake.email() if random.random() > 0.05 else None,\n",
    "            \"address\": fake.address().replace('\\n', ', '),\n",
    "            \"join_date\": fake.date_between(start_date='-2y', end_date='today').isoformat(),\n",
    "            \"customer_tier\": tier,\n",
    "            \"signup_channel\": random.choice(signup_channels)\n",
    "        }\n",
    "        customers.append({\n",
    "            \"customer_id\": i,\n",
    "            \"profile\": json.dumps(profile_details)\n",
    "        })\n",
    "\n",
    "    # Introduce duplicate customer ID\n",
    "    if num_customers > 20:\n",
    "        customers.append({\n",
    "            \"customer_id\": 20,\n",
    "            \"profile\": json.dumps({\n",
    "                \"name\": fake.name(),\n",
    "                \"email\": fake.email(),\n",
    "                \"address\": fake.address().replace('\\n', ', '),\n",
    "                \"join_date\": fake.date_between(start_date='-1y', end_date='today').isoformat(),\n",
    "                \"customer_tier\": random.choice(customer_tiers),\n",
    "                \"signup_channel\": random.choice(signup_channels)\n",
    "            })\n",
    "        })\n",
    "\n",
    "    return customers\n",
    "\n",
    "def generate_products(num_products: int) -> list:\n",
    "    \"\"\"Generates fake product records with inconsistent category naming and business attributes.\"\"\"\n",
    "    categories = ['Electronics', 'Books', 'Home Goods', 'Apparel', 'Toys', 'home goods', 'electronics']\n",
    "    brands = ['BrandA', 'BrandB', 'BrandC', 'BrandD', 'Generic']\n",
    "\n",
    "    products = []\n",
    "    for i in range(1, num_products + 1):\n",
    "        category = random.choice(categories)\n",
    "        unit_price = round(random.uniform(5.5, 299.99), 2)\n",
    "        # Cost is 40-70% of price (margin varies)\n",
    "        unit_cost = round(unit_price * random.uniform(0.40, 0.70), 2)\n",
    "\n",
    "        products.append({\n",
    "            \"product_id\": i,\n",
    "            \"product_name\": f\"Product_{fake.word().capitalize()}_{i}\",\n",
    "            \"category\": category,\n",
    "            \"brand\": random.choice(brands),\n",
    "            \"sku\": f\"SKU-{i:05d}\",\n",
    "            \"unit_price\": unit_price,\n",
    "            \"unit_cost\": unit_cost\n",
    "        })\n",
    "    return products\n",
    "\n",
    "def generate_orders(num_orders: int, customer_ids: list, product_ids: list, customers_data: list, products_data: list) -> list:\n",
    "    \"\"\"Generates fake order records with discounts and shipping costs.\"\"\"\n",
    "    orders = []\n",
    "\n",
    "    # Create lookup for customer tiers\n",
    "    customer_tier_map = {}\n",
    "    for customer in customers_data:\n",
    "        profile = json.loads(customer['profile'])\n",
    "        customer_tier_map[customer['customer_id']] = profile.get('customer_tier', 'Bronze')\n",
    "\n",
    "    for i in range(1, num_orders + 1):\n",
    "        customer_id = random.choice(customer_ids)\n",
    "        order_date = fake.date_time_between(start_date='-1y', end_date='now')\n",
    "\n",
    "        # 5% chance of future order date\n",
    "        if random.random() < 0.05:\n",
    "            order_date = datetime.now() + timedelta(days=random.randint(2, 30))\n",
    "\n",
    "        # Discount based on customer tier\n",
    "        tier = customer_tier_map.get(customer_id, 'Bronze')\n",
    "        if tier == 'Platinum':\n",
    "            discount_percent = random.uniform(0, 0.15)  # Up to 15% off\n",
    "        elif tier == 'Gold':\n",
    "            discount_percent = random.uniform(0, 0.10)  # Up to 10% off\n",
    "        elif tier == 'Silver':\n",
    "            discount_percent = random.uniform(0, 0.05)  # Up to 5% off\n",
    "        else:\n",
    "            discount_percent = random.uniform(0, 0.02)  # Up to 2% off\n",
    "\n",
    "        # Shipping cost\n",
    "        shipping_cost = round(random.uniform(0, 15.99), 2)\n",
    "\n",
    "        # Create line items\n",
    "        line_items = []\n",
    "        products_in_order = random.sample(product_ids, k=random.randint(1, MAX_ITEMS_PER_ORDER))\n",
    "\n",
    "        for product_id in products_in_order:\n",
    "            line_items.append({\n",
    "                \"product_id\": product_id,\n",
    "                \"quantity\": random.randint(1, 5)\n",
    "            })\n",
    "\n",
    "        orders.append({\n",
    "            \"order_id\": 1000 + i,\n",
    "            \"customer_id\": customer_id,\n",
    "            \"order_timestamp\": order_date.isoformat(),\n",
    "            \"discount_percent\": round(discount_percent, 4),\n",
    "            \"shipping_cost\": shipping_cost,\n",
    "            \"line_items\": json.dumps(line_items)\n",
    "        })\n",
    "    return orders"
   ],
   "metadata": {
    "id": "ZqAqt74cGjAg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759982319480,
     "user_tz": 240,
     "elapsed": 201,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "ZqAqt74cGjAg",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Cleanup Functions ---\n",
    "\n",
    "def cleanup_all_resources(bq_client: bigquery.Client, storage_client: storage.Client,\n",
    "                         delete_datasets: bool = True, delete_bucket: bool = True,\n",
    "                         delete_connection: bool = True):\n",
    "    \"\"\"\n",
    "    Deletes all Google Cloud resources created by this notebook.\n",
    "\n",
    "    Args:\n",
    "        bq_client: BigQuery client instance\n",
    "        storage_client: Storage client instance\n",
    "        delete_datasets: If True, deletes bronze, silver, and gold datasets\n",
    "        delete_bucket: If True, deletes the GCS bucket\n",
    "        delete_connection: If True, deletes the BigQuery connection\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Cleanup ---\")\n",
    "\n",
    "    # Delete BigQuery datasets\n",
    "    if delete_datasets:\n",
    "        for dataset_name in [BRONZE_DATASET, SILVER_DATASET, GOLD_DATASET]:\n",
    "            dataset_id = f\"{PROJECT_ID}.{dataset_name}\"\n",
    "            try:\n",
    "                bq_client.delete_dataset(dataset_id, delete_contents=True, not_found_ok=True)\n",
    "                print(f\"Deleted dataset '{dataset_id}' and all its tables.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting dataset '{dataset_id}': {e}\")\n",
    "\n",
    "    # Delete GCS bucket\n",
    "    if delete_bucket:\n",
    "        try:\n",
    "            bucket = storage_client.bucket(BUCKET_NAME)\n",
    "            # Delete all blobs in the bucket first\n",
    "            blobs = list(bucket.list_blobs())\n",
    "            for blob in blobs:\n",
    "                blob.delete()\n",
    "            # Delete the bucket\n",
    "            bucket.delete()\n",
    "            print(f\"Deleted GCS bucket '{BUCKET_NAME}' and all its contents.\")\n",
    "        except NotFound:\n",
    "            print(f\"Bucket '{BUCKET_NAME}' not found, skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting bucket '{BUCKET_NAME}': {e}\")\n",
    "\n",
    "    # Delete BigQuery connection\n",
    "    if delete_connection:\n",
    "        full_connection_id = f\"{PROJECT_ID}.{REGION}.{BIGQUERY_CONNECTION_ID}\"\n",
    "        try:\n",
    "            delete_command = [\"bq\", \"rm\", \"--connection\", full_connection_id]\n",
    "            result = subprocess.run(delete_command, capture_output=True, text=True)\n",
    "            if result.returncode == 0:\n",
    "                print(f\"Deleted BigQuery connection '{full_connection_id}'.\")\n",
    "            else:\n",
    "                print(f\"Connection '{full_connection_id}' not found or already deleted.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting connection '{full_connection_id}': {e}\")\n",
    "\n",
    "    print(\"\\n--- Cleanup Complete! ---\")"
   ],
   "metadata": {
    "id": "vREn3ow3GmJd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759982319480,
     "user_tz": 240,
     "elapsed": 3,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "vREn3ow3GmJd",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "j6lbz1xy5l",
   "source": [
    "---\n",
    "\n",
    "## Data Pipeline Execution\n",
    "\n",
    "The following cells create the Bronze → Silver → Gold pipeline with intentional data quality issues."
   ],
   "metadata": {
    "id": "j6lbz1xy5l"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Create Bronze Layer ---\n",
    "\n",
    "validate_config()\n",
    "\n",
    "# Initialize GCP clients\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Setup infrastructure\n",
    "setup_gcs_bucket(storage_client)\n",
    "setup_bigquery_connection(bq_client, storage_client)\n",
    "ensure_dataset(bq_client, BIGQUERY_DATASET)\n",
    "\n",
    "# Generate fake data\n",
    "print(\"\\n--- Generating Fake Data (with quality issues) ---\")\n",
    "customers_data = generate_customers(NUM_CUSTOMERS)\n",
    "products_data = generate_products(NUM_PRODUCTS)\n",
    "orders_data = generate_orders(\n",
    "    NUM_ORDERS,\n",
    "    [c['customer_id'] for c in customers_data],\n",
    "    [p['product_id'] for p in products_data],\n",
    "    customers_data,\n",
    "    products_data\n",
    ")\n",
    "\n",
    "customers_df = pd.DataFrame(customers_data)\n",
    "products_df = pd.DataFrame(products_data)\n",
    "orders_df = pd.DataFrame(orders_data)\n",
    "print(\"Data generation complete.\")\n",
    "\n",
    "# Create bronze tables\n",
    "print(\"\\n--- Creating Bronze Layer Tables ---\")\n",
    "\n",
    "# 1. Native BigQuery table for customers\n",
    "table_id = f\"{PROJECT_ID}.{BIGQUERY_DATASET}.raw_customers\"\n",
    "job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
    "job = bq_client.load_table_from_dataframe(customers_df, table_id, job_config=job_config)\n",
    "job.result()\n",
    "print(f\"Created native table 'raw_customers' with {job.output_rows} rows.\")\n",
    "\n",
    "# 2. BigLake external table for products (CSV)\n",
    "table_id = f\"{PROJECT_ID}.{BIGQUERY_DATASET}.raw_products\"\n",
    "gcs_uri = f\"gs://{BUCKET_NAME}/bronze/raw_products/products.csv\"\n",
    "\n",
    "blob = storage.Blob.from_string(gcs_uri, client=storage_client)\n",
    "blob.upload_from_string(products_df.to_csv(index=False), 'text/csv')\n",
    "\n",
    "ddl = f\"\"\"\n",
    "CREATE OR REPLACE EXTERNAL TABLE `{table_id}`\n",
    "(\n",
    "    product_id INT64,\n",
    "    product_name STRING,\n",
    "    category STRING,\n",
    "    brand STRING,\n",
    "    sku STRING,\n",
    "    unit_price FLOAT64,\n",
    "    unit_cost FLOAT64\n",
    ")\n",
    "WITH CONNECTION `{PROJECT_ID}.{REGION}.{BIGQUERY_CONNECTION_ID}`\n",
    "OPTIONS (\n",
    "    format = 'CSV',\n",
    "    uris = ['{gcs_uri}'],\n",
    "    skip_leading_rows = 1,\n",
    "    max_staleness = INTERVAL 30 MINUTE,\n",
    "    metadata_cache_mode = 'AUTOMATIC'\n",
    ");\n",
    "\"\"\"\n",
    "bq_client.query(ddl).result()\n",
    "print(f\"Created BigLake external table 'raw_products'.\")\n",
    "\n",
    "# 3. BigLake table with Iceberg format for orders\n",
    "table_id = f\"{PROJECT_ID}.{BIGQUERY_DATASET}.raw_orders\"\n",
    "gcs_uri = f\"gs://{BUCKET_NAME}/bronze/raw_orders/\"\n",
    "\n",
    "ddl = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{table_id}`\n",
    "(\n",
    "    order_id INT64,\n",
    "    customer_id INT64,\n",
    "    order_timestamp STRING,\n",
    "    discount_percent FLOAT64,\n",
    "    shipping_cost FLOAT64,\n",
    "    line_items STRING\n",
    ")\n",
    "WITH CONNECTION `{PROJECT_ID}.{REGION}.{BIGQUERY_CONNECTION_ID}`\n",
    "OPTIONS (\n",
    "    table_format = 'ICEBERG',\n",
    "    storage_uri = '{gcs_uri}'\n",
    ");\n",
    "\"\"\"\n",
    "bq_client.query(ddl).result()\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_APPEND\")\n",
    "job = bq_client.load_table_from_dataframe(orders_df, table_id, job_config=job_config)\n",
    "job.result()\n",
    "print(f\"Created Iceberg table 'raw_orders' with {job.output_rows} rows.\")\n",
    "\n",
    "print(\"\\n--- Bronze Layer Complete! ---\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CjahCGQBG2E_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759982395276,
     "user_tz": 240,
     "elapsed": 10729,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "6c6e3990-cf53-4a43-f61b-75b7eb4e878e"
   },
   "id": "CjahCGQBG2E_",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Create Silver Layer ---\n",
    "\n",
    "ensure_dataset(bq_client, SILVER_DATASET)\n",
    "\n",
    "print(\"\\n--- Creating Silver Layer Tables ---\")\n",
    "print(\"NOTE: Silver layer performs basic type conversions but PRESERVES data quality issues\")\n",
    "print(\"      Quality issues will be detected later using Dataplex Data Quality checks\")\n",
    "\n",
    "# 1. Customers: Parse JSON, convert types (DO NOT deduplicate or filter nulls)\n",
    "customers_silver_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{SILVER_DATASET}.customers_silver` AS\n",
    "SELECT\n",
    "    customer_id,\n",
    "    JSON_VALUE(profile, '$.name') AS name,\n",
    "    JSON_VALUE(profile, '$.email') AS email,\n",
    "    JSON_VALUE(profile, '$.address') AS address,\n",
    "    CAST(JSON_VALUE(profile, '$.join_date') AS DATE) AS join_date,\n",
    "    JSON_VALUE(profile, '$.customer_tier') AS customer_tier,\n",
    "    JSON_VALUE(profile, '$.signup_channel') AS signup_channel\n",
    "FROM `{PROJECT_ID}.{BRONZE_DATASET}.raw_customers`;\n",
    "\"\"\"\n",
    "bq_client.query(customers_silver_sql).result()\n",
    "print(\"Created 'customers_silver' table (duplicates and nulls preserved).\")\n",
    "\n",
    "# 2. Products: Convert types (DO NOT standardize category naming)\n",
    "products_silver_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{SILVER_DATASET}.products_silver` AS\n",
    "SELECT\n",
    "    product_id,\n",
    "    product_name,\n",
    "    category,  -- Preserving original casing (quality issue)\n",
    "    brand,\n",
    "    sku,\n",
    "    CAST(unit_price AS NUMERIC) AS unit_price,\n",
    "    CAST(unit_cost AS NUMERIC) AS unit_cost\n",
    "FROM `{PROJECT_ID}.{BRONZE_DATASET}.raw_products`;\n",
    "\"\"\"\n",
    "bq_client.query(products_silver_sql).result()\n",
    "print(\"Created 'products_silver' table (inconsistent category casing preserved).\")\n",
    "\n",
    "# 3. Orders: Fix timestamps (DO NOT filter future dates)\n",
    "orders_silver_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{SILVER_DATASET}.orders_silver` AS\n",
    "SELECT\n",
    "    order_id,\n",
    "    customer_id,\n",
    "    CAST(order_timestamp AS TIMESTAMP) AS order_timestamp,\n",
    "    CAST(discount_percent AS NUMERIC) AS discount_percent,\n",
    "    CAST(shipping_cost AS NUMERIC) AS shipping_cost\n",
    "FROM `{PROJECT_ID}.{BRONZE_DATASET}.raw_orders`;\n",
    "\"\"\"\n",
    "bq_client.query(orders_silver_sql).result()\n",
    "print(\"Created 'orders_silver' table (future-dated orders preserved).\")\n",
    "\n",
    "# 4. Order Items: Unnest JSON line items\n",
    "order_items_silver_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{SILVER_DATASET}.order_items_silver` AS\n",
    "SELECT\n",
    "    raw.order_id,\n",
    "    CAST(JSON_VALUE(item_json, '$.product_id') AS INT64) AS product_id,\n",
    "    CAST(JSON_VALUE(item_json, '$.quantity') AS INT64) AS quantity\n",
    "FROM `{PROJECT_ID}.{BRONZE_DATASET}.raw_orders` raw,\n",
    "    UNNEST(JSON_QUERY_ARRAY(line_items)) AS item_json;\n",
    "\"\"\"\n",
    "bq_client.query(order_items_silver_sql).result()\n",
    "print(\"Created 'order_items_silver' table.\")\n",
    "\n",
    "print(\"\\n--- Silver Layer Complete! ---\")\n",
    "print(\"Data quality issues preserved for Dataplex detection\")"
   ],
   "metadata": {
    "id": "2l55JzFoQgBj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759982408513,
     "user_tz": 240,
     "elapsed": 8454,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "af1e707e-fa87-4dd7-8381-b7efc72b12b1"
   },
   "id": "2l55JzFoQgBj",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Create Gold Layer ---\n",
    "\n",
    "ensure_dataset(bq_client, GOLD_DATASET)\n",
    "\n",
    "print(\"\\n--- Creating Gold Layer Dimensional Model ---\")\n",
    "print(\"NOTE: Gold layer creates dimensional model but PRESERVES data quality issues\")\n",
    "print(\"      Issues will flow into dimensions and fact table for Dataplex detection\")\n",
    "\n",
    "# 1. Customer dimension with surrogate key\n",
    "dim_customers_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{GOLD_DATASET}.dim_customers` AS\n",
    "SELECT\n",
    "    GENERATE_UUID() AS customer_key,\n",
    "    customer_id AS customer_natural_key,\n",
    "    name,\n",
    "    email,\n",
    "    address,\n",
    "    join_date,\n",
    "    customer_tier,\n",
    "    signup_channel,\n",
    "    CURRENT_TIMESTAMP() AS effective_date,\n",
    "    TRUE AS is_current\n",
    "FROM `{PROJECT_ID}.{SILVER_DATASET}.customers_silver`;\n",
    "\"\"\"\n",
    "bq_client.query(dim_customers_sql).result()\n",
    "print(\"Created 'dim_customers' (duplicate customer_id=20 preserved).\")\n",
    "\n",
    "# 2. Product dimension with surrogate key\n",
    "dim_products_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{GOLD_DATASET}.dim_products` AS\n",
    "SELECT\n",
    "    GENERATE_UUID() AS product_key,\n",
    "    product_id AS product_natural_key,\n",
    "    product_name,\n",
    "    category,\n",
    "    brand,\n",
    "    sku,\n",
    "    unit_price,\n",
    "    unit_cost,\n",
    "    ROUND(unit_price - unit_cost, 2) AS unit_margin,\n",
    "    ROUND(SAFE_DIVIDE(unit_price - unit_cost, unit_price) * 100, 2) AS margin_percent\n",
    "FROM `{PROJECT_ID}.{SILVER_DATASET}.products_silver`;\n",
    "\"\"\"\n",
    "bq_client.query(dim_products_sql).result()\n",
    "print(\"Created 'dim_products' (inconsistent category casing preserved).\")\n",
    "\n",
    "# 3. Date dimension\n",
    "dim_date_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{GOLD_DATASET}.dim_date` AS\n",
    "SELECT\n",
    "    date,\n",
    "    EXTRACT(YEAR FROM date) AS year,\n",
    "    EXTRACT(MONTH FROM date) AS month,\n",
    "    EXTRACT(DAY FROM date) AS day,\n",
    "    EXTRACT(QUARTER FROM date) AS quarter,\n",
    "    EXTRACT(WEEK FROM date) AS week_of_year,\n",
    "    EXTRACT(DAYOFWEEK FROM date) AS day_of_week_num,\n",
    "    FORMAT_DATE('%A', date) AS day_of_week_name,\n",
    "    FORMAT_DATE('%B', date) AS month_name,\n",
    "    -- Weekend flag\n",
    "    CASE WHEN EXTRACT(DAYOFWEEK FROM date) IN (1, 7) THEN TRUE ELSE FALSE END AS is_weekend,\n",
    "    -- Fiscal year (assuming July 1 start)\n",
    "    CASE\n",
    "        WHEN EXTRACT(MONTH FROM date) >= 7 THEN EXTRACT(YEAR FROM date) + 1\n",
    "        ELSE EXTRACT(YEAR FROM date)\n",
    "    END AS fiscal_year,\n",
    "    -- Fiscal quarter\n",
    "    CASE\n",
    "        WHEN EXTRACT(MONTH FROM date) IN (7, 8, 9) THEN 1\n",
    "        WHEN EXTRACT(MONTH FROM date) IN (10, 11, 12) THEN 2\n",
    "        WHEN EXTRACT(MONTH FROM date) IN (1, 2, 3) THEN 3\n",
    "        ELSE 4\n",
    "    END AS fiscal_quarter,\n",
    "    -- Month start/end flags\n",
    "    CASE WHEN EXTRACT(DAY FROM date) = 1 THEN TRUE ELSE FALSE END AS is_month_start,\n",
    "    CASE WHEN DATE_ADD(date, INTERVAL 1 DAY) != DATE_TRUNC(DATE_ADD(date, INTERVAL 1 DAY), MONTH)\n",
    "         THEN FALSE ELSE TRUE END AS is_month_end,\n",
    "    -- Year start/end flags\n",
    "    CASE WHEN FORMAT_DATE('%m-%d', date) = '01-01' THEN TRUE ELSE FALSE END AS is_year_start,\n",
    "    CASE WHEN FORMAT_DATE('%m-%d', date) = '12-31' THEN TRUE ELSE FALSE END AS is_year_end\n",
    "FROM (\n",
    "    SELECT DISTINCT EXTRACT(DATE FROM order_timestamp) AS date\n",
    "    FROM `{PROJECT_ID}.{SILVER_DATASET}.orders_silver`\n",
    ");\n",
    "\"\"\"\n",
    "bq_client.query(dim_date_sql).result()\n",
    "print(\"Created 'dim_date' (includes future dates from bad data).\")\n",
    "\n",
    "# 4. Sales fact table\n",
    "fct_sales_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{GOLD_DATASET}.fct_sales` AS\n",
    "SELECT\n",
    "    -- Surrogate keys\n",
    "    dc.customer_key,\n",
    "    dp.product_key,\n",
    "    EXTRACT(DATE FROM o.order_timestamp) AS order_date,\n",
    "\n",
    "    -- Degenerate dimension (fact grain identifier)\n",
    "    oi.order_id,\n",
    "\n",
    "    -- Order-level attributes\n",
    "    o.discount_percent AS order_discount_percent,\n",
    "    o.shipping_cost,\n",
    "\n",
    "    -- Line item measures\n",
    "    oi.quantity,\n",
    "    dp.unit_price,\n",
    "    dp.unit_cost,\n",
    "\n",
    "    -- Calculated measures\n",
    "    ROUND(oi.quantity * dp.unit_price, 2) AS gross_revenue,\n",
    "    ROUND((oi.quantity * dp.unit_price) * o.discount_percent, 2) AS discount_amount,\n",
    "    ROUND((oi.quantity * dp.unit_price) * (1 - o.discount_percent), 2) AS net_revenue,\n",
    "    ROUND(oi.quantity * dp.unit_cost, 2) AS cost_of_goods_sold,\n",
    "    ROUND((oi.quantity * dp.unit_price) * (1 - o.discount_percent) - (oi.quantity * dp.unit_cost), 2) AS gross_margin,\n",
    "    ROUND(SAFE_DIVIDE(\n",
    "        (oi.quantity * dp.unit_price) * (1 - o.discount_percent) - (oi.quantity * dp.unit_cost),\n",
    "        (oi.quantity * dp.unit_price) * (1 - o.discount_percent)\n",
    "    ) * 100, 2) AS margin_percent\n",
    "\n",
    "FROM `{PROJECT_ID}.{SILVER_DATASET}.order_items_silver` AS oi\n",
    "INNER JOIN `{PROJECT_ID}.{SILVER_DATASET}.orders_silver` AS o\n",
    "    ON oi.order_id = o.order_id\n",
    "INNER JOIN `{PROJECT_ID}.{GOLD_DATASET}.dim_products` AS dp\n",
    "    ON oi.product_id = dp.product_natural_key\n",
    "INNER JOIN `{PROJECT_ID}.{GOLD_DATASET}.dim_customers` AS dc\n",
    "    ON o.customer_id = dc.customer_natural_key;\n",
    "\"\"\"\n",
    "bq_client.query(fct_sales_sql).result()\n",
    "print(\"Created 'fct_sales' (all quality issues preserved).\")\n",
    "\n",
    "# 5. Wide table view (One Big Table)\n",
    "vw_sales_wide_sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW `{PROJECT_ID}.{GOLD_DATASET}.vw_sales_wide` AS\n",
    "SELECT\n",
    "    -- Fact measures\n",
    "    f.order_id,\n",
    "    f.quantity,\n",
    "    f.gross_revenue,\n",
    "    f.discount_amount,\n",
    "    f.net_revenue,\n",
    "    f.cost_of_goods_sold,\n",
    "    f.gross_margin,\n",
    "    f.margin_percent,\n",
    "    f.order_discount_percent,\n",
    "    f.shipping_cost,\n",
    "\n",
    "    -- Customer attributes\n",
    "    c.customer_natural_key AS customer_id,\n",
    "    c.name AS customer_name,\n",
    "    c.email AS customer_email,\n",
    "    c.customer_tier,\n",
    "    c.signup_channel,\n",
    "    c.join_date AS customer_join_date,\n",
    "\n",
    "    -- Product attributes\n",
    "    p.product_natural_key AS product_id,\n",
    "    p.product_name,\n",
    "    p.category AS product_category,\n",
    "    p.brand AS product_brand,\n",
    "    p.sku,\n",
    "    p.unit_price,\n",
    "    p.unit_cost,\n",
    "    p.unit_margin,\n",
    "\n",
    "    -- Date attributes\n",
    "    d.date AS order_date,\n",
    "    d.year,\n",
    "    d.month,\n",
    "    d.day,\n",
    "    d.quarter,\n",
    "    d.fiscal_year,\n",
    "    d.fiscal_quarter,\n",
    "    d.week_of_year,\n",
    "    d.day_of_week_num,\n",
    "    d.day_of_week_name,\n",
    "    d.month_name,\n",
    "    d.is_weekend,\n",
    "    d.is_month_start,\n",
    "    d.is_month_end,\n",
    "    d.is_year_start,\n",
    "    d.is_year_end\n",
    "\n",
    "FROM `{PROJECT_ID}.{GOLD_DATASET}.fct_sales` AS f\n",
    "INNER JOIN `{PROJECT_ID}.{GOLD_DATASET}.dim_customers` AS c\n",
    "    ON f.customer_key = c.customer_key\n",
    "INNER JOIN `{PROJECT_ID}.{GOLD_DATASET}.dim_products` AS p\n",
    "    ON f.product_key = p.product_key\n",
    "INNER JOIN `{PROJECT_ID}.{GOLD_DATASET}.dim_date` AS d\n",
    "    ON f.order_date = d.date;\n",
    "\"\"\"\n",
    "bq_client.query(vw_sales_wide_sql).result()\n",
    "print(\"Created 'vw_sales_wide' view (contains all data quality issues).\")\n",
    "\n",
    "print(\"\\n--- Gold Layer Complete! ---\")\n",
    "print(\"\\n   Data Quality Issues Present in Gold Layer:\")\n",
    "print(\"  1. Duplicate customers (customer_id=20 appears twice)\")\n",
    "print(\"  2. NULL emails (~5% of customer records)\")\n",
    "print(\"  3. Inconsistent category casing ('Electronics' vs 'electronics')\")\n",
    "print(\"  4. Future-dated orders (~5% of orders)\")\n",
    "print(\"\\nNext Step: Use Dataplex Data Quality to detect these issues in Gold,\")\n",
    "print(\"           then trace them back to Bronze layer for remediation.\")"
   ],
   "metadata": {
    "id": "-_o1RkABfRYi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759982418358,
     "user_tz": 240,
     "elapsed": 9846,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "6598e31d-bcff-4981-916c-82cce4b503e4"
   },
   "id": "-_o1RkABfRYi",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Query: Current month's revenue and margin analysis\n",
    "# NOTE: Results contain quality issues that will be detected by Dataplex:\n",
    "#     - Future-dated orders inflate current month revenue\n",
    "#     - Revenue calculations based on dirty data\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    CONCAT('$', FORMAT('%.2f', SUM(s.gross_revenue))) AS gross_revenue,\n",
    "    CONCAT('$', FORMAT('%.2f', SUM(s.discount_amount))) AS total_discounts,\n",
    "    CONCAT('$', FORMAT('%.2f', SUM(s.net_revenue))) AS net_revenue,\n",
    "    CONCAT('$', FORMAT('%.2f', SUM(s.cost_of_goods_sold))) AS total_cogs,\n",
    "    CONCAT('$', FORMAT('%.2f', SUM(s.gross_margin))) AS gross_margin,\n",
    "    FORMAT('%.1f%%', SAFE_DIVIDE(SUM(s.gross_margin), SUM(s.net_revenue)) * 100) AS margin_percent\n",
    "FROM `{PROJECT_ID}.{GOLD_DATASET}.fct_sales` AS s\n",
    "JOIN `{PROJECT_ID}.{GOLD_DATASET}.dim_date` AS d ON s.order_date = d.date\n",
    "WHERE\n",
    "    d.year = EXTRACT(YEAR FROM CURRENT_DATE())\n",
    "    AND d.month = EXTRACT(MONTH FROM CURRENT_DATE())\n",
    "\"\"\"\n",
    "\n",
    "df = bq_client.query(query).to_dataframe()\n",
    "display(df)"
   ],
   "metadata": {
    "id": "PphRB5ilg1V2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759982425464,
     "user_tz": 240,
     "elapsed": 1756,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "3fd89d11-7903-4a32-cfb8-862ebe8b8678"
   },
   "id": "PphRB5ilg1V2",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ye4yg9rc5k",
   "source": [
    "---\n",
    "\n",
    "## Query Examples\n",
    "\n",
    "The following queries demonstrate the data in the Gold layer. Notice that quality issues are present in the results."
   ],
   "metadata": {
    "id": "ye4yg9rc5k"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Query: Top customers by lifetime value with tier analysis\n",
    "# NOTE: Results contain quality issues that will be detected by Dataplex:\n",
    "#     - Duplicate customer_id=20 may cause revenue misattribution\n",
    "#     - NULL emails mean some customers can't receive marketing\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    c.name,\n",
    "    c.customer_tier,\n",
    "    c.signup_channel,\n",
    "    COUNT(DISTINCT s.order_id) AS total_orders,\n",
    "    SUM(s.quantity) AS total_items,\n",
    "    CONCAT('$', FORMAT('%.2f', SUM(s.net_revenue))) AS lifetime_value,\n",
    "    CONCAT('$', FORMAT('%.2f', AVG(s.net_revenue))) AS avg_line_item_value,\n",
    "    FORMAT('%.1f%%', AVG(s.margin_percent)) AS avg_margin_pct\n",
    "FROM `{PROJECT_ID}.{GOLD_DATASET}.fct_sales` AS s\n",
    "JOIN `{PROJECT_ID}.{GOLD_DATASET}.dim_customers` AS c ON s.customer_key = c.customer_key\n",
    "GROUP BY c.customer_key, c.name, c.customer_tier, c.signup_channel\n",
    "ORDER BY SUM(s.net_revenue) DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "df = bq_client.query(query).to_dataframe()\n",
    "display(df)"
   ],
   "metadata": {
    "id": "MWOr8mXki6iI",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759982430623,
     "user_tz": 240,
     "elapsed": 1627,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "03134aed-f10b-468f-8a92-0d979fe5c340"
   },
   "id": "MWOr8mXki6iI",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "zklgv41fyf",
   "source": [
    "# Query: Product category performance with margin analysis\n",
    "# NOTE: Results contain quality issues that will be detected by Dataplex:\n",
    "#     - Inconsistent category casing splits same category into multiple rows\n",
    "#     - 'Electronics' and 'electronics' appear as separate categories\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    p.category,\n",
    "    p.brand,\n",
    "    COUNT(DISTINCT s.order_id) AS orders,\n",
    "    SUM(s.quantity) AS units_sold,\n",
    "    CONCAT('$', FORMAT('%.2f', SUM(s.gross_revenue))) AS gross_revenue,\n",
    "    CONCAT('$', FORMAT('%.2f', SUM(s.gross_margin))) AS gross_margin,\n",
    "    FORMAT('%.1f%%', SAFE_DIVIDE(SUM(s.gross_margin), SUM(s.net_revenue)) * 100) AS margin_pct\n",
    "FROM `{PROJECT_ID}.{GOLD_DATASET}.fct_sales` AS s\n",
    "JOIN `{PROJECT_ID}.{GOLD_DATASET}.dim_products` AS p ON s.product_key = p.product_key\n",
    "GROUP BY p.category, p.brand\n",
    "ORDER BY SUM(s.gross_revenue) DESC\n",
    "LIMIT 15\n",
    "\"\"\"\n",
    "\n",
    "df = bq_client.query(query).to_dataframe()\n",
    "display(df)"
   ],
   "metadata": {
    "id": "zklgv41fyf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759982435096,
     "user_tz": 240,
     "elapsed": 1538,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "c89cf7fb-327c-4a85-f0f6-01df1f232a04"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "lz74wrlvuiq",
   "source": [
    "## Using the Wide Table View\n",
    "\n",
    "The `vw_sales_wide` view simplifies queries by pre-joining all dimensions.\n",
    "\n",
    "**Note**: This view contains all data quality issues from the underlying tables. Use Dataplex Data Quality to detect and remediate these issues before using in production analytics."
   ],
   "metadata": {
    "id": "lz74wrlvuiq"
   }
  },
  {
   "cell_type": "code",
   "id": "lombcovxey",
   "source": [
    "query = f\"\"\"\n",
    "SELECT\n",
    "    customer_tier,\n",
    "    product_category,\n",
    "    fiscal_year,\n",
    "    fiscal_quarter,\n",
    "    CASE WHEN is_weekend THEN 'Weekend' ELSE 'Weekday' END AS day_type,\n",
    "    COUNT(DISTINCT order_id) AS orders,\n",
    "    SUM(quantity) AS units,\n",
    "    CONCAT('$', FORMAT('%.2f', SUM(net_revenue))) AS revenue,\n",
    "    FORMAT('%.1f%%', AVG(margin_percent)) AS avg_margin\n",
    "FROM `{PROJECT_ID}.{GOLD_DATASET}.vw_sales_wide`\n",
    "WHERE fiscal_year = 2025\n",
    "GROUP BY customer_tier, product_category, fiscal_year, fiscal_quarter, day_type\n",
    "ORDER BY SUM(net_revenue) DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "df = bq_client.query(query).to_dataframe()\n",
    "display(df)"
   ],
   "metadata": {
    "id": "lombcovxey",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759982441571,
     "user_tz": 240,
     "elapsed": 1855,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "b90f6ca8-904b-48ac-ab1d-6dd3cabe939f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "y45sv1dqzbe",
   "source": [
    "# Dataplex Data Quality - Gold Layer\n",
    "\n",
    "Now we'll use **Dataplex Data Profiling and Data Quality** to detect the issues we injected into our Gold layer.\n",
    "\n",
    "## Approach\n",
    "\n",
    "**Phase 1: Data Profiling**\n",
    "- Statistical analysis of all columns\n",
    "- Detects NULL percentages, min/max values, distributions, outliers\n",
    "- No rules required - pure profiling using `DataProfileSpec`\n",
    "\n",
    "**Phase 2: Data Quality Validation**\n",
    "- Custom validation rules based on business logic\n",
    "- Completeness, validity, uniqueness, conformity checks\n",
    "- Validates cross-column accuracy (e.g., revenue calculations)\n",
    "- Uses `DataQualitySpec` with defined rules\n",
    "\n",
    "**Phase 3: Trace to Source**\n",
    "- Use Data Lineage to understand the source of the data quality problems\n",
    "- Create necessary checks to catch DQ issues prior to reaching the gold layer\n",
    "\n",
    "Let's start with Phase 1 and 2 on the Gold layer."
   ],
   "metadata": {
    "id": "y45sv1dqzbe"
   }
  },
  {
   "cell_type": "markdown",
   "id": "9tjzuhb8wh7",
   "source": [
    "---\n"
   ],
   "metadata": {
    "id": "9tjzuhb8wh7"
   }
  },
  {
   "cell_type": "code",
   "id": "7sxvk5f4cy3",
   "source": [
    "from google.cloud import dataplex_v1\n",
    "from google.protobuf import field_mask_pb2\n",
    "from google.api_core.exceptions import ResourceExhausted, AlreadyExists\n",
    "from typing import Optional\n",
    "import yaml\n",
    "import time\n",
    "\n",
    "# Initialize Dataplex client\n",
    "dataplex_client = dataplex_v1.DataScanServiceClient()\n",
    "\n",
    "# Configuration\n",
    "DATAPLEX_REGION = REGION\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATAPLEX DATA PROFILING & DATA QUALITY SETUP\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nProject: {PROJECT_ID}\")\n",
    "print(f\"Region: {DATAPLEX_REGION}\")\n",
    "print(f\"Gold Dataset: {GOLD_DATASET}\")\n",
    "print(\"\\nThis will create Dataplex scans for:\")\n",
    "print(\"\\n  Phase 1 - Data Profiling (DataProfileSpec):\")\n",
    "print(\"    - dim_customers\")\n",
    "print(\"    - dim_products\")\n",
    "print(\"    - dim_date\")\n",
    "print(\"    - fct_sales\")\n",
    "print(\"\\n  Phase 2 - Data Quality Validation (DataQualitySpec with rules):\")\n",
    "print(\"    - dim_customers (4 rules)\")\n",
    "print(\"    - dim_products (4 rules)\")\n",
    "print(\"    - dim_date (1 rule)\")\n",
    "print(\"    - fct_sales (4 rules)\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ],
   "metadata": {
    "id": "7sxvk5f4cy3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759982498136,
     "user_tz": 240,
     "elapsed": 373,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "fed05e71-076f-4b7d-afcb-de25d08d49d5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "n8k959ulviq",
   "source": [
    "## Phase 1: Data Profiling\n",
    "\n",
    "Data Profiling will automatically analyze all columns and provide statistics:\n",
    "- NULL percentages and counts\n",
    "- Min/Max/Average values\n",
    "- Data type distributions\n",
    "- Cardinality and uniqueness metrics\n",
    "- Potential quality issues detected\n",
    "\n",
    "This runs **without any rule definitions** - it's pure statistical profiling using `DataProfileSpec`."
   ],
   "metadata": {
    "id": "n8k959ulviq"
   }
  },
  {
   "cell_type": "code",
   "id": "obgpejdre",
   "source": [
    "def create_profiling_scan(table_name: str, description: str) -> None:\n",
    "    \"\"\"\n",
    "    Creates a Dataplex Data Profiling scan for a BigQuery table with retry logic.\n",
    "\n",
    "    Args:\n",
    "        table_name: Name of the table (e.g., 'dim_customers')\n",
    "        description: Human-readable description of the scan\n",
    "    \"\"\"\n",
    "    scan_id = f\"profile-{table_name.replace('_', '-')}\"\n",
    "    parent = f\"projects/{PROJECT_ID}/locations/{DATAPLEX_REGION}\"\n",
    "\n",
    "    # Define the data source\n",
    "    data_source = dataplex_v1.DataSource(\n",
    "        resource=f\"//bigquery.googleapis.com/projects/{PROJECT_ID}/datasets/{GOLD_DATASET}/tables/{table_name}\"\n",
    "    )\n",
    "\n",
    "    # Data Profile spec - statistical analysis without rules\n",
    "    data_profile_spec = dataplex_v1.DataProfileSpec(\n",
    "        sampling_percent=100.0,  # Profile 100% of data\n",
    "        row_filter=None,  # No filter, profile all rows\n",
    "    )\n",
    "\n",
    "    # Create the DataScan\n",
    "    data_scan = dataplex_v1.DataScan(\n",
    "        description=f\"{description} - Data Profiling\",\n",
    "        display_name=f\"Profile: {table_name}\",\n",
    "        data=data_source,\n",
    "        data_profile_spec=data_profile_spec,\n",
    "    )\n",
    "\n",
    "    request = dataplex_v1.CreateDataScanRequest(\n",
    "        parent=parent,\n",
    "        data_scan=data_scan,\n",
    "        data_scan_id=scan_id,\n",
    "    )\n",
    "\n",
    "    # Retry with exponential backoff for the create call only\n",
    "    max_retries = 5\n",
    "    base_delay = 2  # seconds\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            operation = dataplex_client.create_data_scan(request=request)\n",
    "            print(f\"Created Data Profiling scan for '{table_name}': {scan_id}\")\n",
    "            # Don't wait for operation to complete - just return after successful creation\n",
    "            return None\n",
    "        except AlreadyExists:\n",
    "            print(f\"Data Profiling scan already exists for '{table_name}': {scan_id}\")\n",
    "            return None\n",
    "        except ResourceExhausted as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                delay = base_delay * (2 ** attempt)  # Exponential backoff: 2, 4, 8, 16, 32 seconds\n",
    "                print(f\"Rate limit hit. Retrying in {delay} seconds... (attempt {attempt + 1}/{max_retries})\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(f\"Error creating scan for '{table_name}' after {max_retries} attempts: {e}\")\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating scan for '{table_name}': {e}\")\n",
    "            raise\n",
    "\n",
    "    return None\n",
    "\n",
    "print(\"\\n--- Creating Data Profiling Scans for Gold Layer Tables ---\\n\")\n",
    "\n",
    "# Create Data Profiling scans for each Gold table\n",
    "gold_tables = [\n",
    "    (\"dim_customers\", \"Customer dimension with duplicate and NULL email issues\"),\n",
    "    (\"dim_products\", \"Product dimension with inconsistent category casing\"),\n",
    "    (\"dim_date\", \"Date dimension with future dates\"),\n",
    "    (\"fct_sales\", \"Sales fact table with quality issues from dimensions\"),\n",
    "]\n",
    "\n",
    "for i, (table_name, description) in enumerate(gold_tables):\n",
    "    create_profiling_scan(table_name, description)\n",
    "    # Add delay between scans to avoid rate limiting (except after last scan)\n",
    "    if i < len(gold_tables) - 1:\n",
    "        time.sleep(2)\n",
    "\n",
    "print(\"\\nData Profiling scans created!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Go to Dataplex in GCP Console\")\n",
    "print(\"2. Navigate to Data Profiling > Scans\")\n",
    "print(\"3. Run the scans manually or wait for scheduled execution\")\n",
    "print(\"4. Review the profiling results to see column statistics\")"
   ],
   "metadata": {
    "id": "obgpejdre",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759982510120,
     "user_tz": 240,
     "elapsed": 8685,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "c56f214f-0962-47a5-b492-f352adf78337"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ss08z4kdky",
   "source": [
    "## Phase 2: Custom Business Rules\n",
    "\n",
    "Based on business requirements, we'll define custom validation rules to catch specific issues:\n",
    "\n",
    "**For dim_customers:**\n",
    "- Uniqueness: customer_natural_key must be unique\n",
    "- Completeness: email, name, customer_tier cannot be NULL\n",
    "- Validity: join_date must be <= today\n",
    "\n",
    "**For dim_products:**\n",
    "- Uniqueness: product_natural_key, sku must be unique\n",
    "- Conformity: category must match approved list\n",
    "- Range: unit_price, unit_cost must be >= 0\n",
    "\n",
    "**For dim_date:**\n",
    "- Validity: date must be <= today (no future dates)\n",
    "\n",
    "**For fct_sales:**\n",
    "- Range: revenue/margin metrics must be >= 0\n",
    "- Accuracy: net_revenue = gross_revenue - discount_amount\n",
    "- Referential Integrity: Keys must exist in dimensions"
   ],
   "metadata": {
    "id": "ss08z4kdky"
   }
  },
  {
   "cell_type": "code",
   "id": "nykd3cx135o",
   "source": [
    "def create_custom_dq_scan(table_name: str, description: str, rules: List[dataplex_v1.DataQualityRule]) -> None:\n",
    "    \"\"\"\n",
    "    Creates a Dataplex Data Quality scan with custom validation rules and retry logic.\n",
    "\n",
    "    Args:\n",
    "        table_name: Name of the table\n",
    "        description: Human-readable description\n",
    "        rules: List of DataQualityRule objects\n",
    "    \"\"\"\n",
    "    scan_id = f\"custom-dq-{table_name.replace('_', '-')}\"\n",
    "    parent = f\"projects/{PROJECT_ID}/locations/{DATAPLEX_REGION}\"\n",
    "\n",
    "    # Define the data source\n",
    "    data_source = dataplex_v1.DataSource(\n",
    "        resource=f\"//bigquery.googleapis.com/projects/{PROJECT_ID}/datasets/{GOLD_DATASET}/tables/{table_name}\"\n",
    "    )\n",
    "\n",
    "    # Custom DQ spec with rules\n",
    "    data_quality_spec = dataplex_v1.DataQualitySpec(\n",
    "        sampling_percent=100.0,\n",
    "        rules=rules,\n",
    "    )\n",
    "\n",
    "    # Create the DataScan\n",
    "    data_scan = dataplex_v1.DataScan(\n",
    "        description=f\"{description} - Custom Business Rules\",\n",
    "        display_name=f\"Custom DQ: {table_name}\",\n",
    "        data=data_source,\n",
    "        data_quality_spec=data_quality_spec,\n",
    "    )\n",
    "\n",
    "    request = dataplex_v1.CreateDataScanRequest(\n",
    "        parent=parent,\n",
    "        data_scan=data_scan,\n",
    "        data_scan_id=scan_id,\n",
    "    )\n",
    "\n",
    "    # Retry with exponential backoff for the create call only\n",
    "    max_retries = 5\n",
    "    base_delay = 2  # seconds\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            operation = dataplex_client.create_data_scan(request=request)\n",
    "            print(f\"Created Custom DQ scan for '{table_name}': {scan_id}\")\n",
    "            print(f\"  Rules: {len(rules)} validation rules defined\")\n",
    "            # Don't wait for operation to complete - just return after successful creation\n",
    "            return None\n",
    "        except AlreadyExists:\n",
    "            print(f\"Custom DQ scan already exists for '{table_name}': {scan_id}\")\n",
    "            return None\n",
    "        except ResourceExhausted as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                delay = base_delay * (2 ** attempt)  # Exponential backoff: 2, 4, 8, 16, 32 seconds\n",
    "                print(f\"Rate limit hit. Retrying in {delay} seconds... (attempt {attempt + 1}/{max_retries})\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                print(f\"Error creating scan for '{table_name}' after {max_retries} attempts: {e}\")\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating scan for '{table_name}': {e}\")\n",
    "            raise\n",
    "\n",
    "    return None\n",
    "\n",
    "print(\"\\n--- Creating Custom DQ Scans with Business Rules ---\\n\")\n",
    "\n",
    "# Define rules for dim_customers\n",
    "dim_customers_rules = [\n",
    "    # Uniqueness: customer_natural_key must be unique (will catch duplicate customer_id=20)\n",
    "    dataplex_v1.DataQualityRule(\n",
    "        column=\"customer_natural_key\",\n",
    "        dimension=\"UNIQUENESS\",\n",
    "        uniqueness_expectation=dataplex_v1.DataQualityRule.UniquenessExpectation(),\n",
    "        description=\"Customer natural key must be unique\",\n",
    "        threshold=0.99,  # Allow 1% tolerance\n",
    "    ),\n",
    "    # Completeness: email cannot be NULL (will catch ~5% NULL emails)\n",
    "    dataplex_v1.DataQualityRule(\n",
    "        column=\"email\",\n",
    "        dimension=\"COMPLETENESS\",\n",
    "        non_null_expectation=dataplex_v1.DataQualityRule.NonNullExpectation(),\n",
    "        description=\"Customer email must not be NULL\",\n",
    "        threshold=0.95,  # Expect 95% completeness\n",
    "    ),\n",
    "    # Completeness: name cannot be NULL\n",
    "    dataplex_v1.DataQualityRule(\n",
    "        column=\"name\",\n",
    "        dimension=\"COMPLETENESS\",\n",
    "        non_null_expectation=dataplex_v1.DataQualityRule.NonNullExpectation(),\n",
    "        description=\"Customer name must not be NULL\",\n",
    "        threshold=1.0,\n",
    "    ),\n",
    "    # Validity: join_date must be <= today (SqlAssertion returns FAILING rows)\n",
    "    dataplex_v1.DataQualityRule(\n",
    "        column=\"join_date\",\n",
    "        dimension=\"VALIDITY\",\n",
    "        sql_assertion=dataplex_v1.DataQualityRule.SqlAssertion(\n",
    "            sql_statement=f\"SELECT join_date FROM `{PROJECT_ID}.{GOLD_DATASET}.dim_customers` WHERE join_date > CURRENT_DATE()\"\n",
    "        ),\n",
    "        description=\"Customer join date cannot be in the future\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "create_custom_dq_scan(\"dim_customers\", \"Customer dimension validation\", dim_customers_rules)\n",
    "time.sleep(2)  # Pause between scans to avoid rate limiting\n",
    "\n",
    "# Define rules for dim_products\n",
    "dim_products_rules = [\n",
    "    # Uniqueness: product_natural_key must be unique\n",
    "    dataplex_v1.DataQualityRule(\n",
    "        column=\"product_natural_key\",\n",
    "        dimension=\"UNIQUENESS\",\n",
    "        uniqueness_expectation=dataplex_v1.DataQualityRule.UniquenessExpectation(),\n",
    "        description=\"Product natural key must be unique\",\n",
    "        threshold=1.0,\n",
    "    ),\n",
    "    # Uniqueness: SKU must be unique\n",
    "    dataplex_v1.DataQualityRule(\n",
    "        column=\"sku\",\n",
    "        dimension=\"UNIQUENESS\",\n",
    "        uniqueness_expectation=dataplex_v1.DataQualityRule.UniquenessExpectation(),\n",
    "        description=\"Product SKU must be unique\",\n",
    "        threshold=1.0,\n",
    "    ),\n",
    "    # Conformity: category must be in approved list (will catch lowercase variants)\n",
    "    dataplex_v1.DataQualityRule(\n",
    "        column=\"category\",\n",
    "        dimension=\"VALIDITY\",\n",
    "        set_expectation=dataplex_v1.DataQualityRule.SetExpectation(\n",
    "            values=['Electronics', 'Books', 'Home Goods', 'Apparel', 'Toys']\n",
    "        ),\n",
    "        description=\"Product category must match approved list (case-sensitive)\",\n",
    "        threshold=1.0,\n",
    "    ),\n",
    "    # Range: unit_price must be positive\n",
    "    dataplex_v1.DataQualityRule(\n",
    "        column=\"unit_price\",\n",
    "        dimension=\"VALIDITY\",\n",
    "        range_expectation=dataplex_v1.DataQualityRule.RangeExpectation(\n",
    "            min_value=\"0\",\n",
    "            strict_min_enabled=True,\n",
    "        ),\n",
    "        description=\"Product unit price must be greater than 0\",\n",
    "        threshold=1.0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "create_custom_dq_scan(\"dim_products\", \"Product dimension validation\", dim_products_rules)\n",
    "time.sleep(2)  # Pause between scans to avoid rate limiting\n",
    "\n",
    "# Define rules for dim_date\n",
    "dim_date_rules = [\n",
    "    # Validity: date cannot be in the future (SqlAssertion returns FAILING rows)\n",
    "    dataplex_v1.DataQualityRule(\n",
    "        column=\"date\",\n",
    "        dimension=\"VALIDITY\",\n",
    "        sql_assertion=dataplex_v1.DataQualityRule.SqlAssertion(\n",
    "            sql_statement=f\"SELECT date FROM `{PROJECT_ID}.{GOLD_DATASET}.dim_date` WHERE date > CURRENT_DATE()\"\n",
    "        ),\n",
    "        description=\"Order date cannot be in the future\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "create_custom_dq_scan(\"dim_date\", \"Date dimension validation\", dim_date_rules)\n",
    "time.sleep(2)  # Pause between scans to avoid rate limiting\n",
    "\n",
    "# Define rules for fct_sales\n",
    "fct_sales_rules = [\n",
    "    # Range: quantity must be positive\n",
    "    dataplex_v1.DataQualityRule(\n",
    "        column=\"quantity\",\n",
    "        dimension=\"VALIDITY\",\n",
    "        range_expectation=dataplex_v1.DataQualityRule.RangeExpectation(\n",
    "            min_value=\"0\",\n",
    "            strict_min_enabled=True,\n",
    "        ),\n",
    "        description=\"Order quantity must be greater than 0\",\n",
    "        threshold=1.0,\n",
    "    ),\n",
    "    # Range: gross_revenue must be non-negative\n",
    "    dataplex_v1.DataQualityRule(\n",
    "        column=\"gross_revenue\",\n",
    "        dimension=\"VALIDITY\",\n",
    "        range_expectation=dataplex_v1.DataQualityRule.RangeExpectation(\n",
    "            min_value=\"0\",\n",
    "        ),\n",
    "        description=\"Gross revenue must be >= 0\",\n",
    "        threshold=1.0,\n",
    "    ),\n",
    "    # Range: net_revenue must be non-negative\n",
    "    dataplex_v1.DataQualityRule(\n",
    "        column=\"net_revenue\",\n",
    "        dimension=\"VALIDITY\",\n",
    "        range_expectation=dataplex_v1.DataQualityRule.RangeExpectation(\n",
    "            min_value=\"0\",\n",
    "        ),\n",
    "        description=\"Net revenue must be >= 0\",\n",
    "        threshold=1.0,\n",
    "    ),\n",
    "    # Accuracy: net_revenue should equal gross_revenue - discount_amount (SqlAssertion returns FAILING rows)\n",
    "    dataplex_v1.DataQualityRule(\n",
    "        column=\"net_revenue\",\n",
    "        dimension=\"ACCURACY\",\n",
    "        sql_assertion=dataplex_v1.DataQualityRule.SqlAssertion(\n",
    "            sql_statement=f\"SELECT net_revenue, gross_revenue, discount_amount FROM `{PROJECT_ID}.{GOLD_DATASET}.fct_sales` WHERE ABS(net_revenue - (gross_revenue - discount_amount)) >= 0.01\"\n",
    "        ),\n",
    "        description=\"Net revenue must equal gross revenue minus discount (within rounding)\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "create_custom_dq_scan(\"fct_sales\", \"Sales fact table validation\", fct_sales_rules)\n",
    "\n",
    "print(\"\\nCustom DQ scans created with business validation rules!\")\n",
    "print(\"\\nExpected Failures:\")\n",
    "print(\"  - dim_customers: Uniqueness (duplicate customer_id=20)\")\n",
    "print(\"  - dim_customers: Completeness (NULL emails)\")\n",
    "print(\"  - dim_products: Set membership (lowercase categories)\")\n",
    "print(\"  - dim_date: Validity (future dates)\")\n",
    "print(\"  - fct_sales: (inherits issues from dimensions)\")\n",
    "print(\"\\nNext: Run these scans in Dataplex Console to see the actual failures!\")"
   ],
   "metadata": {
    "id": "nykd3cx135o",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759982591725,
     "user_tz": 240,
     "elapsed": 8495,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b7b5fc01-b651-488c-9613-c619b9e1dfbe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "o068k2hjj9b",
   "source": [
    "## Summary: Dataplex Scans Created\n",
    "\n",
    "You now have **8 Dataplex scans** configured on your Gold layer:\n",
    "\n",
    "### Data Profiling Scans (Phase 1 - Statistical Analysis)\n",
    "1. `profile-dim-customers` - Profiles all customer dimension columns\n",
    "2. `profile-dim-products` - Profiles all product dimension columns  \n",
    "3. `profile-dim-date` - Profiles all date dimension columns\n",
    "4. `profile-fct-sales` - Profiles all fact table columns\n",
    "\n",
    "### Data Quality Scans (Phase 2 - Business Validation)\n",
    "5. `custom-dq-dim-customers` - 4 validation rules (uniqueness, completeness, validity)\n",
    "6. `custom-dq-dim-products` - 4 validation rules (uniqueness, conformity, range)\n",
    "7. `custom-dq-dim-date` - 1 validation rule (validity)\n",
    "8. `custom-dq-fct-sales` - 4 validation rules (range, accuracy)\n",
    "\n",
    "### How to Run and View Results\n",
    "\n",
    "**Option 1: GCP Console **\n",
    "```\n",
    "1. Go to: https://console.cloud.google.com/dataplex/data-quality\n",
    "2. Select your project\n",
    "3. Click on each scan\n",
    "4. Click \"RUN NOW\" to execute\n",
    "5. View results after scan completes\n",
    "```\n",
    "\n",
    "**Option 2: Command Line**\n",
    "```bash\n",
    "# List all scans\n",
    "gcloud dataplex datascans list --location=us-central1\n",
    "\n",
    "# Run a specific profiling scan\n",
    "gcloud dataplex datascans run profile-dim-customers --location=us-central1\n",
    "\n",
    "# Run a specific DQ scan\n",
    "gcloud dataplex datascans run custom-dq-dim-customers --location=us-central1\n",
    "\n",
    "# Get scan results\n",
    "gcloud dataplex datascans describe profile-dim-customers --location=us-central1\n",
    "```\n",
    "\n",
    "**Option 3: Python API** (see next cell for example)"
   ],
   "metadata": {
    "id": "o068k2hjj9b"
   }
  },
  {
   "cell_type": "code",
   "id": "g5q4z2yk5s8",
   "source": [
    "# Optional: Run scans programmatically\n",
    "\n",
    "def run_data_scan(scan_id: str) -> Optional[dataplex_v1.RunDataScanResponse]:\n",
    "    \"\"\"Triggers a Dataplex scan to run.\"\"\"\n",
    "    name = f\"projects/{PROJECT_ID}/locations/{DATAPLEX_REGION}/dataScans/{scan_id}\"\n",
    "\n",
    "    request = dataplex_v1.RunDataScanRequest(name=name)\n",
    "\n",
    "    try:\n",
    "        response = dataplex_client.run_data_scan(request=request)\n",
    "        print(f\"Scan '{scan_id}' started. Job: {response.job.name}\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error running scan '{scan_id}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Running all scans programmatically\n",
    "print(\"Running all Data Profiling scans...\\n\")\n",
    "run_data_scan(\"profile-dim-customers\")\n",
    "run_data_scan(\"profile-dim-products\")\n",
    "run_data_scan(\"profile-dim-date\")\n",
    "run_data_scan(\"profile-fct-sales\")\n",
    "\n",
    "print(\"\\nRunning all Data Quality scans...\\n\")\n",
    "run_data_scan(\"custom-dq-dim-customers\")\n",
    "run_data_scan(\"custom-dq-dim-products\")\n",
    "run_data_scan(\"custom-dq-dim-date\")\n",
    "run_data_scan(\"custom-dq-fct-sales\")\n",
    "\n",
    "print(f\"\\nView results in the GCP Console:\")\n",
    "print(f\"https://console.cloud.google.com/dataplex/govern/data-profiling-and-quality?project={PROJECT_ID}\")"
   ],
   "metadata": {
    "id": "g5q4z2yk5s8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759982600293,
     "user_tz": 240,
     "elapsed": 649,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "25b1e23a-3179-4ab8-c9fc-557bf1e2fbeb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we're going to investigate the data quality failures using Dataplex and Lineage.  \n",
    "\n",
    "Open the Dataplex console using the URL above.  Click on the \"Custom DQ - dim_products\" scan, and view the results:\n",
    "\n",
    "![Dataplex scan](images/dq01.png)\n",
    "\n",
    "Copy the query to find the failed rows and paste it in to BigQuery:\n",
    "\n",
    "![Failed rows](images/dq02.png)\n",
    "\n",
    "The failed rows are all in the category column - next, let's use column-level lineage to figure out where they came from!\n",
    "\n",
    "**Note:** Lineage may take several (10+) minutes to appear.  Please move on to the next notebook in the mean time, and check back in 10 minutes for lineage results.\n",
    "\n",
    "![CLL](images/dq03.png)\n",
    "\n",
    "Finally, implement your own data quality checks in the bronze layer to detect these anomalies.  The cell below has the basic Python framework for creating them, or feel free to implement them via the UI, more information can be found here about creating them via the Dataplex UI."
   ],
   "metadata": {
    "id": "XbU16VTixOcJ"
   },
   "id": "XbU16VTixOcJ"
  },
  {
   "cell_type": "markdown",
   "id": "82c2hs3mwja",
   "source": [
    "---\n",
    "\n",
    "## Investigating Data Quality Issues"
   ],
   "metadata": {
    "id": "82c2hs3mwja"
   }
  },
  {
   "cell_type": "code",
   "id": "0nuhq8fbpdj",
   "source": [
    "# --- Bronze Layer Data Quality Checks ---\n",
    "# TODO: Complete this skeleton to create DQ scans for the bronze layer\n",
    "\n",
    "print(\"\\n--- Creating Data Quality Scans for Bronze Layer ---\\n\")\n",
    "\n",
    "# Hint: You traced the quality issues back to the bronze layer using lineage.\n",
    "# Now create scans to detect those issues at the source!\n",
    "\n",
    "# TODO: Create a scan for raw_customers\n",
    "# Hint: What issues did you find in dim_customers that came from raw_customers?\n",
    "# - Duplicate customer_id?\n",
    "# - NULL values in the JSON profile field?\n",
    "# - Check the profile JSON structure itself?\n",
    "\n",
    "bronze_customers_rules = [\n",
    "    # TODO: Add uniqueness check for customer_id\n",
    "    # dataplex_v1.DataQualityRule(\n",
    "    #     column=\"???\",\n",
    "    #     dimension=\"???\",\n",
    "    #     ???_expectation=dataplex_v1.DataQualityRule.???Expectation(),\n",
    "    #     description=\"???\",\n",
    "    #     threshold=???,\n",
    "    # ),\n",
    "\n",
    "    # TODO: Add completeness check for profile field\n",
    "    # Hint: Can profile be NULL?\n",
    "\n",
    "    # TODO: Add validation for JSON structure\n",
    "    # Hint: Use SqlAssertion to check if JSON_VALUE can extract expected fields\n",
    "]\n",
    "\n",
    "# create_custom_dq_scan(\"raw_customers\", \"Bronze customer data validation\", bronze_customers_rules)\n",
    "\n",
    "\n",
    "# TODO: Create a scan for raw_products\n",
    "# Hint: What issues did you find in dim_products that came from raw_products?\n",
    "# - Category casing inconsistency?\n",
    "# - Invalid category values?\n",
    "# - Price validation?\n",
    "\n",
    "bronze_products_rules = [\n",
    "    # TODO: Add category conformity check\n",
    "    # Hint: Should you check for the exact approved list, or something else at bronze layer?\n",
    "\n",
    "    # TODO: Add price range validation\n",
    "    # Hint: Prices should be positive, costs should be less than prices\n",
    "\n",
    "    # TODO: What about product_name or SKU format?\n",
    "]\n",
    "\n",
    "# create_custom_dq_scan(\"raw_products\", \"Bronze product data validation\", bronze_products_rules)\n",
    "\n",
    "\n",
    "# TODO: Create a scan for raw_orders\n",
    "# Hint: What issues did you find in dim_date/fct_sales that came from raw_orders?\n",
    "# - Future order_timestamp?\n",
    "# - Invalid discount_percent range?\n",
    "# - Negative shipping_cost?\n",
    "\n",
    "bronze_orders_rules = [\n",
    "    # TODO: Add timestamp validation\n",
    "    # Hint: Use SqlAssertion to find future timestamps\n",
    "\n",
    "    # TODO: Add discount_percent range check\n",
    "    # Hint: Should be between 0 and 1 (or 0% to 100%)\n",
    "\n",
    "    # TODO: Add shipping_cost validation\n",
    "]\n",
    "\n",
    "# create_custom_dq_scan(\"raw_orders\", \"Bronze order data validation\", bronze_orders_rules)\n",
    "\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review the data quality failures you found in the Gold layer\")\n",
    "print(\"2. Trace them back to Bronze using column-level lineage\")\n",
    "print(\"3. Complete the rules above to catch issues at the source\")\n",
    "print(\"4. Can we implement data scans on BigLake tables?  What about BQ Omni tables?\")\n",
    "print(\"5. Consider: where should we be implementing these checks? Bronze? Silver? Why?\")\n",
    "print(\"6. Think about: How would you handle these issues - reject, quarantine, or alert/flag?\")\n"
   ],
   "metadata": {
    "id": "0nuhq8fbpdj",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1759982334389,
     "user_tz": 240,
     "elapsed": 7,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9n3e9tw29b",
   "source": [
    "## Cleanup\n",
    "\n",
    "The following cell will delete all resources created in this notebook:\n",
    "- BigQuery datasets (bronze, silver, gold) and all their tables\n",
    "- GCS bucket and all its contents\n",
    "- BigQuery connection\n",
    "\n",
    "** **WARNING**:** This action is irreversible. Uncomment and run only when you want to completely remove all resources."
   ],
   "metadata": {
    "id": "9n3e9tw29b"
   }
  },
  {
   "cell_type": "markdown",
   "id": "rk3ikxra0br",
   "source": [
    "---\n"
   ],
   "metadata": {
    "id": "rk3ikxra0br"
   }
  },
  {
   "cell_type": "code",
   "id": "ppbv8qskpt",
   "source": [
    "def cleanup_dataplex_scans() -> None:\n",
    "    \"\"\"Deletes all Dataplex scans created by this notebook.\"\"\"\n",
    "    print(\"\\n--- Cleaning up Dataplex Scans ---\")\n",
    "\n",
    "    scan_ids = [\n",
    "        # Data Profiling scans\n",
    "        \"profile-dim-customers\",\n",
    "        \"profile-dim-products\",\n",
    "        \"profile-dim-date\",\n",
    "        \"profile-fct-sales\",\n",
    "        # Data Quality scans\n",
    "        \"custom-dq-dim-customers\",\n",
    "        \"custom-dq-dim-products\",\n",
    "        \"custom-dq-dim-date\",\n",
    "        \"custom-dq-fct-sales\",\n",
    "    ]\n",
    "\n",
    "    for scan_id in scan_ids:\n",
    "        try:\n",
    "            name = f\"projects/{PROJECT_ID}/locations/{DATAPLEX_REGION}/dataScans/{scan_id}\"\n",
    "            request = dataplex_v1.DeleteDataScanRequest(name=name)\n",
    "            dataplex_client.delete_data_scan(request=request)\n",
    "            print(f\"Deleted Dataplex scan: {scan_id}\")\n",
    "        except Exception as e:\n",
    "            if \"NOT_FOUND\" in str(e) or \"not found\" in str(e).lower():\n",
    "                print(f\"Scan not found (already deleted): {scan_id}\")\n",
    "            else:\n",
    "                print(f\"Error deleting scan '{scan_id}': {e}\")\n",
    "\n",
    "# Uncomment to delete all resources including Dataplex scans\n",
    "# cleanup_dataplex_scans()\n",
    "# cleanup_all_resources(bq_client, storage_client)\n",
    "\n",
    "# Or selectively delete resources:\n",
    "# cleanup_all_resources(bq_client, storage_client,\n",
    "#                       delete_datasets=True,\n",
    "#                       delete_bucket=True,\n",
    "#                       delete_connection=False)"
   ],
   "metadata": {
    "id": "ppbv8qskpt",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1759982334389,
     "user_tz": 240,
     "elapsed": 7,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "colab": {
   "provenance": [],
   "name": "01_load_data"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}