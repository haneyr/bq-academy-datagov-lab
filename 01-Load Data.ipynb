{
  "cells": [
    {
      "cell_type": "code",
      "id": "gbeCgy1Mtrn8l696erocgEcE",
      "metadata": {
        "tags": [],
        "id": "gbeCgy1Mtrn8l696erocgEcE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758212275765,
          "user_tz": 240,
          "elapsed": 28191,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "9b1d1d94-a665-406b-874a-b4e06eaf004d"
      },
      "source": [
        "import csv\n",
        "import io\n",
        "import time\n",
        "from datetime import datetime\n",
        "import random\n",
        "from google.cloud import bigquery, storage\n",
        "from google.cloud import bigquery_connection_v1 as bq_connection\n",
        "from google.api_core import exceptions\n",
        "from google.iam.v1 import iam_policy_pb2, policy_pb2\n",
        "\n",
        "# --- Configuration ---\n",
        "PROJECT_ID = \"bq-sme-governance-build\"\n",
        "DATASET_ID = \"sme_raw_layer\"\n",
        "# We can use us-west1, this is fine.\n",
        "LOCATION = \"us-west1\"\n",
        "# This is the ID for the BQ Connection resource\n",
        "CONNECTION_ID = \"bq-gcs-lab-connection\"\n",
        "\n",
        "# GCS Bucket for external table data.\n",
        "# !- UPDATE THIS to a globally unique name -!\n",
        "BUCKET_NAME = f\"{PROJECT_ID}-lab-data-source\"\n",
        "# GCS path for the new Iceberg table's data and metadata\n",
        "ICEBERG_STORAGE_URI = f\"gs://{BUCKET_NAME}/iceberg_metadata/orders\"\n",
        "# ---------------------\n",
        "\n",
        "# Initialize clients\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "connection_client = bq_connection.ConnectionServiceClient()\n",
        "\n",
        "# --- Mock Data Generation ---\n",
        "\n",
        "def get_mock_customers():\n",
        "    \"\"\"Generates mock data for the 'customers' table.\"\"\"\n",
        "    return [\n",
        "        {\"customer_id\": \"C1001\", \"first_name\": \"Alice\", \"last_name\": \"Smith\", \"email\": \"alice@example.com\", \"join_date\": \"2023-01-15\"},\n",
        "        {\"customer_id\": \"C1002\", \"first_name\": \"Bob\", \"last_name\": \"Johnson\", \"email\": \"bob@example.com\", \"join_date\": \"2023-02-10\"},\n",
        "        {\"customer_id\": \"C1003\", \"first_name\": \"Charlie\", \"last_name\": \"Brown\", \"email\": \"charlie@example.com\", \"join_date\": \"2023-03-05\"},\n",
        "        {\"customer_id\": \"C1004\", \"first_name\": \"David\", \"last_name\": \"Lee\", \"email\": \"david@example.com\", \"join_date\": \"2023-04-20\"},\n",
        "        {\"customer_id\": \"C1005\", \"first_name\": \"Eve\", \"last_name\": \"Davis\", \"email\": \"eve@example.com\", \"join_date\": \"2023-05-15\"},\n",
        "    ]\n",
        "\n",
        "def get_mock_products():\n",
        "    \"\"\"Generates mock data for the 'products' table.\"\"\"\n",
        "    return [\n",
        "        {\"product_id\": \"P2001\", \"product_name\": \"Laptop\", \"category\": \"Electronics\", \"unit_price\": 1200.00},\n",
        "        {\"product_id\": \"P2002\", \"product_name\": \"Mouse\", \"category\": \"Electronics\", \"unit_price\": 25.50},\n",
        "        {\"product_id\": \"P2003\", \"product_name\": \"Coffee Mug\", \"category\": \"Homeware\", \"unit_price\": 15.00},\n",
        "        {\"product_id\": \"P2004\", \"product_name\": \"Notebook\", \"category\": \"Stationery\", \"unit_price\": 5.75},\n",
        "        {\"product_id\": \"P9999\", \"product_name\": \"Test Item\", \"category\": \"UNKNOWN\", \"unit_price\": -1.00},\n",
        "    ]\n",
        "\n",
        "def get_mock_orders():\n",
        "    \"\"\"Generates mock data for 'orders' as a list of dicts.\"\"\"\n",
        "    return [\n",
        "        {\"order_id\": \"E101\", \"customer_id\": \"C1001\", \"order_date\": \"2024-05-01\", \"status\": \"Shipped\"},\n",
        "        {\"order_id\": \"E102\", \"customer_id\": \"C1002\", \"order_date\": \"2024-05-03\", \"status\": \"Processing\"},\n",
        "        {\"order_id\": \"E103\", \"customer_id\": \"C1001\", \"order_date\": \"2024-05-04\", \"status\": \"Shipped\"},\n",
        "        {\"order_id\": \"E104\", \"customer_id\": \"C1003\", \"order_date\": \"2024-05-05\", \"status\": \"Delivered\"},\n",
        "        {\"order_id\": \"E105\", \"customer_id\": \"C1004\", \"order_date\": \"2024-05-06\", \"status\": \"Shipped\"},\n",
        "        {\"order_id\": \"E106\", \"customer_id\": \"C9999\", \"order_date\": \"2024-05-07\", \"status\": \"Pending\"},\n",
        "    ]\n",
        "\n",
        "def get_mock_order_items_csv():\n",
        "    \"\"\"Generates mock data for 'order_items' as a CSV string.\"\"\"\n",
        "    data = [\n",
        "        [\"item_id\", \"order_id\", \"product_id\", \"quantity\"],\n",
        "        [\"OI301\", \"E101\", \"P2001\", 1],\n",
        "        [\"OI32\", \"E101\", \"P2002\", 1],\n",
        "        [\"OI303\", \"E102\", \"P2003\", 2],\n",
        "        [\"OI304\", \"E103\", \"P2004\", 5],\n",
        "        [\"OI305\", \"E104\", \"P2001\", 1],\n",
        "        [\"OI306\", \"E105\", \"P2003\", 1],\n",
        "        [\"OI307\", \"E106\", \"P9999\", 99],\n",
        "    ]\n",
        "    output = io.StringIO()\n",
        "    writer = csv.writer(output)\n",
        "    writer.writerows(data)\n",
        "    return output.getvalue()\n",
        "# --- End of Mock Data Generation ---\n",
        "\n",
        "\n",
        "# --- Cloud Resource Setup ---\n",
        "\n",
        "def ensure_gcs_bucket_exists():\n",
        "    \"\"\"Checks for GCS bucket and creates it if not found.\"\"\"\n",
        "    print(f\"Checking for GCS bucket: {BUCKET_NAME}...\")\n",
        "    try:\n",
        "        bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "        print(\"...bucket already exists.\")\n",
        "    except exceptions.NotFound:\n",
        "        print(\"...bucket not found, creating new bucket.\")\n",
        "        bucket = storage_client.create_bucket(BUCKET_NAME, location=LOCATION)\n",
        "        # Add a short delay to help with eventual consistency\n",
        "        print(f\"...created bucket {bucket.name} in {bucket.location}. Waiting 5s...\")\n",
        "        time.sleep(5)\n",
        "    return bucket\n",
        "\n",
        "def ensure_bq_dataset_exists():\n",
        "    \"\"\"Checks for BQ dataset and creates it if not found.\"\"\"\n",
        "    dataset_ref = bq_client.dataset(DATASET_ID)\n",
        "    print(f\"Checking for BigQuery dataset: {DATASET_ID}...\")\n",
        "    try:\n",
        "        bq_client.get_dataset(dataset_ref)\n",
        "        print(\"...dataset already exists.\")\n",
        "    except exceptions.NotFound:\n",
        "        print(\"...dataset not found, creating new dataset.\")\n",
        "        dataset = bigquery.Dataset(dataset_ref)\n",
        "        dataset.location = LOCATION\n",
        "        bq_client.create_dataset(dataset, timeout=30)\n",
        "        print(f\"...created dataset in {dataset.location}.\")\n",
        "\n",
        "def get_or_create_bq_connection():\n",
        "    \"\"\"Creates or gets a BQ Connection for GCS. Returns its service_account_id.\"\"\"\n",
        "    parent = f\"projects/{PROJECT_ID}/locations/{LOCATION}\"\n",
        "    connection_full_name = f\"{parent}/connections/{CONNECTION_ID}\"\n",
        "\n",
        "    print(f\"Checking for BQ Connection: {CONNECTION_ID}...\")\n",
        "    try:\n",
        "        # Check if connection exists\n",
        "        connection = connection_client.get_connection(\n",
        "            request=bq_connection.GetConnectionRequest(name=connection_full_name)\n",
        "        )\n",
        "        print(\"...connection already exists.\")\n",
        "    except exceptions.NotFound:\n",
        "        print(\"...connection not found, creating new connection.\")\n",
        "        # Create a connection for GCS (CLOUD_RESOURCE)\n",
        "        connection_request = bq_connection.CreateConnectionRequest(\n",
        "            parent=parent,\n",
        "            connection_id=CONNECTION_ID,\n",
        "            connection=bq_connection.Connection(\n",
        "                cloud_resource=bq_connection.CloudResourceProperties()\n",
        "            ),\n",
        "        )\n",
        "        connection = connection_client.create_connection(request=connection_request)\n",
        "        print(\"...connection created. Waiting 10s for SA to be created...\")\n",
        "        # Add a delay to allow the service account to be created before we try to use it\n",
        "        time.sleep(10)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: Could not get or create BQ Connection.\")\n",
        "        print(\"Please ensure the BigQuery Connection API is enabled.\")\n",
        "        raise e\n",
        "\n",
        "    # Extract the service account ID\n",
        "    if not connection.cloud_resource.service_account_id:\n",
        "        print(\"...ERROR: Connection created but service_account_id is missing.\")\n",
        "        raise ValueError(\"Connection service account ID is empty\")\n",
        "\n",
        "    print(f\"...Connection Service Account: {connection.cloud_resource.service_account_id}\")\n",
        "    return connection.name, connection.cloud_resource.service_account_id\n",
        "\n",
        "def grant_gcs_permissions_to_sa(bucket_name, service_account_email, max_retries=5, delay_seconds=10):\n",
        "    \"\"\"\n",
        "    Grants Storage Object Admin role to the connection's service account.\n",
        "    Includes retry logic to handle IAM propagation delay.\n",
        "    \"\"\"\n",
        "    print(f\"Granting GCS permissions to: {service_account_email}...\")\n",
        "\n",
        "    try:\n",
        "        bucket = storage_client.get_bucket(bucket_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to get bucket {bucket_name}: {e}\")\n",
        "        raise\n",
        "\n",
        "    role = \"roles/storage.objectAdmin\"\n",
        "    member = f\"serviceAccount:{service_account_email}\"\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            policy = bucket.get_iam_policy(requested_policy_version=3)\n",
        "\n",
        "            # Check if the binding already exists\n",
        "            binding_exists = False\n",
        "            for binding in policy.bindings:\n",
        "                if binding[\"role\"] == role and member in binding[\"members\"]:\n",
        "                    print(\"...IAM binding already exists.\")\n",
        "                    binding_exists = True\n",
        "                    break\n",
        "\n",
        "            if binding_exists:\n",
        "                return # Already done, success!\n",
        "\n",
        "            # Add the new binding to the policy if it doesn't exist\n",
        "            print(\"...IAM binding not found, adding it.\")\n",
        "            policy.bindings.append({\"role\": role, \"members\": {member}})\n",
        "\n",
        "            bucket.set_iam_policy(policy)\n",
        "            print(f\"...Successfully granted role {role} on bucket {bucket_name}.\")\n",
        "            return # Success\n",
        "\n",
        "        except exceptions.BadRequest as e:\n",
        "            # This is the error GCS API returns for a non-existent SA\n",
        "            if \"does not exist\" in str(e).lower():\n",
        "                print(f\"...Attempt {attempt + 1} of {max_retries} failed: IAM service account not yet propagated.\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    print(f\"...Waiting {delay_seconds}s for IAM propagation...\")\n",
        "                    time.sleep(delay_seconds)\n",
        "                else:\n",
        "                    print(\"...Max retries reached. IAM grant failed.\")\n",
        "                    raise e\n",
        "            else:\n",
        "                # Some other bad request\n",
        "                print(f\"Error granting IAM permissions to service account: {e}\")\n",
        "                raise e\n",
        "        except Exception as e:\n",
        "            # Other unexpected errors\n",
        "            print(f\"An unexpected error occurred during IAM grant (Attempt {attempt + 1}): {e}\")\n",
        "            if attempt >= max_retries - 1:\n",
        "                raise e # Re-raise last error\n",
        "            time.sleep(delay_seconds)\n",
        "\n",
        "\n",
        "def upload_to_gcs(bucket, blob_name, data, content_type, max_retries=3, delay_seconds=5):\n",
        "    \"\"\"\n",
        "    Uploads a string or bytes as a file to GCS.\n",
        "    Includes retries to handle eventual consistency on bucket creation.\n",
        "    \"\"\"\n",
        "    print(f\"Uploading {blob_name} to GCS bucket {bucket.name}...\")\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            blob = bucket.blob(blob_name)\n",
        "            blob.upload_from_string(data, content_type=content_type)\n",
        "            print(\"...upload complete.\")\n",
        "            return f\"gs://{bucket.name}/{blob_name}\"\n",
        "        except exceptions.NotFound as e:\n",
        "            print(f\"...Attempt {attempt + 1} of {max_retries} failed (404 NotFound).\")\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"...Waiting {delay_seconds}s for GCS to propagate bucket creation...\")\n",
        "                time.sleep(delay_seconds)\n",
        "            else:\n",
        "                print(\"...Max retries reached. Upload failed.\")\n",
        "                raise e\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during upload: {e}\")\n",
        "            raise e\n",
        "\n",
        "# --- BigQuery Table Creation ---\n",
        "\n",
        "def load_table_from_memory(table_id, data, schema):\n",
        "    \"\"\"Loads data from a list of dicts into a new BQ table.\"\"\"\n",
        "    full_table_id = f\"{PROJECT_ID}.{DATASET_ID}.{table_id}\"\n",
        "    print(f\"Starting load job for table: {full_table_id}...\")\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        load_job = bq_client.load_table_from_json(\n",
        "            data, full_table_id, job_config=job_config\n",
        "        )\n",
        "        load_job.result()\n",
        "        print(\"...load job finished.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading table {full_table_id}: {e}\")\n",
        "\n",
        "def create_and_load_external_iceberg_table(table_id, data, schema, connection_name, storage_uri):\n",
        "    \"\"\"Creates an External Iceberg table (BigLake) and inserts data.\"\"\"\n",
        "    full_table_id_sql_safe = f\"`{PROJECT_ID}`.`{DATASET_ID}`.`{table_id}`\"\n",
        "    print(f\"Creating External Iceberg table: {full_table_id_sql_safe}...\")\n",
        "\n",
        "    try:\n",
        "        # 1. Generate the DDL for the Iceberg table definition\n",
        "        schema_sql_parts = []\n",
        "        for field in schema:\n",
        "            field_sql = f\"`{field.name}` {field.field_type}\"\n",
        "            if field.mode == \"REQUIRED\":\n",
        "                field_sql += \" NOT NULL\"\n",
        "            schema_sql_parts.append(field_sql)\n",
        "        schema_sql = \", \".join(schema_sql_parts)\n",
        "\n",
        "        # This DDL query creates the managed Iceberg table explicitly\n",
        "        ddl_query = f\"\"\"\n",
        "        CREATE OR REPLACE TABLE {full_table_id_sql_safe}\n",
        "        (\n",
        "            {schema_sql}\n",
        "        )\n",
        "        WITH CONNECTION `{connection_name}`\n",
        "        OPTIONS(\n",
        "            table_format = 'ICEBERG',\n",
        "            file_format = 'PARQUET',\n",
        "            storage_uri = '{storage_uri}'\n",
        "        )\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"...executing DDL to create table definition:\")\n",
        "\n",
        "        # 2. Execute the DDL query to create the table structure\n",
        "        query_job = bq_client.query(ddl_query)\n",
        "        query_job.result()\n",
        "        print(\"...External Iceberg table definition created.\")\n",
        "\n",
        "        # 3. Load data using an INSERT INTO ... VALUES ... query\n",
        "        print(\"...preparing INSERT query to load data.\")\n",
        "        rows_to_insert = []\n",
        "        for row in data:\n",
        "            # Format values for SQL (e.g., strings in quotes, dates in quotes)\n",
        "            values = [\n",
        "                f\"'{row['order_id']}'\",\n",
        "                f\"'{row['customer_id']}'\",\n",
        "                f\"DATE('{row['order_date']}')\",\n",
        "                f\"'{row['status']}'\"\n",
        "            ]\n",
        "            rows_to_insert.append(f\"({', '.join(values)})\")\n",
        "\n",
        "        insert_query = f\"\"\"\n",
        "        INSERT INTO {full_table_id_sql_safe}\n",
        "        (order_id, customer_id, order_date, status)\n",
        "        VALUES {', '.join(rows_to_insert)}\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"...executing INSERT query.\")\n",
        "        query_job = bq_client.query(insert_query)\n",
        "        query_job.result()  # Wait for the insert to complete\n",
        "        print(\"...data loaded into Iceberg table.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating or loading Iceberg table {full_table_id_sql_safe}: {e}\")\n",
        "        raise e\n",
        "\n",
        "\n",
        "def create_external_table(table_id, schema, gcs_uri, format=\"CSV\"):\n",
        "    \"\"\"Creates a new BQ external table pointing to a GCS file (CSV).\"\"\"\n",
        "    full_table_id = f\"{PROJECT_ID}.{DATASET_ID}.{table_id}\"\n",
        "    print(f\"Creating external {format} table: {full_table_id}...\")\n",
        "\n",
        "    if format == \"CSV\":\n",
        "        external_config = bigquery.ExternalConfig(\"CSV\")\n",
        "        external_config.source_uris = [gcs_uri]\n",
        "        external_config.schema = schema\n",
        "        external_config.csv_options.skip_leading_rows = 1\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported external format: {format}\")\n",
        "\n",
        "    table = bigquery.Table(full_table_id)\n",
        "    table.external_data_configuration = external_config\n",
        "\n",
        "    try:\n",
        "        bq_client.delete_table(full_table_id, not_found_ok=True)\n",
        "        print(f\"...deleted existing table (if any).\")\n",
        "        bq_client.create_table(table)\n",
        "        print(\"...external table created.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating external table {full_table_id}: {e}\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "def main():\n",
        "    print(f\"Starting data setup for project: {PROJECT_ID}\\n\")\n",
        "\n",
        "    try:\n",
        "        # 1. Create GCS and BQ resources\n",
        "        bucket = ensure_gcs_bucket_exists()\n",
        "        ensure_bq_dataset_exists()\n",
        "\n",
        "        # 2. Create BQ Connection and grant it permissions on the bucket\n",
        "        connection_name, sa_email = get_or_create_bq_connection()\n",
        "        grant_gcs_permissions_to_sa(bucket.name, sa_email)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create prerequisite cloud resources: {e}\")\n",
        "        print(\"Please check your permissions and project configuration.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- Step 1: Handling Internal Tables (Load Jobs) ---\")\n",
        "\n",
        "    # 3. 'customers' (Standard Table)\n",
        "    customers_schema = [\n",
        "        bigquery.SchemaField(\"customer_id\", \"STRING\", \"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"first_name\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"last_name\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"email\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"join_date\", \"DATE\"),\n",
        "    ]\n",
        "    customers_data = get_mock_customers()\n",
        "    load_table_from_memory(\"customers\", customers_data, customers_schema)\n",
        "\n",
        "    # 4. 'products' (Standard Table)\n",
        "    products_schema = [\n",
        "        bigquery.SchemaField(\"product_id\", \"STRING\", \"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"product_name\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"category\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"unit_price\", \"FLOAT\"),\n",
        "    ]\n",
        "    products_data = get_mock_products()\n",
        "    load_table_from_memory(\"products\", products_data, products_schema)\n",
        "\n",
        "    print(\"\\n--- Step 2: Handling External & BigLake Tables ---\")\n",
        "\n",
        "    # 5. 'orders' (External Iceberg Table via BigLake)\n",
        "    orders_schema = [\n",
        "        bigquery.SchemaField(\"order_id\", \"STRING\", \"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"customer_id\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"order_date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"status\", \"STRING\"),\n",
        "    ]\n",
        "    orders_data = get_mock_orders()\n",
        "    # Pass the connection name and storage URI to the creation function\n",
        "    create_and_load_external_iceberg_table(\n",
        "        \"orders\",\n",
        "        orders_data,\n",
        "        orders_schema,\n",
        "        connection_name,\n",
        "        ICEBERG_STORAGE_URI\n",
        "    )\n",
        "\n",
        "    # 6. Upload 'order_items' data to GCS\n",
        "    order_items_csv_data = get_mock_order_items_csv()\n",
        "    order_items_gcs_uri = upload_to_gcs(\n",
        "        bucket,\n",
        "        \"raw/order_items/order_items.csv\",\n",
        "        order_items_csv_data,\n",
        "        content_type=\"text/csv\"\n",
        "    )\n",
        "\n",
        "    # 7. Create 'order_items' external CSV table\n",
        "    order_items_schema = [\n",
        "        bigquery.SchemaField(\"item_id\", \"STRING\", \"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"order_id\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"product_id\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"quantity\", \"INTEGER\"),\n",
        "    ]\n",
        "    create_external_table(\"order_items\", order_items_schema, order_items_gcs_uri, format=\"CSV\")\n",
        "\n",
        "    print(\"\\n--- Setup Complete! ---\")\n",
        "    print(f\"Project: {PROJECT_ID}\")\n",
        "    print(f\"Dataset: {DATASET_ID}\")\n",
        "    print(\"Tables created:\")\n",
        "    print(\" - customers (Standard BQ Table)\")\n",
        "    print(\" - products (Standard BQ Table)\")\n",
        "    print(\" - orders (External Iceberg/BigLake Table)\")\n",
        "    print(\" - order_items (External CSV Table)\")\n",
        "\n",
        "# --- Cleanup Function ---\n",
        "\n",
        "def cleanup_resources():\n",
        "    \"\"\"\n",
        "    Deletes all created resources for a clean teardown.\n",
        "    WARNING: This is destructive and irreversible.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- STARTING RESOURCE CLEANUP ---\")\n",
        "\n",
        "    # 1. Delete the GCS Bucket and its contents\n",
        "    print(f\"Attempting to delete GCS Bucket: {BUCKET_NAME}...\")\n",
        "    try:\n",
        "        bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "        print(\"...deleting all blobs in bucket...\")\n",
        "        # This will also delete the Iceberg metadata/data files\n",
        "        blobs = list(bucket.list_blobs(force=True))\n",
        "        for blob in blobs:\n",
        "            try:\n",
        "                blob.delete()\n",
        "            except Exception as e:\n",
        "                print(f\"...warning: could not delete blob {blob.name}: {e}\")\n",
        "        print(f\"...deleted {len(blobs)} blobs.\")\n",
        "\n",
        "        # Wait a moment for deletes to propagate\n",
        "        time.sleep(2)\n",
        "\n",
        "        bucket.delete(force=True)\n",
        "        print(f\"...bucket {BUCKET_NAME} deleted successfully.\")\n",
        "    except exceptions.NotFound:\n",
        "        print(f\"...bucket {BUCKET_NAME} not found, skipping.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting bucket {BUCKET_NAME}: {e}\")\n",
        "\n",
        "    # 2. Delete the BigQuery Dataset and its contents\n",
        "    # This will also delete the 'orders' table definition\n",
        "    print(f\"Attempting to delete BigQuery Dataset: {DATASET_ID}...\")\n",
        "    try:\n",
        "        bq_client.delete_dataset(\n",
        "            DATASET_ID, delete_contents=True, not_found_ok=True\n",
        "        )\n",
        "        print(f\"...dataset {DATASET_ID} deleted successfully.\")\n",
        "    except exceptions.NotFound:\n",
        "        print(f\"...dataset {DATASET_ID} not found, skipping.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting dataset {DATASET_ID}: {e}\")\n",
        "\n",
        "    # 3. Delete the BQ Connection\n",
        "    parent = f\"projects/{PROJECT_ID}/locations/{LOCATION}\"\n",
        "    connection_full_name = f\"{parent}/connections/{CONNECTION_ID}\"\n",
        "    print(f\"Attempting to delete BQ Connection: {CONNECTION_ID}...\")\n",
        "    try:\n",
        "        connection_client.delete_connection(\n",
        "            request=bq_connection.DeleteConnectionRequest(name=connection_full_name)\n",
        "        )\n",
        "        print(f\"...connection {CONNECTION_ID} deleted successfully.\")\n",
        "    except exceptions.NotFound:\n",
        "        print(f\"...connection {CONNECTION_ID} not found, skipping.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting connection {CONNECTION_ID}: {e}\")\n",
        "\n",
        "    print(\"--- CLEANUP COMPLETE ---\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Main execution ---\n",
        "    main()\n",
        "\n",
        "    # --- To run cleanup ---\n",
        "    #cleanup_resources()\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data setup for project: bq-sme-governance-build\n",
            "\n",
            "Checking for GCS bucket: bq-sme-governance-build-lab-data-source...\n",
            "...bucket already exists.\n",
            "Checking for BigQuery dataset: sme_raw_layer...\n",
            "...dataset not found, creating new dataset.\n",
            "...created dataset in us-west1.\n",
            "Checking for BQ Connection: bq-gcs-lab-connection...\n",
            "...connection not found, creating new connection.\n",
            "...connection created. Waiting 10s for SA to be created...\n",
            "...Connection Service Account: bqcx-184517388310-8nih@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
            "Granting GCS permissions to: bqcx-184517388310-8nih@gcp-sa-bigquery-condel.iam.gserviceaccount.com...\n",
            "...IAM binding not found, adding it.\n",
            "...Successfully granted role roles/storage.objectAdmin on bucket bq-sme-governance-build-lab-data-source.\n",
            "\n",
            "--- Step 1: Handling Internal Tables (Load Jobs) ---\n",
            "Starting load job for table: bq-sme-governance-build.sme_raw_layer.customers...\n",
            "...load job finished.\n",
            "Starting load job for table: bq-sme-governance-build.sme_raw_layer.products...\n",
            "...load job finished.\n",
            "\n",
            "--- Step 2: Handling External & BigLake Tables ---\n",
            "Creating External Iceberg table: `bq-sme-governance-build`.`sme_raw_layer`.`orders`...\n",
            "...executing DDL to create table definition:\n",
            "...External Iceberg table definition created.\n",
            "...preparing INSERT query to load data.\n",
            "...executing INSERT query.\n",
            "...data loaded into Iceberg table.\n",
            "Uploading raw/order_items/order_items.csv to GCS bucket bq-sme-governance-build-lab-data-source...\n",
            "...upload complete.\n",
            "Creating external CSV table: bq-sme-governance-build.sme_raw_layer.order_items...\n",
            "...deleted existing table (if any).\n",
            "...external table created.\n",
            "\n",
            "--- Setup Complete! ---\n",
            "Project: bq-sme-governance-build\n",
            "Dataset: sme_raw_layer\n",
            "Tables created:\n",
            " - customers (Standard BQ Table)\n",
            " - products (Standard BQ Table)\n",
            " - orders (External Iceberg/BigLake Table)\n",
            " - order_items (External CSV Table)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}