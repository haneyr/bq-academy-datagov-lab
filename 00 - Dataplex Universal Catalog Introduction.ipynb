{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "vQ8yIyedT5LE",
      "metadata": {
        "id": "vQ8yIyedT5LE"
      },
      "source": [
        "# Dataplex Universal Catalog Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68Ix5_V34acm",
      "metadata": {
        "id": "68Ix5_V34acm"
      },
      "source": [
        "**Overview**: The purpose of this workshop is to show how to use the Dataplex Universal Catalog.\n",
        "\n",
        "This notebook will generate synthetic data using the 'faker' python package in a CSV file, load this data to GCS and then to BigQuery.  Also, custom aspects will be applied to the entry in the catalog.   Finally, the notebook will demonstrate using various search techniques with the Catalog.\n",
        "\n",
        "**Process Flow:**\n",
        "\n",
        "1.  Create CSV File using Faker.\n",
        "2.  Load CSV File to GCS.\n",
        "3.  Load GCS CSV File to BigQuery Table\n",
        "4.  Create and apply aspects to BigQuery Table\n",
        "5.  Search using the Dataplex Universal Catalog UI\n",
        "\n",
        "Notes:\n",
        "* TBD.\n",
        "\n",
        "Cost:\n",
        "* Approximate cost: Less than 1 dollar\n",
        "\n",
        "Author:\n",
        "* Jay O'Leary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R9aJyJxvxYqm",
      "metadata": {
        "id": "R9aJyJxvxYqm"
      },
      "source": [
        "# License"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jCHsn1Z3zUFW",
      "metadata": {
        "id": "jCHsn1Z3zUFW"
      },
      "source": [
        "```\n",
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iIfTfYrtn9Tt",
      "metadata": {
        "id": "iIfTfYrtn9Tt"
      },
      "source": [
        "# Prerequisites\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tYzqd6D5_MMl",
      "metadata": {
        "id": "tYzqd6D5_MMl"
      },
      "source": [
        "- See [Prerequisites](./README.md#prerequisites)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb7d153b",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4a57BiDBw0Gj",
      "metadata": {
        "id": "4a57BiDBw0Gj"
      },
      "source": [
        "# Pip Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ehnv3axb9o8AwTI8ZUnVlubV",
      "metadata": {
        "id": "Ehnv3axb9o8AwTI8ZUnVlubV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install Faker"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OcO3681Fzjbm",
      "metadata": {
        "id": "OcO3681Fzjbm"
      },
      "source": [
        "# Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XA43W-DHzrOa",
      "metadata": {
        "id": "XA43W-DHzrOa"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "import subprocess\n",
        "import json\n",
        "import csv\n",
        "from faker import Faker\n",
        "import uuid\n",
        "\n",
        "#Declare some variables\n",
        "GCP_PROJECT_ID  = ! gcloud config get project\n",
        "GCP_PROJECT_ID  = GCP_PROJECT_ID[0]\n",
        "PROJECT_NUMBER  = !gcloud projects describe {GCP_PROJECT_ID} | grep projectNumber | cut -d':' -f2 |  tr -d \"'\" | xargs\n",
        "PROJECT_NUMBER  = PROJECT_NUMBER[0]\n",
        "GCP_LOCATION    = \"us-central1\"\n",
        "DATASET_NAME    = \"openlineage_demo\"\n",
        "TABLE_NAME      = \"csv2bq\"\n",
        "BUCKET_NAME     = f\"openlineage-demo-{PROJECT_NUMBER}\"\n",
        "# Dataplex Data Lineage API endpoint\n",
        "DATAPLEX_API_URL=f\"https://datalineage.googleapis.com/v1/projects/{GCP_PROJECT_ID}/locations/{GCP_LOCATION}:processOpenLineageRunEvent\"\n",
        "# An OpenLineage namespace for your jobs\n",
        "OPENLINEAGE_NAMESPACE=\"bash-scripts-dataplex\"\n",
        "# A unique run ID for this job execution\n",
        "RUN_ID=uuid.uuid4()\n",
        "# The name of the fake data file\n",
        "DATA_FILE=f\"faux_data_{RUN_ID}.csv\"\n",
        "# The Name of the custom entry group\n",
        "ENTRY_GROUP_NAME=\"lineage-test-entry-group\"\n",
        "# The Name of the custom entry\n",
        "ENTRY_NAME=\"lineage-test-entry\"\n",
        "# The Name of the custom entry type\n",
        "ENTRY_NAME_TYPE=\"lineage-test-entry-type\"\n",
        "DATASET_SQL = f\"create schema {DATASET_NAME}\"\n",
        "VPC_NAME=\"default\"\n",
        "\n",
        "storage_client = storage.Client(project=GCP_PROJECT_ID)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H8dLCjhcx8A4",
      "metadata": {
        "id": "H8dLCjhcx8A4"
      },
      "source": [
        "# Helper Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CgjPMu1mfTEJ",
      "metadata": {
        "id": "CgjPMu1mfTEJ"
      },
      "outputs": [],
      "source": [
        "def restAPIHelper(url: str, http_verb: str, request_body: str) -> str:\n",
        "  \"\"\"Calls the Google Cloud REST API passing in the current users credentials\"\"\"\n",
        "\n",
        "  import google.auth.transport.requests\n",
        "  import requests\n",
        "  import google.auth\n",
        "  import json\n",
        "\n",
        "  # Get an access token based upon the current user\n",
        "  creds, project = google.auth.default()\n",
        "  auth_req = google.auth.transport.requests.Request()\n",
        "  creds.refresh(auth_req)\n",
        "  access_token=creds.token\n",
        "\n",
        "  headers = {\n",
        "    \"Content-Type\" : \"application/json\",\n",
        "    \"Authorization\" : \"Bearer \" + access_token\n",
        "  }\n",
        "\n",
        "  if http_verb == \"GET\":\n",
        "    response = requests.get(url, headers=headers)\n",
        "  elif http_verb == \"POST\":\n",
        "    response = requests.post(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"PUT\":\n",
        "    response = requests.put(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"PATCH\":\n",
        "    response = requests.patch(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"DELETE\":\n",
        "    response = requests.delete(url, headers=headers)\n",
        "  else:\n",
        "    raise RuntimeError(f\"Unknown HTTP verb: {http_verb}\")\n",
        "\n",
        "  if response.status_code == 200:\n",
        "    return json.loads(response.content)\n",
        "    #image_data = json.loads(response.content)[\"predictions\"][0][\"bytesBase64Encoded\"]\n",
        "  else:\n",
        "    error = f\"Error restAPIHelper -> ' Status: '{response.status_code}' Text: '{response.text}'\"\n",
        "    raise RuntimeError(error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NIyPcjQp5Muf",
      "metadata": {
        "id": "NIyPcjQp5Muf"
      },
      "outputs": [],
      "source": [
        "#create bucket if it does not exist\n",
        "def create_storage_bucket():\n",
        "  buckets = storage_client.list_buckets()\n",
        "  bucket_names = [bucket.name for bucket in buckets]\n",
        "\n",
        "  bucket = storage_client.bucket(BUCKET_NAME)\n",
        "\n",
        "  if not bucket.exists():\n",
        "      try:\n",
        "          bucket = storage_client.create_bucket(BUCKET_NAME)\n",
        "          print(f\"Bucket {bucket.name} created.\")\n",
        "      except Exception as e:\n",
        "          print(f\"Error creating bucket: {e}\")\n",
        "  else:\n",
        "      print(f\"Bucket {BUCKET_NAME} already exists.\")\n",
        "\n",
        "create_storage_bucket()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "huk4QNni-HxB",
      "metadata": {
        "id": "huk4QNni-HxB"
      },
      "outputs": [],
      "source": [
        "def create_entry():\n",
        "  #create dataplex universal catalog entry group\n",
        "  !gcloud dataplex entry-groups create {ENTRY_GROUP_NAME} --location={GCP_LOCATION}\n",
        "\n",
        "  #create dataplex universal catalog entry type\n",
        "  !gcloud dataplex entry-types create {ENTRY_NAME_TYPE} --location={GCP_LOCATION}\n",
        "\n",
        "  #create dataplex universal catalog entry\n",
        "  cmd=f\"\"\"\n",
        "  gcloud dataplex entries create \"{ENTRY_NAME}-{RUN_ID}\" \\\n",
        "              --location={GCP_LOCATION} \\\n",
        "              --entry-group={ENTRY_GROUP_NAME} \\\n",
        "              --entry-type=projects/{GCP_PROJECT_ID}/locations/{GCP_LOCATION}/entryTypes/{ENTRY_NAME_TYPE} \\\n",
        "              --fully-qualified-name=\"gcs:{BUCKET_NAME}.\\`{DATA_FILE}\\`\"\n",
        "  \"\"\"\n",
        "  !{cmd}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Omwiqf-T_nw_",
      "metadata": {
        "id": "Omwiqf-T_nw_"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3pCqlu422Jrj",
      "metadata": {
        "id": "3pCqlu422Jrj"
      },
      "source": [
        "# Create CSV & Create Dataplex Universal Catalog Entry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-_JdYX4J5kYf",
      "metadata": {
        "id": "-_JdYX4J5kYf"
      },
      "outputs": [],
      "source": [
        "#create synthetic data\n",
        "import csv\n",
        "from faker import Faker\n",
        "\n",
        "# 1. Initialize Faker\n",
        "# Create a Faker instance, optionally setting a locale (e.g., 'en_US')\n",
        "fake = Faker('en_US')\n",
        "# Optional: Set a seed for reproducible data generation\n",
        "Faker.seed(42)\n",
        "\n",
        "# --- Configuration ---\n",
        "NUM_RECORDS = 100\n",
        "FIELDNAMES = ['id', 'first_name', 'last_name', 'email', 'date_of_birth', 'address']\n",
        "OUTPUT_FILE = 'synthetic_users.csv'\n",
        "\n",
        "# --- Data Generation Function ---\n",
        "def create_fake_user(user_id):\n",
        "    \"\"\"Generates a single dictionary representing a synthetic user record.\"\"\"\n",
        "    return {\n",
        "        'id': user_id,\n",
        "        'first_name': fake.first_name(),\n",
        "        'last_name': fake.last_name(),\n",
        "        'email': fake.email(),\n",
        "        'date_of_birth': fake.date_of_birth(minimum_age=18, maximum_age=65),\n",
        "        'address': fake.street_address() + ', ' + fake.city() + ', ' + fake.postcode()\n",
        "    }\n",
        "\n",
        "# --- Write to CSV ---\n",
        "print(f\"Generating {NUM_RECORDS} records and writing to {DATA_FILE}...\")\n",
        "\n",
        "with open(DATA_FILE, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    # Create a DictWriter object\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=FIELDNAMES)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writeheader()\n",
        "\n",
        "    # Write the data rows\n",
        "    for i in range(1, NUM_RECORDS + 1):\n",
        "        writer.writerow(create_fake_user(i))\n",
        "\n",
        "print(f\"CSV file '{DATA_FILE}' successfully created.\")\n",
        "\n",
        "#load csv file to gcs storage\n",
        "storage_client.bucket(BUCKET_NAME).blob(DATA_FILE).upload_from_filename(DATA_FILE)\n",
        "print(f\"CSV file '{DATA_FILE}' loaded to GCS Bucket '{BUCKET_NAME}'.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5NTH7NkB4KNz",
      "metadata": {
        "id": "5NTH7NkB4KNz"
      },
      "outputs": [],
      "source": [
        "#create the Dataplex Universal Catalog Entry\n",
        "create_entry()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pn_GUhPy2d29",
      "metadata": {
        "id": "pn_GUhPy2d29"
      },
      "source": [
        "# Load CSV to BigQuery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UAuwo7eo_XVk",
      "metadata": {
        "id": "UAuwo7eo_XVk"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "$DATASET_SQL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vssYu0NPB3dY",
      "metadata": {
        "id": "vssYu0NPB3dY"
      },
      "outputs": [],
      "source": [
        "#load file into bq table\n",
        "table_id = f\"{DATASET_NAME}.{TABLE_NAME}-{RUN_ID}\"\n",
        "cmd=f\"bq load --source_format=CSV --autodetect {table_id} gs://{BUCKET_NAME}/{DATA_FILE}\"\n",
        "!{cmd}\n",
        "print(f\"Created table '{table_id}' in dataset '{DATASET_NAME}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ODnBlpO32lOD",
      "metadata": {
        "id": "ODnBlpO32lOD"
      },
      "source": [
        "# Create And Apply Custom Aspects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QxQTQQbVjPsK",
      "metadata": {
        "id": "QxQTQQbVjPsK"
      },
      "outputs": [],
      "source": [
        "#TBD\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3OvRg4LHIYBN",
      "metadata": {
        "id": "3OvRg4LHIYBN"
      },
      "source": [
        "# Search Dataplex Universal Catalog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1gzE_D4LIlUO",
      "metadata": {
        "id": "1gzE_D4LIlUO"
      },
      "outputs": [],
      "source": [
        "#tbd"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vQ8yIyedT5LE",
        "R9aJyJxvxYqm",
        "iIfTfYrtn9Tt",
        "4a57BiDBw0Gj",
        "OcO3681Fzjbm",
        "H8dLCjhcx8A4",
        "3pCqlu422Jrj",
        "pn_GUhPy2d29",
        "ODnBlpO32lOD",
        "3OvRg4LHIYBN"
      ],
      "name": "Dataplex_OpenLineage_Example",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
