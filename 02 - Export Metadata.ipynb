{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exporting Dataplex Metadata\n",
    "\n",
    "You can run a **metadata export job** to get a snapshot of your Dataplex Universal Catalog metadata (which consists of entries and  aspects) for use in external systems.\n",
    "\n",
    "### Defining the Export Scope\n",
    "\n",
    "Every export job requires a **job scope** to define exactly what metadata to export. You must choose one of the following primary scopes:\n",
    "\n",
    "- `Organization`: Export all metadata belonging to your organization.\n",
    "- `Projects`: Export metadata from one or more specified projects.\n",
    "- `Entry groups`: Export metadata from one or more specified entry groups.\n",
    "\n",
    "You can further refine the scope by specifying the entry types or aspect types to include, ensuring the job only exports the specific entries and aspects you need."
   ],
   "metadata": {
    "id": "Al6bR1gHAs-I"
   },
   "id": "Al6bR1gHAs-I"
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import google.auth\n",
    "from google.api_core.exceptions import Conflict\n",
    "from google.auth.transport.requests import AuthorizedSession\n",
    "from google.cloud import bigquery, storage\n",
    "from google.cloud.exceptions import NotFound\n",
    "from requests import HTTPError"
   ],
   "metadata": {
    "id": "6olpz1eCqb3u",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759987436334,
     "user_tz": 240,
     "elapsed": 854,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "6olpz1eCqb3u",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6036qy0NMWfDG6jPLl4aVqCr",
   "metadata": {
    "tags": [],
    "id": "6036qy0NMWfDG6jPLl4aVqCr",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759987454750,
     "user_tz": 240,
     "elapsed": 206,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "ef8589a3-bbaf-44a7-ba7d-5d4c92812262"
   },
   "source": "# --- Configuration ---\n# @title Metadata Export Configuration { display-mode: \"form\" }\n\n# GCP Settings\nPROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"your-gcp-project-id\") #@param {type:\"string\"}\nLOCATION = \"us-central1\" #@param [\"us-central1\", \"us-east1\", \"us-west1\", \"europe-west1\", \"asia-southeast1\"]\n\n# Export Settings\nEXPORT_BUCKET_NAME = f\"{PROJECT_ID}-lab-data-export\" #@param {type:\"string\"}\n\n# BigQuery Configuration\nDATASET_ID = \"dataplex_metadata\" #@param {type:\"string\"}\nTABLE_ID = \"metadata_export\" #@param {type:\"string\"}\nDATASET_LOCATION = \"us-central1\" #@param [\"US\", \"EU\", \"us-central1\", \"us-east1\", \"europe-west1\"]\n\nprint(\"Configuration loaded:\")\nprint(f\"   Project: {PROJECT_ID}\")\nprint(f\"   Location: {LOCATION}\")\nprint(f\"   Export Bucket: {EXPORT_BUCKET_NAME}\")\nprint(f\"   BigQuery Dataset: {DATASET_ID}\")\nprint(f\"   BigQuery Table: {TABLE_ID}\")\nprint(f\"   Dataset Location: {DATASET_LOCATION}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def validate_config():\n    \"\"\"Validates that required configuration is set.\"\"\"\n    if PROJECT_ID == \"your-gcp-project-id\":\n        raise ValueError(\"Please update the PROJECT_ID variable before running.\")\n\n\ndef call_google_api(\n    url: str,\n    http_verb: str,\n    request_body: Optional[Dict[str, Any]] = None\n) -> Dict[str, Any]:\n    \"\"\"\n    Makes authenticated API calls to Google Cloud services.\n\n    Args:\n        url: The complete API endpoint URL\n        http_verb: HTTP method (GET, POST, PUT, DELETE, etc.)\n        request_body: Optional request payload as a dictionary\n\n    Returns:\n        Response data as a dictionary (empty dict for 204 responses)\n\n    Raises:\n        RuntimeError: If the API call fails with detailed error information\n    \"\"\"\n    creds, project = google.auth.default(\n        scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n    )\n    authed_session = AuthorizedSession(creds)\n\n    try:\n        response = authed_session.request(\n            method=http_verb,\n            url=url,\n            json=request_body\n        )\n        response.raise_for_status()\n\n        # Handle no-content responses\n        if response.status_code == 204:\n            return {}\n\n        return response.json()\n\n    except HTTPError as e:\n        error_message = (\n            f\"API call failed with status {e.response.status_code}: \"\n            f\"{e.response.text}\"\n        )\n        print(error_message)\n        raise RuntimeError(error_message) from e",
   "metadata": {
    "id": "6FC8b-trpAPb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759987492136,
     "user_tz": 240,
     "elapsed": 159,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "6FC8b-trpAPb",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_storage_bucket() -> None:\n",
    "    \"\"\"\n",
    "    Creates a GCS bucket for metadata exports if it doesn't already exist (idempotent).\n",
    "\n",
    "    Uses the globally configured PROJECT_ID and EXPORT_BUCKET_NAME variables.\n",
    "    Prints status messages indicating whether the bucket was created or already exists.\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "    try:\n",
    "        bucket = storage_client.get_bucket(EXPORT_BUCKET_NAME)\n",
    "        print(f\"Bucket {EXPORT_BUCKET_NAME} already exists.\")\n",
    "    except NotFound:\n",
    "        try:\n",
    "            bucket = storage_client.create_bucket(\n",
    "                EXPORT_BUCKET_NAME,\n",
    "                location=LOCATION\n",
    "            )\n",
    "            print(f\"Bucket {bucket.name} created in {LOCATION}.\")\n",
    "        except Conflict:\n",
    "            # Handle race condition where bucket was created between get and create\n",
    "            print(f\"Bucket {EXPORT_BUCKET_NAME} already exists.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating bucket: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "create_storage_bucket()"
   ],
   "metadata": {
    "id": "LF7vBAyapImd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759987494270,
     "user_tz": 240,
     "elapsed": 221,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "52ebe4f3-6ffc-4d47-d952-164948c01195"
   },
   "id": "LF7vBAyapImd",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the export scope - choose one of the following options:\n",
    "\n",
    "# Option 1: Export metadata for specific entry groups\n",
    "# request_body = {\n",
    "#     \"type\": \"EXPORT\",\n",
    "#     \"export_spec\": {\n",
    "#         \"output_path\": f\"gs://{EXPORT_BUCKET_NAME}/\",\n",
    "#         \"scope\": {\n",
    "#             \"entryGroups\": [\n",
    "#                 \"@bigquery\",\n",
    "#                 # Add additional entry groups as needed\n",
    "#             ],\n",
    "#         },\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# Option 2: Export metadata for specific projects\n",
    "# request_body = {\n",
    "#     \"type\": \"EXPORT\",\n",
    "#     \"export_spec\": {\n",
    "#         \"output_path\": f\"gs://{EXPORT_BUCKET_NAME}/\",\n",
    "#         \"scope\": {\n",
    "#             \"projects\": [\n",
    "#                 f\"projects/{PROJECT_ID}\"\n",
    "#             ]\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# Option 3: Export metadata for the entire organization (currently active)\n",
    "request_body = {\n",
    "    \"type\": \"EXPORT\",\n",
    "    \"export_spec\": {\n",
    "        \"output_path\": f\"gs://{EXPORT_BUCKET_NAME}/\",\n",
    "        \"scope\": {\n",
    "            \"organizationLevel\": \"true\",\n",
    "        },\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "id": "2b0apTZYpMHe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759987496623,
     "user_tz": 240,
     "elapsed": 215,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "2b0apTZYpMHe",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create and trigger the metadata export job\n",
    "url = f\"https://dataplex.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/metadataJobs\"\n",
    "response = call_google_api(url, \"POST\", request_body)\n",
    "\n",
    "# Store the job target for status monitoring\n",
    "metadata_job_target = response['metadata']['target']\n",
    "\n",
    "\n",
    "# Display the job creation response\n",
    "pretty_json = json.dumps(response, indent=4, sort_keys=True)\n",
    "print(pretty_json)\n"
   ],
   "metadata": {
    "id": "96w1wB3ZpklO",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759987499728,
     "user_tz": 240,
     "elapsed": 1222,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "116da804-fdb9-4581-eda4-d0972807af9a"
   },
   "id": "96w1wB3ZpklO",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The metadata export takes approximately 20-25 minutes to complete.  You can refresh this cell to monitor the progress.  \n",
    "\n",
    "Feel free to move to the next section of the notebook, due to time constraints a complete export is provided for the next section of the lab."
   ],
   "metadata": {
    "id": "IAYg2WGXugih"
   },
   "id": "IAYg2WGXugih"
  },
  {
   "cell_type": "code",
   "source": [
    "# Check the status of the metadata export job\n",
    "status_url = f\"https://dataplex.googleapis.com/v1/{metadata_job_target}\"\n",
    "response = call_google_api(status_url, \"GET\")\n",
    "\n",
    "# Display the job status\n",
    "pretty_json = json.dumps(response, indent=4, sort_keys=True)\n",
    "print(pretty_json)"
   ],
   "metadata": {
    "id": "tIKieMphsfUr",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759987532770,
     "user_tz": 240,
     "elapsed": 214,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "130f082d-aa39-4c0d-d62d-2231c8cc0645"
   },
   "id": "tIKieMphsfUr",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyzing Dataplex Metadata in BigQuery\n",
    "\n",
    "We've just exported our Dataplex metadata to GCS. When you want to analyze this metadata in BigQuery, you can create an external table. This lets you query the data directly from its exported location without needing to load or transform it first.\n",
    "\n",
    "### Why a Business Would Import Dataplex Metadata into BigQuery\n",
    "\n",
    "There are several key reasons why a business would want to bring its Dataplex metadata into BigQuery for analysis:\n",
    "\n",
    "* **Advanced Querying and Analysis**: By having the metadata in BigQuery, you can run SQL queries to gain deeper insights.\n",
    "    * *Example*: Count the number of entries by entry group, or find all entries that have a specific aspect (like data quality scores).\n",
    "    ```sql\n",
    "    -- Example: Count entries per entry group\n",
    "    SELECT\n",
    "      entry_group,\n",
    "      COUNT(entry_id) AS number_of_entries\n",
    "    FROM\n",
    "      `your_project.your_dataset.dataplex_metadata_external_table`\n",
    "    GROUP BY\n",
    "      entry_group\n",
    "    ORDER BY\n",
    "      number_of_entries DESC;\n",
    "    ```\n",
    "\n",
    "* **Integration with Analytics Tools**: Importing the metadata to BigQuery allows you to analyze your metadata alongside other business data, or visualize it in tools like Looker Studio.\n",
    "\n",
    "* **Programmatic Processing**: For businesses that need to process large volumes of metadata, exporting it allows for programmatic manipulation using SQL. This processed metadata can then be imported back into Dataplex via API if needed.\n",
    "\n",
    "* **Custom Applications and Third-Party Tools**: You can integrate your metadata into custom-built applications (like a data governance dashboard) or other third-party tools that connect with BigQuery, extending the functionality and use of your metadata."
   ],
   "metadata": {
    "id": "9Rtkx4fgu2e3"
   },
   "id": "9Rtkx4fgu2e3"
  },
  {
   "cell_type": "code",
   "source": [
    "def create_hive_partitioned_external_table(\n",
    "    project_id: str,\n",
    "    export_bucket_name: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a Hive-partitioned external table in BigQuery pointing to exported metadata.\n",
    "\n",
    "    This function is idempotent - it can be run multiple times safely. If the table\n",
    "    already exists, it will be replaced with the updated configuration.\n",
    "\n",
    "    This function creates a BigQuery external table that reads newline-delimited JSON\n",
    "    files from a GCS bucket. The table uses Hive-style partitioning for efficient\n",
    "    querying of time-partitioned data.\n",
    "\n",
    "    Args:\n",
    "        project_id: Google Cloud project ID\n",
    "        export_bucket_name: GCS bucket name containing the exported metadata files\n",
    "    \"\"\"\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    dataset_ref = client.dataset(DATASET_ID)\n",
    "    table_ref = dataset_ref.table(TABLE_ID)\n",
    "\n",
    "    # Ensure dataset exists (idempotent)\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    dataset.location = DATASET_LOCATION\n",
    "    dataset = client.create_dataset(dataset, exists_ok=True)\n",
    "    print(f\"Dataset '{DATASET_ID}' ready (created or already exists).\")\n",
    "\n",
    "    # Define table schema matching Dataplex metadata export format\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\n",
    "            \"entry\", \"RECORD\", \"NULLABLE\",\n",
    "            fields=[\n",
    "                bigquery.SchemaField(\"name\", \"STRING\", \"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"entryType\", \"STRING\", \"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"createTime\", \"STRING\", \"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"updateTime\", \"STRING\", \"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"aspects\", \"JSON\", \"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"parentEntry\", \"STRING\", \"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"fullyQualifiedName\", \"STRING\", \"NULLABLE\"),\n",
    "                bigquery.SchemaField(\n",
    "                    \"entrySource\", \"RECORD\", \"NULLABLE\",\n",
    "                    fields=[\n",
    "                        bigquery.SchemaField(\"resource\", \"STRING\", \"NULLABLE\"),\n",
    "                        bigquery.SchemaField(\"system\", \"STRING\", \"NULLABLE\"),\n",
    "                        bigquery.SchemaField(\"platform\", \"STRING\", \"NULLABLE\"),\n",
    "                        bigquery.SchemaField(\"displayName\", \"STRING\", \"NULLABLE\"),\n",
    "                        bigquery.SchemaField(\"description\", \"STRING\", \"NULLABLE\"),\n",
    "                        bigquery.SchemaField(\"labels\", \"JSON\", \"NULLABLE\"),\n",
    "                        bigquery.SchemaField(\n",
    "                            \"ancestors\", \"RECORD\", \"REPEATED\",\n",
    "                            fields=[\n",
    "                                bigquery.SchemaField(\"name\", \"STRING\", \"NULLABLE\"),\n",
    "                                bigquery.SchemaField(\"type\", \"STRING\", \"NULLABLE\"),\n",
    "                            ],\n",
    "                        ),\n",
    "                        bigquery.SchemaField(\"createTime\", \"STRING\", \"NULLABLE\"),\n",
    "                        bigquery.SchemaField(\"updateTime\", \"STRING\", \"NULLABLE\"),\n",
    "                        bigquery.SchemaField(\"location\", \"STRING\", \"NULLABLE\"),\n",
    "                    ],\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Configure external data source with Hive partitioning\n",
    "    external_config = bigquery.ExternalConfig(\"NEWLINE_DELIMITED_JSON\")\n",
    "    gcs_uri = f\"gs://{export_bucket_name}/*\"\n",
    "    external_config.source_uris = [gcs_uri]\n",
    "\n",
    "    hive_partitioning_options = bigquery.HivePartitioningOptions()\n",
    "    hive_partitioning_options.mode = \"AUTO\"\n",
    "    hive_partitioning_options.source_uri_prefix = f\"gs://{export_bucket_name}/\"\n",
    "    external_config.hive_partitioning = hive_partitioning_options\n",
    "\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table.external_data_configuration = external_config\n",
    "\n",
    "    # Create or replace the table (idempotent)\n",
    "    try:\n",
    "        # Check if table exists\n",
    "        existing_table = client.get_table(table_ref)\n",
    "        # Table exists, update it\n",
    "        existing_table.schema = schema\n",
    "        existing_table.external_data_configuration = external_config\n",
    "        updated_table = client.update_table(\n",
    "            existing_table,\n",
    "            [\"schema\", \"external_data_configuration\"]\n",
    "        )\n",
    "        print(\n",
    "            f\"Updated existing external table: \"\n",
    "            f\"{updated_table.project}.{updated_table.dataset_id}.{updated_table.table_id}\"\n",
    "        )\n",
    "    except NotFound:\n",
    "        # Table doesn't exist, create it\n",
    "        created_table = client.create_table(table)\n",
    "        print(\n",
    "            f\"Created new external table: \"\n",
    "            f\"{created_table.project}.{created_table.dataset_id}.{created_table.table_id}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def create_aspect_extraction_udf() -> None:\n",
    "    \"\"\"\n",
    "    Creates a persistent UDF to extract aspect information from the nested JSON structure.\n",
    "    This UDF handles the dynamic keys in the aspects JSON.\n",
    "    \"\"\"\n",
    "    udf_sql = f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION `{PROJECT_ID}.{DATASET_ID}.extract_aspect_types`(aspects_json JSON)\n",
    "    RETURNS STRING\n",
    "    LANGUAGE js AS r\\\"\\\"\\\"\n",
    "      if (!aspects_json) return null;\n",
    "\n",
    "      try {{\n",
    "        const aspectTypes = [];\n",
    "\n",
    "        // Iterate through all keys in the aspects object\n",
    "        for (const aspectId in aspects_json) {{\n",
    "          if (aspects_json.hasOwnProperty(aspectId)) {{\n",
    "            const aspect = aspects_json[aspectId];\n",
    "            if (aspect && aspect.aspectType) {{\n",
    "              aspectTypes.push(aspect.aspectType);\n",
    "            }}\n",
    "          }}\n",
    "        }}\n",
    "\n",
    "        // Return unique, sorted aspect types as comma-separated string\n",
    "        return [...new Set(aspectTypes)].sort().join(', ');\n",
    "      }} catch (e) {{\n",
    "        return null;\n",
    "      }}\n",
    "    \\\"\\\"\\\";\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        bq_client.query(udf_sql).result()\n",
    "        print(f\"Created/updated UDF: {PROJECT_ID}.{DATASET_ID}.extract_aspect_types\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating UDF: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def create_unnested_metadata_view() -> None:\n",
    "    \"\"\"\n",
    "    Creates a view that unnests the metadata export table for easier querying.\n",
    "\n",
    "    The view flattens the nested entry structure and parses aspect JSON to extract\n",
    "    key metadata fields. This makes it easier to query and analyze the metadata\n",
    "    without dealing with complex nested structures.\n",
    "    \"\"\"\n",
    "    view_id = f\"{PROJECT_ID}.{DATASET_ID}.vw_{TABLE_ID}_unnested\"\n",
    "\n",
    "    # Create view SQL that unnests and parses the metadata\n",
    "    view_sql = f\"\"\"\n",
    "    CREATE OR REPLACE VIEW `{view_id}` AS\n",
    "    SELECT\n",
    "      -- Entry identification\n",
    "      entry.name AS entry_name,\n",
    "      entry.entryType AS entry_type,\n",
    "      entry.fullyQualifiedName AS fully_qualified_name,\n",
    "      entry.parentEntry AS parent_entry,\n",
    "\n",
    "      -- Entry metadata (parse timestamps)\n",
    "      PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E*SZ', entry.createTime) AS entry_create_time,\n",
    "      PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E*SZ', entry.updateTime) AS entry_update_time,\n",
    "\n",
    "      -- Entry Source: Resource information\n",
    "      entry.entrySource.resource AS resource,\n",
    "      entry.entrySource.system AS system,\n",
    "      entry.entrySource.platform AS platform,\n",
    "      entry.entrySource.displayName AS display_name,\n",
    "      entry.entrySource.description AS description,\n",
    "      entry.entrySource.location AS resource_location,\n",
    "      PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E*SZ', entry.entrySource.createTime) AS resource_create_time,\n",
    "      PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E*SZ', entry.entrySource.updateTime) AS resource_update_time,\n",
    "\n",
    "      -- Labels (keep as JSON for flexibility)\n",
    "      entry.entrySource.labels AS labels_json,\n",
    "\n",
    "      -- Aspects: Keep full JSON for detailed analysis\n",
    "      entry.aspects AS aspects_json,\n",
    "\n",
    "      -- Extract aspect types using the UDF\n",
    "      `{PROJECT_ID}.{DATASET_ID}.extract_aspect_types`(entry.aspects) AS aspect_types,\n",
    "\n",
    "      -- Ancestor information (unnest the ancestors array)\n",
    "      ancestor.name AS ancestor_name,\n",
    "      ancestor.type AS ancestor_type,\n",
    "\n",
    "      -- Partition columns for efficient filtering\n",
    "      project,\n",
    "      year,\n",
    "      month,\n",
    "      day\n",
    "\n",
    "    FROM\n",
    "      `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n",
    "    LEFT JOIN\n",
    "      UNNEST(entry.entrySource.ancestors) AS ancestor\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        bq_client.query(view_sql).result()\n",
    "        print(f\"Created/updated unnested view: {view_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating view: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Initialize BigQuery client\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Create the external table\n",
    "create_hive_partitioned_external_table(PROJECT_ID, EXPORT_BUCKET_NAME)\n",
    "\n",
    "# Create the UDF for aspect extraction\n",
    "create_aspect_extraction_udf()\n",
    "\n",
    "# Create the unnested view\n",
    "create_unnested_metadata_view()"
   ],
   "metadata": {
    "id": "iuaVbJc4Dk2R",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759987554942,
     "user_tz": 240,
     "elapsed": 2674,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "b8ad9d1b-1956-404d-86ac-5cd19fb56977"
   },
   "id": "iuaVbJc4Dk2R",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Query the unnested view for easier analysis\n",
    "# This view flattens the nested structure making queries simpler\n",
    "\n",
    "query = f\"\"\"\n",
    "-- List the top 10 projects with the most resources\n",
    "SELECT\n",
    "    project,\n",
    "    COUNT(DISTINCT resource) AS unique_resources,\n",
    "    COUNT(DISTINCT entry_name) AS total_entries,\n",
    "    COUNT(DISTINCT entry_type) AS entry_type_count\n",
    "FROM\n",
    "    `{PROJECT_ID}.{DATASET_ID}.vw_{TABLE_ID}_unnested`\n",
    "WHERE\n",
    "    year = EXTRACT(YEAR FROM CURRENT_DATE())\n",
    "GROUP BY\n",
    "    project\n",
    "ORDER BY\n",
    "    unique_resources DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "df = bq_client.query(query).to_dataframe()\n",
    "display(df)"
   ],
   "metadata": {
    "id": "PjV5lDEK2uYQ"
   },
   "id": "PjV5lDEK2uYQ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Analyze aspect types across all entries using the unnested view\n",
    "# The view has already extracted aspect types, making this query much simpler\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    TRIM(aspect_type) AS aspect_type,\n",
    "    COUNT(DISTINCT entry_name) AS entry_count,\n",
    "    COUNT(DISTINCT project) AS project_count,\n",
    "    COUNT(DISTINCT system) AS system_count\n",
    "FROM\n",
    "    `{PROJECT_ID}.{DATASET_ID}.vw_{TABLE_ID}_unnested`,\n",
    "    UNNEST(SPLIT(aspect_types, ', ')) AS aspect_type\n",
    "WHERE\n",
    "    aspect_type IS NOT NULL\n",
    "    AND aspect_type != ''\n",
    "GROUP BY\n",
    "    aspect_type\n",
    "ORDER BY\n",
    "    entry_count DESC;\n",
    "\"\"\"\n",
    "\n",
    "df = bq_client.query(query).to_dataframe()\n",
    "display(df)"
   ],
   "metadata": {
    "id": "f_lvfB8S5KWX",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 771
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759987572565,
     "user_tz": 240,
     "elapsed": 4042,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "6bbb3c97-260f-402b-eef4-1c61a655d7b9"
   },
   "id": "f_lvfB8S5KWX",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Interesting Questions to Explore in Data Canvas\n",
    "\n",
    "Once your metadata is loaded, here are some compelling questions you can investigate and visualize:\n",
    "\n",
    "### **Data Governance Dashboard**\n",
    "- **Metadata Coverage**: What percentage of resources have descriptions?\n",
    "- **Freshness Analysis**: Resources that haven't been updated in 6+ months (potential for cleanup)\n",
    "- **Documentation Gaps**: Entry types missing critical aspects (schema, data quality, ownership)\n",
    "\n",
    "### **Data Estate Insights**\n",
    "- **Growth Trends**: New resources created per month by project/system (line chart)\n",
    "- **Platform Distribution**: Pie chart of resources by platform (BigQuery, GCS, Dataform, etc.)\n",
    "- **Entry Type Breakdown**: Bar chart showing most common entry types across the organization\n",
    "\n",
    "Some examples:\n",
    "\n",
    "![Entites created by system over time](https://raw.githubusercontent.com/haneyr/bq-academy-datagov-lab/main/media/metadata01.png)\n",
    "\n",
    "\n",
    "![Resources created over time by project](https://raw.githubusercontent.com/haneyr/bq-academy-datagov-lab/main/media/metadata02.png)\n",
    "\n",
    "\n",
    "Try these queries in Data Canvas to get started!"
   ],
   "metadata": {
    "id": "DugGdczbbvbm"
   },
   "id": "DugGdczbbvbm"
  },
  {
   "cell_type": "markdown",
   "id": "7l85zpsmqa4",
   "source": "---\n\n## Cleanup\n\nThe following cells will help you clean up resources created by this notebook. Run these only when you're done with the lab and want to remove all created resources.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "wlidhb3i5i",
   "source": "def cleanup_bigquery_resources(delete_dataset: bool = True) -> None:\n    \"\"\"\n    Cleans up BigQuery resources created by this notebook.\n    \n    Args:\n        delete_dataset: If True, deletes the entire dataset including all tables and views.\n                       If False, only deletes the specific table, view, and UDF.\n    \"\"\"\n    client = bigquery.Client(project=PROJECT_ID)\n    \n    if delete_dataset:\n        # Delete the entire dataset (includes table, view, and UDF)\n        dataset_id = f\"{PROJECT_ID}.{DATASET_ID}\"\n        try:\n            client.delete_dataset(dataset_id, delete_contents=True, not_found_ok=True)\n            print(f\"Deleted dataset '{dataset_id}' and all its contents.\")\n        except Exception as e:\n            print(f\"Error deleting dataset '{dataset_id}': {e}\")\n    else:\n        # Delete individual resources\n        resources_deleted = []\n        resources_failed = []\n        \n        # Delete the view\n        view_id = f\"{PROJECT_ID}.{DATASET_ID}.vw_{TABLE_ID}_unnested\"\n        try:\n            client.delete_table(view_id, not_found_ok=True)\n            resources_deleted.append(f\"View: {view_id}\")\n        except Exception as e:\n            resources_failed.append(f\"View: {view_id} - {e}\")\n        \n        # Delete the table\n        table_id = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n        try:\n            client.delete_table(table_id, not_found_ok=True)\n            resources_deleted.append(f\"Table: {table_id}\")\n        except Exception as e:\n            resources_failed.append(f\"Table: {table_id} - {e}\")\n        \n        # Delete the UDF\n        udf_id = f\"{PROJECT_ID}.{DATASET_ID}.extract_aspect_types\"\n        try:\n            client.delete_routine(udf_id, not_found_ok=True)\n            resources_deleted.append(f\"UDF: {udf_id}\")\n        except Exception as e:\n            resources_failed.append(f\"UDF: {udf_id} - {e}\")\n        \n        # Print results\n        if resources_deleted:\n            print(\"Successfully deleted resources:\")\n            for resource in resources_deleted:\n                print(f\"  - {resource}\")\n        \n        if resources_failed:\n            print(\"\\nFailed to delete:\")\n            for resource in resources_failed:\n                print(f\"  - {resource}\")\n\n\ndef cleanup_gcs_bucket(delete_bucket: bool = True, delete_contents_only: bool = False) -> None:\n    \"\"\"\n    Cleans up GCS bucket and/or its contents.\n    \n    Args:\n        delete_bucket: If True, deletes the entire bucket.\n        delete_contents_only: If True, only deletes bucket contents but keeps the bucket.\n    \"\"\"\n    storage_client = storage.Client(project=PROJECT_ID)\n    \n    try:\n        bucket = storage_client.get_bucket(EXPORT_BUCKET_NAME)\n        \n        # Delete all blobs in the bucket\n        blobs = list(bucket.list_blobs())\n        if blobs:\n            for blob in blobs:\n                blob.delete()\n            print(f\"Deleted {len(blobs)} file(s) from bucket '{EXPORT_BUCKET_NAME}'.\")\n        else:\n            print(f\"Bucket '{EXPORT_BUCKET_NAME}' is already empty.\")\n        \n        # Delete the bucket itself if requested\n        if delete_bucket and not delete_contents_only:\n            bucket.delete()\n            print(f\"Deleted bucket '{EXPORT_BUCKET_NAME}'.\")\n        elif delete_contents_only:\n            print(f\"Bucket '{EXPORT_BUCKET_NAME}' contents deleted, bucket retained.\")\n            \n    except NotFound:\n        print(f\"Bucket '{EXPORT_BUCKET_NAME}' not found (already deleted).\")\n    except Exception as e:\n        print(f\"Error cleaning up bucket '{EXPORT_BUCKET_NAME}': {e}\")\n\n\ndef cleanup_all_resources(\n    delete_bigquery_dataset: bool = True,\n    delete_gcs_bucket: bool = True\n) -> None:\n    \"\"\"\n    Cleans up all resources created by this notebook.\n    \n    Args:\n        delete_bigquery_dataset: If True, deletes the entire BigQuery dataset.\n        delete_gcs_bucket: If True, deletes the GCS bucket and all contents.\n    \"\"\"\n    print(\"=\" * 80)\n    print(\"CLEANING UP RESOURCES\")\n    print(\"=\" * 80)\n    \n    print(\"\\n1. BigQuery Resources:\")\n    cleanup_bigquery_resources(delete_dataset=delete_bigquery_dataset)\n    \n    print(\"\\n2. GCS Bucket:\")\n    cleanup_gcs_bucket(delete_bucket=delete_gcs_bucket)\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"CLEANUP COMPLETE\")\n    print(\"=\" * 80)\n\n\n# Example usage - UNCOMMENT to run cleanup\n# Cleanup options:\n\n# Option 1: Delete everything (dataset, bucket, and all contents)\n# cleanup_all_resources(delete_bigquery_dataset=True, delete_gcs_bucket=True)\n\n# Option 2: Delete only BigQuery resources, keep GCS bucket\n# cleanup_all_resources(delete_bigquery_dataset=True, delete_gcs_bucket=False)\n\n# Option 3: Delete only specific BigQuery resources (table, view, UDF), keep dataset\n# cleanup_bigquery_resources(delete_dataset=False)\n\n# Option 4: Delete only GCS bucket contents, keep the bucket\n# cleanup_gcs_bucket(delete_bucket=False, delete_contents_only=True)\n\nprint(\"Cleanup functions defined. Uncomment one of the options above to run cleanup.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "colab": {
   "provenance": [],
   "name": "02 - Export Metadata"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}