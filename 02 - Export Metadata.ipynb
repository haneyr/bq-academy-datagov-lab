{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "from google.cloud import bigquery\n",
        "import os\n",
        "import json\n"
      ],
      "metadata": {
        "id": "6olpz1eCqb3u",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758941009732,
          "user_tz": 240,
          "elapsed": 2224,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "6olpz1eCqb3u",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "6036qy0NMWfDG6jPLl4aVqCr",
      "metadata": {
        "tags": [],
        "id": "6036qy0NMWfDG6jPLl4aVqCr",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758941568651,
          "user_tz": 240,
          "elapsed": 197,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "source": [
        "# Configuration\n",
        "PROJECT_ID = \"PROJECT HERE\"\n",
        "LOCATION = \"us-central1\"\n",
        "EXPORT_BUCKET_NAME = f\"{PROJECT_ID}-lab-data-export\""
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.auth\n",
        "from google.auth.transport.requests import AuthorizedSession\n",
        "from requests import HTTPError\n",
        "from typing import Any, Optional, Dict\n",
        "\n",
        "def call_google_api(\n",
        "    url: str,\n",
        "    http_verb: str,\n",
        "    request_body: Optional[Dict[str, Any]] = None\n",
        ") -> Dict[str, Any]:\n",
        "    creds, project = google.auth.default(\n",
        "        scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
        "    )\n",
        "    authed_session = AuthorizedSession(creds)\n",
        "    try:\n",
        "        response = authed_session.request(\n",
        "            method=http_verb,\n",
        "            url=url,\n",
        "            json=request_body  # requests handles None for json param gracefully\n",
        "        )\n",
        "\n",
        "        response.raise_for_status()\n",
        "\n",
        "        if response.status_code == 204:\n",
        "            return {}\n",
        "\n",
        "        return response.json()\n",
        "\n",
        "    except HTTPError as e:\n",
        "        # Provide more structured error information\n",
        "        error_message = f\"API call failed with status {e.response.status_code}: {e.response.text}\"\n",
        "        print(error_message) # Or use logging\n",
        "        raise RuntimeError(error_message) from e"
      ],
      "metadata": {
        "id": "6FC8b-trpAPb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758941020097,
          "user_tz": 240,
          "elapsed": 217,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "6FC8b-trpAPb",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create bucket if it does not exist\n",
        "def create_storage_bucket():\n",
        "  storage_client = storage.Client(project=PROJECT_ID)\n",
        "  buckets = storage_client.list_buckets()\n",
        "  bucket_names = [bucket.name for bucket in buckets]\n",
        "\n",
        "  bucket = storage_client.bucket(EXPORT_BUCKET_NAME)\n",
        "\n",
        "  if not bucket.exists():\n",
        "      try:\n",
        "          bucket = storage_client.create_bucket(EXPORT_BUCKET_NAME)\n",
        "          print(f\"Bucket {bucket.name} created.\")\n",
        "      except Exception as e:\n",
        "          print(f\"Error creating bucket: {e}\")\n",
        "  else:\n",
        "      print(f\"Bucket {EXPORT_BUCKET_NAME} already exists.\")\n",
        "\n",
        "create_storage_bucket()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF7vBAyapImd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758941031452,
          "user_tz": 240,
          "elapsed": 212,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "663ee591-7561-484a-82d5-64b80b4be5c4"
      },
      "id": "LF7vBAyapImd",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bucket haneyr-477-20250813153731-lab-data-export already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# request_body = {\n",
        "#   \"type\": \"EXPORT\",\n",
        "#   \"export_spec\": {\n",
        "#     \"output_path\": f\"gs://{EXPORT_BUCKET_NAME}/\",\n",
        "#     \"scope\": {\n",
        "#       \"projects\": [\n",
        "#         f\"projects/{PROJECT_ID}\"\n",
        "#       ]\n",
        "#     }\n",
        "#   }\n",
        "# }\n",
        "\n",
        "request_body = {\n",
        "  \"type\": \"EXPORT\",\n",
        "  \"export_spec\": {\n",
        "    \"output_path\": f\"gs://{EXPORT_BUCKET_NAME}/\",\n",
        "    \"scope\": {\n",
        "      \"organizationLevel\": \"true\",\n",
        "    },\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "2b0apTZYpMHe",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758941921247,
          "user_tz": 240,
          "elapsed": 169,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "2b0apTZYpMHe",
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = f\"https://dataplex.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/metadataJobs\"\n",
        "response = call_google_api(url, \"POST\", request_body)\n",
        "metadata_job_target = response['metadata']['target']\n",
        "pretty_json = json.dumps(response, indent=4, sort_keys=True)\n",
        "print(pretty_json)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96w1wB3ZpklO",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758942047988,
          "user_tz": 240,
          "elapsed": 332,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "c78086f6-42bc-46b6-a762-e9ccdb515305"
      },
      "id": "96w1wB3ZpklO",
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"done\": false,\n",
            "    \"metadata\": {\n",
            "        \"@type\": \"type.googleapis.com/google.cloud.dataplex.v1.OperationMetadata\",\n",
            "        \"apiVersion\": \"v1\",\n",
            "        \"createTime\": \"2025-09-27T03:00:48.064044165Z\",\n",
            "        \"requestedCancellation\": false,\n",
            "        \"target\": \"projects/haneyr-1200-20250807004910/locations/us-central1/metadataJobs/metadata-job-703cf6c8-7515-4027-ae09-089495955bac\",\n",
            "        \"verb\": \"create\"\n",
            "    },\n",
            "    \"name\": \"projects/haneyr-1200-20250807004910/locations/us-central1/operations/operation-1758942047873-63fbf9bf1a819-99d7eddf-4bf6238c\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "status_url = f\"https://dataplex.googleapis.com/v1/{metadata_job_target}\"\n",
        "response = call_google_api(status_url, \"GET\")\n",
        "pretty_json = json.dumps(response, indent=4, sort_keys=True)\n",
        "print(pretty_json)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIKieMphsfUr",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758942090162,
          "user_tz": 240,
          "elapsed": 311,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "044ce705-1a44-42c1-abc6-c8de124d898b"
      },
      "id": "tIKieMphsfUr",
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"createTime\": \"2025-09-27T03:00:48.059153143Z\",\n",
            "    \"exportResult\": {},\n",
            "    \"exportSpec\": {\n",
            "        \"outputPath\": \"gs://haneyr-1200-20250807004910-lab-data-export/\",\n",
            "        \"scope\": {\n",
            "            \"organizationLevel\": true\n",
            "        }\n",
            "    },\n",
            "    \"name\": \"projects/haneyr-1200-20250807004910/locations/us-central1/metadataJobs/metadata-job-703cf6c8-7515-4027-ae09-089495955bac\",\n",
            "    \"status\": {\n",
            "        \"message\": \"Logs for this MetadataJob can be found at: https://console.cloud.google.com/logs/query;query=resource.type=\\\"dataplex.googleapis.com/MetadataJob\\\"\\nresource.labels.location=\\\"us-central1\\\"\\nresource.labels.metadata_job_id=\\\"metadata-job-703cf6c8-7515-4027-ae09-089495955bac\\\";?project=609577334843\\n\",\n",
            "        \"state\": \"RUNNING\",\n",
            "        \"updateTime\": \"2025-09-27T03:01:26.110532Z\"\n",
            "    },\n",
            "    \"type\": \"EXPORT\",\n",
            "    \"uid\": \"e3ed5988-110a-493f-990a-1787c507ade9\",\n",
            "    \"updateTime\": \"2025-09-27T03:01:26.184859161Z\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "from google.api_core.exceptions import Conflict\n",
        "\n",
        "def create_hive_partitioned_external_table(project_id: str, export_bucket_name: str) -> None:\n",
        "    \"\"\"\n",
        "    Creates a Hive-partitioned external table in BigQuery.\n",
        "\n",
        "    Checks if the dataset exists and creates it if necessary before attempting\n",
        "    to create the table. The table's data is stored in newline-delimited JSON\n",
        "    format in a Google Cloud Storage bucket with a Hive-style directory structure.\n",
        "\n",
        "    Args:\n",
        "        project_id (str): Your Google Cloud project ID.\n",
        "        export_bucket_name (str): The GCS bucket name containing the source data.\n",
        "    \"\"\"\n",
        "    # Set these variables\n",
        "    dataset_id = \"dataplex_metadata\"\n",
        "    table_id = \"metadata_export\"\n",
        "    location = \"US\"\n",
        "\n",
        "\n",
        "    client = bigquery.Client(project=project_id)\n",
        "    dataset_ref = client.dataset(dataset_id)\n",
        "    table_ref = dataset_ref.table(table_id)\n",
        "\n",
        "    #Check for and create the dataset if it doesn't exist\n",
        "    try:\n",
        "        client.get_dataset(dataset_ref)\n",
        "        print(f\"ℹ️ Dataset '{dataset_id}' already exists.\")\n",
        "    except NotFound:\n",
        "        print(f\"ℹ️ Dataset '{dataset_id}' not found. Creating it in location '{location}'.\")\n",
        "        try:\n",
        "            dataset = bigquery.Dataset(dataset_ref)\n",
        "            dataset.location = location\n",
        "            client.create_dataset(dataset, timeout=30)\n",
        "            print(f\"Successfully created dataset '{dataset_id}'.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to create dataset '{dataset_id}': {e}\")\n",
        "            return\n",
        "\n",
        "    # Table schema\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\n",
        "            \"entry\", \"RECORD\", \"NULLABLE\",\n",
        "            fields=[\n",
        "                bigquery.SchemaField(\"name\", \"STRING\", \"NULLABLE\"),\n",
        "                bigquery.SchemaField(\"entryType\", \"STRING\", \"NULLABLE\"),\n",
        "                bigquery.SchemaField(\"createTime\", \"STRING\", \"NULLABLE\"),\n",
        "                bigquery.SchemaField(\"updateTime\", \"STRING\", \"NULLABLE\"),\n",
        "                bigquery.SchemaField(\"aspects\", \"JSON\", \"NULLABLE\"),\n",
        "                bigquery.SchemaField(\"parentEntry\", \"STRING\", \"NULLABLE\"),\n",
        "                bigquery.SchemaField(\"fullyQualifiedName\", \"STRING\", \"NULLABLE\"),\n",
        "                bigquery.SchemaField(\n",
        "                    \"entrySource\", \"RECORD\", \"NULLABLE\",\n",
        "                    fields=[\n",
        "                        bigquery.SchemaField(\"resource\", \"STRING\", \"NULLABLE\"),\n",
        "                        bigquery.SchemaField(\"system\", \"STRING\", \"NULLABLE\"),\n",
        "                        bigquery.SchemaField(\"platform\", \"STRING\", \"NULLABLE\"),\n",
        "                        bigquery.SchemaField(\"displayName\", \"STRING\", \"NULLABLE\"),\n",
        "                        bigquery.SchemaField(\"description\", \"STRING\", \"NULLABLE\"),\n",
        "                        bigquery.SchemaField(\"labels\", \"JSON\", \"NULLABLE\"),\n",
        "                        bigquery.SchemaField(\n",
        "                            \"ancestors\", \"RECORD\", \"REPEATED\",\n",
        "                            fields=[\n",
        "                                bigquery.SchemaField(\"name\", \"STRING\", \"NULLABLE\"),\n",
        "                                bigquery.SchemaField(\"type\", \"STRING\", \"NULLABLE\"),\n",
        "                            ],\n",
        "                        ),\n",
        "                        bigquery.SchemaField(\"createTime\", \"STRING\", \"NULLABLE\"),\n",
        "                        bigquery.SchemaField(\"updateTime\", \"STRING\", \"NULLABLE\"),\n",
        "                        bigquery.SchemaField(\"location\", \"STRING\", \"NULLABLE\"),\n",
        "                    ],\n",
        "                ),\n",
        "            ],\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    external_config = bigquery.ExternalConfig(\"NEWLINE_DELIMITED_JSON\")\n",
        "    gcs_uri = f\"gs://{export_bucket_name}/*\"\n",
        "    external_config.source_uris = [gcs_uri]\n",
        "\n",
        "    hive_partitioning_options = bigquery.HivePartitioningOptions()\n",
        "    hive_partitioning_options.mode = \"AUTO\"\n",
        "    hive_partitioning_options.source_uri_prefix = f\"gs://{export_bucket_name}/\"\n",
        "    external_config.hive_partitioning = hive_partitioning_options\n",
        "\n",
        "    table = bigquery.Table(table_ref, schema=schema)\n",
        "    table.external_data_configuration = external_config\n",
        "\n",
        "    try:\n",
        "        created_table = client.create_table(table)\n",
        "        print(\n",
        "            f\"Successfully created external table: {created_table.project}.{created_table.dataset_id}.{created_table.table_id}\"\n",
        "        )\n",
        "    except Conflict:\n",
        "        print(f\"ℹ️ Table '{table_id}' already exists.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while creating the table: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "create_hive_partitioned_external_table(PROJECT_ID, EXPORT_BUCKET_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuaVbJc4Dk2R",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758941576350,
          "user_tz": 240,
          "elapsed": 1448,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "63afc1c4-d591-4aee-efb0-6a64670db69f"
      },
      "id": "iuaVbJc4Dk2R",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ℹ️ Dataset 'dataplex_metadata' not found. Creating it in location 'US'...\n",
            "✅ Successfully created dataset 'dataplex_metadata'.\n",
            "✅ Successfully created external table: haneyr-1200-20250807004910.dataplex_metadata.metadata_export\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}