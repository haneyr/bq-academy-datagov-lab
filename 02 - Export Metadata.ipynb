{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exporting Dataplex Metadata\n",
        "\n",
        "You can run a **metadata export job** to get a snapshot of your Dataplex Universal Catalog metadata (which consists of entries and  aspects) for use in external systems.\n",
        "\n",
        "### Defining the Export Scope\n",
        "\n",
        "Every export job requires a **job scope** to define exactly what metadata to export. You must choose one of the following primary scopes:\n",
        "\n",
        "- `Organization`: Export all metadata belonging to your organization.\n",
        "- `Projects`: Export metadata from one or more specified projects.\n",
        "- `Entry groups`: Export metadata from one or more specified entry groups.\n",
        "\n",
        "You can further refine the scope by specifying the entry types or aspect types to include, ensuring the job only exports the specific entries and aspects you need."
      ],
      "metadata": {
        "id": "Al6bR1gHAs-I"
      },
      "id": "Al6bR1gHAs-I"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "from google.cloud import bigquery\n",
        "import os\n",
        "import json\n"
      ],
      "metadata": {
        "id": "6olpz1eCqb3u"
      },
      "id": "6olpz1eCqb3u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "6036qy0NMWfDG6jPLl4aVqCr",
      "metadata": {
        "tags": [],
        "id": "6036qy0NMWfDG6jPLl4aVqCr"
      },
      "source": [
        "# Configuration\n",
        "PROJECT_ID = \"bq-sme-governance-build\"\n",
        "LOCATION = \"us-central1\"\n",
        "EXPORT_BUCKET_NAME = f\"{PROJECT_ID}-lab-data-export\"\n",
        "EXISTING_EXPORT_BUCKET = gs://"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.auth\n",
        "from google.auth.transport.requests import AuthorizedSession\n",
        "from requests import HTTPError\n",
        "from typing import Any, Optional, Dict\n",
        "\n",
        "def call_google_api(\n",
        "    url: str,\n",
        "    http_verb: str,\n",
        "    request_body: Optional[Dict[str, Any]] = None\n",
        ") -> Dict[str, Any]:\n",
        "    creds, project = google.auth.default(\n",
        "        scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
        "    )\n",
        "    authed_session = AuthorizedSession(creds)\n",
        "    try:\n",
        "        response = authed_session.request(\n",
        "            method=http_verb,\n",
        "            url=url,\n",
        "            json=request_body  # requests handles None for json param gracefully\n",
        "        )\n",
        "\n",
        "        response.raise_for_status()\n",
        "\n",
        "        if response.status_code == 204:\n",
        "            return {}\n",
        "\n",
        "        return response.json()\n",
        "\n",
        "    except HTTPError as e:\n",
        "        # Provide more structured error information\n",
        "        error_message = f\"API call failed with status {e.response.status_code}: {e.response.text}\"\n",
        "        print(error_message) # Or use logging\n",
        "        raise RuntimeError(error_message) from e"
      ],
      "metadata": {
        "id": "6FC8b-trpAPb"
      },
      "id": "6FC8b-trpAPb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create bucket if it does not exist\n",
        "def create_storage_bucket():\n",
        "  storage_client = storage.Client(project=PROJECT_ID)\n",
        "  buckets = storage_client.list_buckets()\n",
        "  bucket_names = [bucket.name for bucket in buckets]\n",
        "\n",
        "  bucket = storage_client.bucket(EXPORT_BUCKET_NAME)\n",
        "\n",
        "  if not bucket.exists():\n",
        "      try:\n",
        "          bucket = storage_client.create_bucket(EXPORT_BUCKET_NAME)\n",
        "          print(f\"Bucket {bucket.name} created.\")\n",
        "      except Exception as e:\n",
        "          print(f\"Error creating bucket: {e}\")\n",
        "  else:\n",
        "      print(f\"Bucket {EXPORT_BUCKET_NAME} already exists.\")\n",
        "\n",
        "create_storage_bucket()"
      ],
      "metadata": {
        "id": "LF7vBAyapImd"
      },
      "id": "LF7vBAyapImd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### This example exports the metadata for a specific entry group or groups ######\n",
        "# request_body = {\n",
        "#   \"type\": \"EXPORT\",\n",
        "#   \"export_spec\": {\n",
        "#     \"output_path\": f\"gs://{EXPORT_BUCKET_NAME}/,\n",
        "#     \"scope\": {\n",
        "#       \"entryGroups\": [\n",
        "#         \"@bigquery\",\n",
        "#         # Additional entry groups\n",
        "#       ],\n",
        "#     },\n",
        "#   }\n",
        "# }\n",
        "\n",
        "##### This example exports the metadata for a project or projects ######\n",
        "# request_body = {\n",
        "#   \"type\": \"EXPORT\",\n",
        "#   \"export_spec\": {\n",
        "#     \"output_path\": f\"gs://{EXPORT_BUCKET_NAME}/\",\n",
        "#     \"scope\": {\n",
        "#       \"projects\": [\n",
        "#         f\"projects/{PROJECT_ID}\"\n",
        "#       ]\n",
        "#     }\n",
        "#   }\n",
        "# }\n",
        "\n",
        "##### This example exports the metadata for your organization ######\n",
        "request_body = {\n",
        "  \"type\": \"EXPORT\",\n",
        "  \"export_spec\": {\n",
        "    \"output_path\": f\"gs://{EXPORT_BUCKET_NAME}/\",\n",
        "    \"scope\": {\n",
        "      \"organizationLevel\": \"true\",\n",
        "    },\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "2b0apTZYpMHe"
      },
      "id": "2b0apTZYpMHe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = f\"https://dataplex.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/metadataJobs\"\n",
        "response = call_google_api(url, \"POST\", request_body)\n",
        "metadata_job_target = response['metadata']['target']\n",
        "pretty_json = json.dumps(response, indent=4, sort_keys=True)\n",
        "print(pretty_json)"
      ],
      "metadata": {
        "id": "96w1wB3ZpklO"
      },
      "id": "96w1wB3ZpklO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The metadata export takes approximately 20-25 minutes to complete.  You can refresh this cell to monitor the progress.  \n",
        "\n",
        "Feel free to move to the next section of the notebook, due to time constraints a complete export is provided for the next section of the lab."
      ],
      "metadata": {
        "id": "IAYg2WGXugih"
      },
      "id": "IAYg2WGXugih"
    },
    {
      "cell_type": "code",
      "source": [
        "status_url = f\"https://dataplex.googleapis.com/v1/{metadata_job_target}\"\n",
        "response = call_google_api(status_url, \"GET\")\n",
        "pretty_json = json.dumps(response, indent=4, sort_keys=True)\n",
        "print(pretty_json)"
      ],
      "metadata": {
        "id": "tIKieMphsfUr"
      },
      "id": "tIKieMphsfUr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyzing Dataplex Metadata in BigQuery\n",
        "\n",
        "We've just exported our Dataplex metadata to GCS. When you want to analyze this metadata in BigQuery, you can create an external table. This lets you query the data directly from its exported location without needing to load or transform it first.\n",
        "\n",
        "### Why a Business Would Import Dataplex Metadata into BigQuery\n",
        "\n",
        "There are several key reasons why a business would want to bring its Dataplex metadata into BigQuery for analysis:\n",
        "\n",
        "* **Advanced Querying and Analysis**: By having the metadata in BigQuery, you can run SQL queries to gain deeper insights.\n",
        "    * *Example*: Count the number of entries by entry group, or find all entries that have a specific aspect (like data quality scores).\n",
        "    ```sql\n",
        "    -- Example: Count entries per entry group\n",
        "    SELECT\n",
        "      entry_group,\n",
        "      COUNT(entry_id) AS number_of_entries\n",
        "    FROM\n",
        "      `your_project.your_dataset.dataplex_metadata_external_table`\n",
        "    GROUP BY\n",
        "      entry_group\n",
        "    ORDER BY\n",
        "      number_of_entries DESC;\n",
        "    ```\n",
        "\n",
        "* **Integration with Analytics Tools**: Importing the metadata to BigQuery allows you to analyze your metadata alongside other business data, or visualize it in tools like Looker Studio.\n",
        "\n",
        "* **Programmatic Processing**: For businesses that need to process large volumes of metadata, exporting it allows for programmatic manipulation using SQL. This processed metadata can then be imported back into Dataplex via API if needed.\n",
        "\n",
        "* **Custom Applications and Third-Party Tools**: You can integrate your metadata into custom-built applications (like a data governance dashboard) or other third-party tools that connect with BigQuery, extending the functionality and use of your metadata."
      ],
      "metadata": {
        "id": "9Rtkx4fgu2e3"
      },
      "id": "9Rtkx4fgu2e3"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "from google.api_core.exceptions import Conflict\n",
        "\n",
        "def create_hive_partitioned_external_table(project_id: str, export_bucket_name: str) -> None:\n",
        "    \"\"\"\n",
        "    Creates a Hive-partitioned external table in BigQuery.\n",
        "\n",
        "    Checks if the dataset exists and creates it if necessary before attempting\n",
        "    to create the table. The table's data is stored in newline-delimited JSON\n",
        "    format in a Google Cloud Storage bucket with a Hive-style directory structure.\n",
        "\n",
        "    Args:\n",
        "        project_id (str): Your Google Cloud project ID.\n",
        "        export_bucket_name (str): The GCS bucket name containing the source data.\n",
        "    \"\"\"\n",
        "    # Set these variables\n",
        "    dataset_id = \"dataplex_metadata\"\n",
        "    table_id = \"metadata_export\"\n",
        "    location = \"US\"\n",
        "\n",
        "\n",
        "    client = bigquery.Client(project=project_id)\n",
        "    dataset_ref = client.dataset(dataset_id)\n",
        "    table_ref = dataset_ref.table(table_id)\n",
        "\n",
        "    #Check for and create the dataset if it doesn't exist\n",
        "    try:\n",
        "        client.get_dataset(dataset_ref)\n",
        "        print(f\"Dataset '{dataset_id}' already exists.\")\n",
        "    except NotFound:\n",
        "        print(f\"Dataset '{dataset_id}' not found. Creating it in location '{location}'.\")\n",
        "        try:\n",
        "            dataset = bigquery.Dataset(dataset_ref)\n",
        "            dataset.location = location\n",
        "            client.create_dataset(dataset, timeout=30)\n",
        "            print(f\"Successfully created dataset '{dataset_id}'.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to create dataset '{dataset_id}': {e}\")\n",
        "            return\n",
        "\n",
        "    # Table schema\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\n",
        "            \"entry\", \"RECORD\", \"NULLABLE\",\n",
        "            fields=[\n",
        "                bigquery.SchemaField(\"name\", \"STRING\", \"NULLABLE\"),\n",
        "                bigquery.SchemaField(\"entryType\", \"STRING\", \"NULLABLE\"),\n",
        "                bigquery.SchemaField(\"createTime\", \"STRING\", \"NULLABLE\"),\n",
        "                bigquery.SchemaField(\"updateTime\", \"STRING\", \"NULLABLE\"),\n",
        "                bigquery.SchemaField(\"aspects\", \"JSON\", \"NULLABLE\"),\n",
        "                bigquery.SchemaField(\"parentEntry\", \"STRING\", \"NULLABLE\"),\n",
        "                bigquery.SchemaField(\"fullyQualifiedName\", \"STRING\", \"NULLABLE\"),\n",
        "                bigquery.SchemaField(\n",
        "                    \"entrySource\", \"RECORD\", \"NULLABLE\",\n",
        "                    fields=[\n",
        "                        bigquery.SchemaField(\"resource\", \"STRING\", \"NULLABLE\"),\n",
        "                        bigquery.SchemaField(\"system\", \"STRING\", \"NULLABLE\"),\n",
        "                        bigquery.SchemaField(\"platform\", \"STRING\", \"NULLABLE\"),\n",
        "                        bigquery.SchemaField(\"displayName\", \"STRING\", \"NULLABLE\"),\n",
        "                        bigquery.SchemaField(\"description\", \"STRING\", \"NULLABLE\"),\n",
        "                        bigquery.SchemaField(\"labels\", \"JSON\", \"NULLABLE\"),\n",
        "                        bigquery.SchemaField(\n",
        "                            \"ancestors\", \"RECORD\", \"REPEATED\",\n",
        "                            fields=[\n",
        "                                bigquery.SchemaField(\"name\", \"STRING\", \"NULLABLE\"),\n",
        "                                bigquery.SchemaField(\"type\", \"STRING\", \"NULLABLE\"),\n",
        "                            ],\n",
        "                        ),\n",
        "                        bigquery.SchemaField(\"createTime\", \"STRING\", \"NULLABLE\"),\n",
        "                        bigquery.SchemaField(\"updateTime\", \"STRING\", \"NULLABLE\"),\n",
        "                        bigquery.SchemaField(\"location\", \"STRING\", \"NULLABLE\"),\n",
        "                    ],\n",
        "                ),\n",
        "            ],\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    external_config = bigquery.ExternalConfig(\"NEWLINE_DELIMITED_JSON\")\n",
        "    gcs_uri = f\"gs://{export_bucket_name}/*\"\n",
        "    external_config.source_uris = [gcs_uri]\n",
        "\n",
        "    hive_partitioning_options = bigquery.HivePartitioningOptions()\n",
        "    hive_partitioning_options.mode = \"AUTO\"\n",
        "    hive_partitioning_options.source_uri_prefix = f\"gs://{export_bucket_name}/\"\n",
        "    external_config.hive_partitioning = hive_partitioning_options\n",
        "\n",
        "    table = bigquery.Table(table_ref, schema=schema)\n",
        "    table.external_data_configuration = external_config\n",
        "\n",
        "    try:\n",
        "        created_table = client.create_table(table)\n",
        "        print(\n",
        "            f\"Successfully created external table: {created_table.project}.{created_table.dataset_id}.{created_table.table_id}\"\n",
        "        )\n",
        "    except Conflict:\n",
        "        print(f\"Table '{table_id}' already exists.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while creating the table: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "create_hive_partitioned_external_table(PROJECT_ID, EXPORT_BUCKET_NAME)"
      ],
      "metadata": {
        "id": "iuaVbJc4Dk2R"
      },
      "id": "iuaVbJc4Dk2R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List the top 10 projects with the highest number of unique entry source resources.\n",
        "%%bigquery\n",
        "SELECT\n",
        "  PROJECT,\n",
        "  COUNT(DISTINCT entry.entrySource.resource) AS unique_resources\n",
        "FROM\n",
        "  `dataplex_metadata.metadata_export`\n",
        "WHERE\n",
        "  year = 2025\n",
        "GROUP BY\n",
        "  PROJECT\n",
        "ORDER BY\n",
        "  unique_resources DESC\n",
        "LIMIT\n",
        "  10;"
      ],
      "metadata": {
        "id": "PjV5lDEK2uYQ"
      },
      "id": "PjV5lDEK2uYQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the unique aspect types and counts\n",
        "\n",
        "%%bigquery\n",
        "CREATE TEMP FUNCTION extract_aspect_info(json_str STRING)\n",
        "RETURNS STRUCT<aspectType STRING, creator STRING, assetType STRING, createTime TIMESTAMP, updateTime TIMESTAMP>\n",
        "LANGUAGE js AS \"\"\"\n",
        "  try {\n",
        "    const obj = JSON.parse(json_str);\n",
        "    const dynamicKey = Object.keys(obj)[0];\n",
        "    if (dynamicKey) {\n",
        "      const aspectData = obj[dynamicKey];\n",
        "      return {\n",
        "        aspectType: aspectData.aspectType,\n",
        "        creator: aspectData.data.creatorIamPrincipal,\n",
        "        assetType: aspectData.data.type,\n",
        "        createTime: new Date(aspectData.createTime),\n",
        "        updateTime: new Date(aspectData.updateTime)\n",
        "      };\n",
        "    }\n",
        "  } catch (e) {\n",
        "    return null;\n",
        "  }\n",
        "  return null;\n",
        "\"\"\";\n",
        "\n",
        "SELECT\n",
        "  (extract_aspect_info(TO_JSON_STRING(entry.aspects))).aspectType AS aspect_type,\n",
        "  COUNT(1) AS count\n",
        "FROM\n",
        "  `bq-sme-governance-build.dataplex_metadata.metadata_export`\n",
        "WHERE (extract_aspect_info(TO_JSON_STRING(entry.aspects))).aspectType IS NOT NULL\n",
        "GROUP BY\n",
        "  aspect_type\n",
        "ORDER BY\n",
        "  count DESC;"
      ],
      "metadata": {
        "id": "f_lvfB8S5KWX"
      },
      "id": "f_lvfB8S5KWX",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "02 - Export Metadata"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}