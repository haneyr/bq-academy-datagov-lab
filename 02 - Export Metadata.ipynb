{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Al6bR1gHAs-I",
   "metadata": {
    "id": "Al6bR1gHAs-I"
   },
   "source": [
    "# Exporting Dataplex Metadata\n",
    "\n",
    "You can run a **metadata export job** to get a snapshot of your Dataplex Universal Catalog metadata (which consists of entries and  aspects) for use in external systems.\n",
    "\n",
    "### Defining the Export Scope\n",
    "\n",
    "Every export job requires a **job scope** to define exactly what metadata to export. You must choose one of the following primary scopes:\n",
    "\n",
    "- `Organization`: Export all metadata belonging to your organization.\n",
    "- `Projects`: Export metadata from one or more specified projects.\n",
    "- `Entry groups`: Export metadata from one or more specified entry groups.\n",
    "\n",
    "You can further refine the scope by specifying the entry types or aspect types to include, ensuring the job only exports the specific entries and aspects you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6olpz1eCqb3u",
   "metadata": {
    "executionInfo": {
     "elapsed": 935,
     "status": "ok",
     "timestamp": 1759994805496,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "6olpz1eCqb3u"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import google.auth\n",
    "from google.api_core.exceptions import Conflict\n",
    "from google.auth.transport.requests import AuthorizedSession\n",
    "from google.cloud import bigquery, storage\n",
    "from google.cloud.exceptions import NotFound\n",
    "from requests import HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6036qy0NMWfDG6jPLl4aVqCr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 177,
     "status": "ok",
     "timestamp": 1759994810826,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "6036qy0NMWfDG6jPLl4aVqCr",
    "outputId": "ed2ae67c-1902-42f2-cedc-1f3d6d038c3f",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Configuration loaded:\n",
      "   Project: bq-sme-governance-build\n",
      "   Location: us-central1\n",
      "   Export Bucket: bq-sme-governance-build-lab-data-export\n",
      "   Source Bucket: sme-gov-lab01\n",
      "   BigQuery Dataset: dataplex_metadata\n",
      "   BigQuery Table: metadata_export\n",
      "   Dataset Location: us-central1\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "# @title Metadata Export Configuration { display-mode: \"form\" }\n",
    "\n",
    "# GCP Settings\n",
    "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"your-gcp-project-id\") #@param {type:\"string\"}\n",
    "LOCATION = \"us-central1\" #@param [\"us-central1\", \"us-east1\", \"us-west1\", \"europe-west1\", \"asia-southeast1\"]\n",
    "\n",
    "# Export Settings\n",
    "EXPORT_BUCKET_NAME = f\"{PROJECT_ID}-lab-data-export\" #@param {type:\"string\"}\n",
    "SOURCE_BUCKET_NAME = \"sme-gov-lab01\" #@param {type:\"string\"}\n",
    "\n",
    "# BigQuery Configuration\n",
    "DATASET_ID = \"dataplex_metadata\" #@param {type:\"string\"}\n",
    "TABLE_ID = \"metadata_export\" #@param {type:\"string\"}\n",
    "DATASET_LOCATION = \"us-central1\" #@param [\"US\", \"EU\", \"us-central1\", \"us-east1\", \"europe-west1\"]\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"   Project: {PROJECT_ID}\")\n",
    "print(f\"   Location: {LOCATION}\")\n",
    "print(f\"   Export Bucket: {EXPORT_BUCKET_NAME}\")\n",
    "print(f\"   Source Bucket: {SOURCE_BUCKET_NAME}\")\n",
    "print(f\"   BigQuery Dataset: {DATASET_ID}\")\n",
    "print(f\"   BigQuery Table: {TABLE_ID}\")\n",
    "print(f\"   Dataset Location: {DATASET_LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6FC8b-trpAPb",
   "metadata": {
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1759994813115,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "6FC8b-trpAPb"
   },
   "outputs": [],
   "source": [
    "def validate_config():\n",
    "    \"\"\"Validates that required configuration is set.\"\"\"\n",
    "    if PROJECT_ID == \"your-gcp-project-id\":\n",
    "        raise ValueError(\"Please update the PROJECT_ID variable before running.\")\n",
    "\n",
    "\n",
    "def call_google_api(\n",
    "    url: str,\n",
    "    http_verb: str,\n",
    "    request_body: Optional[Dict[str, Any]] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Makes authenticated API calls to Google Cloud services.\n",
    "\n",
    "    Args:\n",
    "        url: The complete API endpoint URL\n",
    "        http_verb: HTTP method (GET, POST, PUT, DELETE, etc.)\n",
    "        request_body: Optional request payload as a dictionary\n",
    "\n",
    "    Returns:\n",
    "        Response data as a dictionary (empty dict for 204 responses)\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If the API call fails with detailed error information\n",
    "    \"\"\"\n",
    "    creds, project = google.auth.default(\n",
    "        scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "    )\n",
    "    authed_session = AuthorizedSession(creds)\n",
    "\n",
    "    try:\n",
    "        response = authed_session.request(\n",
    "            method=http_verb,\n",
    "            url=url,\n",
    "            json=request_body\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Handle no-content responses\n",
    "        if response.status_code == 204:\n",
    "            return {}\n",
    "\n",
    "        return response.json()\n",
    "\n",
    "    except HTTPError as e:\n",
    "        error_message = (\n",
    "            f\"API call failed with status {e.response.status_code}: \"\n",
    "            f\"{e.response.text}\"\n",
    "        )\n",
    "        print(error_message)\n",
    "        raise RuntimeError(error_message) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "LF7vBAyapImd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1759994816118,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "LF7vBAyapImd",
    "outputId": "e476e742-5fbf-4b75-d0c4-93e53b06a3b6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bucket bq-sme-governance-build-lab-data-export already exists.\n"
     ]
    }
   ],
   "source": [
    "def create_storage_bucket() -> None:\n",
    "    \"\"\"\n",
    "    Creates a GCS bucket for metadata exports if it doesn't already exist (idempotent).\n",
    "\n",
    "    Uses the globally configured PROJECT_ID and EXPORT_BUCKET_NAME variables.\n",
    "    Prints status messages indicating whether the bucket was created or already exists.\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "    try:\n",
    "        bucket = storage_client.get_bucket(EXPORT_BUCKET_NAME)\n",
    "        print(f\"Bucket {EXPORT_BUCKET_NAME} already exists.\")\n",
    "    except NotFound:\n",
    "        try:\n",
    "            bucket = storage_client.create_bucket(\n",
    "                EXPORT_BUCKET_NAME,\n",
    "                location=LOCATION\n",
    "            )\n",
    "            print(f\"Bucket {bucket.name} created in {LOCATION}.\")\n",
    "        except Conflict:\n",
    "            # Handle race condition where bucket was created between get and create\n",
    "            print(f\"Bucket {EXPORT_BUCKET_NAME} already exists.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating bucket: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "create_storage_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b0apTZYpMHe",
   "metadata": {
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1759994819074,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "2b0apTZYpMHe"
   },
   "outputs": [],
   "source": [
    "# Define the export scope - choose one of the following options:\n",
    "\n",
    "# Option 1: Export metadata for specific entry groups\n",
    "# request_body = {\n",
    "#   \"type\": \"EXPORT\",\n",
    "#   \"export_spec\": {\n",
    "#     \"output_path\": f\"gs://{EXPORT_BUCKET_NAME}/\",\n",
    "#     \"scope\": {\n",
    "#       \"entryGroups\": [\n",
    "#         f\"projects/{PROJECT_ID}/locations/{LOCATION}/entryGroups/@bigquery\",\n",
    "#         # Additional entry groups\n",
    "#       ],\n",
    "#     },\n",
    "#   }\n",
    "# }\n",
    "\n",
    "# Option 2: Export metadata for specific projects\n",
    "# request_body = {\n",
    "#     \"type\": \"EXPORT\",\n",
    "#     \"export_spec\": {\n",
    "#         \"output_path\": f\"gs://{EXPORT_BUCKET_NAME}/\",\n",
    "#         \"scope\": {\n",
    "#             \"projects\": [\n",
    "#                 f\"projects/{PROJECT_ID}\"\n",
    "#             ]\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# Option 3: Export metadata for the entire organization (currently active)\n",
    "request_body = {\n",
    "    \"type\": \"EXPORT\",\n",
    "    \"export_spec\": {\n",
    "        \"output_path\": f\"gs://{EXPORT_BUCKET_NAME}/\",\n",
    "        \"scope\": {\n",
    "            \"organizationLevel\": \"true\",\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96w1wB3ZpklO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 662,
     "status": "ok",
     "timestamp": 1759994822206,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "96w1wB3ZpklO",
    "outputId": "f45cc88c-476d-4302-a313-a1f58e1655a0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n",
      "    \"done\": false,\n",
      "    \"metadata\": {\n",
      "        \"@type\": \"type.googleapis.com/google.cloud.dataplex.v1.OperationMetadata\",\n",
      "        \"apiVersion\": \"v1\",\n",
      "        \"createTime\": \"2025-10-09T07:27:01.927528845Z\",\n",
      "        \"requestedCancellation\": false,\n",
      "        \"target\": \"projects/bq-sme-governance-build/locations/us-central1/metadataJobs/metadata-job-029623b7-6093-47e6-b913-1ed8e39222cb\",\n",
      "        \"verb\": \"create\"\n",
      "    },\n",
      "    \"name\": \"projects/bq-sme-governance-build/locations/us-central1/operations/operation-1759994821630-640b4ba2659fc-4113c430-94a8cf93\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create and trigger the metadata export job\n",
    "url = f\"https://dataplex.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/metadataJobs\"\n",
    "response = call_google_api(url, \"POST\", request_body)\n",
    "\n",
    "# Store the job target for status monitoring\n",
    "metadata_job_target = response['metadata']['target']\n",
    "\n",
    "\n",
    "# Display the job creation response\n",
    "pretty_json = json.dumps(response, indent=4, sort_keys=True)\n",
    "print(pretty_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IAYg2WGXugih",
   "metadata": {
    "id": "IAYg2WGXugih"
   },
   "source": [
    "The metadata export takes approximately 20-25 minutes to complete.  You can refresh this cell to monitor the progress.  \n",
    "\n",
    "Feel free to move to the next section of the notebook, due to time constraints a complete export is provided for the next section of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "tIKieMphsfUr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 161,
     "status": "ok",
     "timestamp": 1759994839728,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "tIKieMphsfUr",
    "outputId": "47314559-a391-4f31-95e1-f26474797fe6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n",
      "    \"createTime\": \"2025-10-09T07:27:01.920211659Z\",\n",
      "    \"exportResult\": {},\n",
      "    \"exportSpec\": {\n",
      "        \"outputPath\": \"gs://bq-sme-governance-build-lab-data-export/\",\n",
      "        \"scope\": {\n",
      "            \"organizationLevel\": true\n",
      "        }\n",
      "    },\n",
      "    \"name\": \"projects/bq-sme-governance-build/locations/us-central1/metadataJobs/metadata-job-029623b7-6093-47e6-b913-1ed8e39222cb\",\n",
      "    \"status\": {\n",
      "        \"message\": \"Logs for this MetadataJob can be found at: https://console.cloud.google.com/logs/query;query=resource.type=\\\"dataplex.googleapis.com/MetadataJob\\\"\\nresource.labels.location=\\\"us-central1\\\"\\nresource.labels.metadata_job_id=\\\"metadata-job-029623b7-6093-47e6-b913-1ed8e39222cb\\\";?project=184517388310\\n\",\n",
      "        \"state\": \"QUEUED\"\n",
      "    },\n",
      "    \"type\": \"EXPORT\",\n",
      "    \"uid\": \"7357ae6d-ea09-4ceb-8905-69a40ccdedfb\",\n",
      "    \"updateTime\": \"2025-10-09T07:27:05.750551954Z\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check the status of the metadata export job\n",
    "status_url = f\"https://dataplex.googleapis.com/v1/{metadata_job_target}\"\n",
    "response = call_google_api(status_url, \"GET\")\n",
    "\n",
    "# Display the job status\n",
    "pretty_json = json.dumps(response, indent=4, sort_keys=True)\n",
    "print(pretty_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9Rtkx4fgu2e3",
   "metadata": {
    "id": "9Rtkx4fgu2e3"
   },
   "source": [
    "## Analyzing Dataplex Metadata in BigQuery\n",
    "\n",
    "We've just exported our Dataplex metadata to GCS. When you want to analyze this metadata in BigQuery, you can create an external table. This lets you query the data directly from its exported location without needing to load or transform it first.\n",
    "\n",
    "### Why a Business Would Import Dataplex Metadata into BigQuery\n",
    "\n",
    "There are several key reasons why a business would want to bring its Dataplex metadata into BigQuery for analysis:\n",
    "\n",
    "* **Advanced Querying and Analysis**: By having the metadata in BigQuery, you can run SQL queries to gain deeper insights.\n",
    "    * *Example*: Count the number of entries by entry group, or find all entries that have a specific aspect (like data quality scores).\n",
    "    ```sql\n",
    "    -- Example: Count entries per entry group\n",
    "    SELECT\n",
    "      entry_group,\n",
    "      COUNT(entry_id) AS number_of_entries\n",
    "    FROM\n",
    "      `your_project.your_dataset.dataplex_metadata_external_table`\n",
    "    GROUP BY\n",
    "      entry_group\n",
    "    ORDER BY\n",
    "      number_of_entries DESC;\n",
    "    ```\n",
    "\n",
    "* **Integration with Analytics Tools**: Importing the metadata to BigQuery allows you to analyze your metadata alongside other business data, or visualize it in tools like Looker Studio.\n",
    "\n",
    "* **Programmatic Processing**: For businesses that need to process large volumes of metadata, exporting it allows for programmatic manipulation using SQL. This processed metadata can then be imported back into Dataplex via API if needed.\n",
    "\n",
    "* **Custom Applications and Third-Party Tools**: You can integrate your metadata into custom-built applications (like a data governance dashboard) or other third-party tools that connect with BigQuery, extending the functionality and use of your metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "iuaVbJc4Dk2R",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2523,
     "status": "ok",
     "timestamp": 1759994846926,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "iuaVbJc4Dk2R",
    "outputId": "f384c74d-1901-4cc4-8eb5-83cb5d280cad"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset 'dataplex_metadata' ready (created or already exists).\n",
      "Created new external table: bq-sme-governance-build.dataplex_metadata.metadata_export\n",
      "Created/updated UDF: bq-sme-governance-build.dataplex_metadata.extract_aspect_types\n",
      "Created/updated unnested view: bq-sme-governance-build.dataplex_metadata.vw_metadata_export_unnested\n"
     ]
    }
   ],
   "source": [
    "def create_hive_partitioned_external_table() -> None:\n",
    "    \"\"\"\n",
    "    Creates a Hive-partitioned external table in BigQuery pointing to exported metadata.\n",
    "\n",
    "    This function is idempotent - it can be run multiple times safely. If the table\n",
    "    already exists, it will be replaced with the updated configuration.\n",
    "\n",
    "    This function creates a BigQuery external table that reads newline-delimited JSON\n",
    "    files from a GCS bucket. The table uses Hive-style partitioning for efficient\n",
    "    querying of time-partitioned data.\n",
    "\n",
    "    Uses the SOURCE_BUCKET_NAME from configuration for the pre-existing metadata export.\n",
    "    \"\"\"\n",
    "    dataset_ref = bq_client.dataset(DATASET_ID)\n",
    "    table_ref = dataset_ref.table(TABLE_ID)\n",
    "\n",
    "    # Ensure dataset exists (idempotent)\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "    dataset.location = DATASET_LOCATION\n",
    "    dataset = bq_client.create_dataset(dataset, exists_ok=True)\n",
    "    print(f\"Dataset '{DATASET_ID}' ready (created or already exists).\")\n",
    "\n",
    "    # Define table schema matching Dataplex metadata export format\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\n",
    "            \"entry\", \"RECORD\", \"NULLABLE\",\n",
    "            fields=[\n",
    "                bigquery.SchemaField(\"name\", \"STRING\", \"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"entryType\", \"STRING\", \"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"createTime\", \"STRING\", \"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"updateTime\", \"STRING\", \"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"aspects\", \"JSON\", \"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"parentEntry\", \"STRING\", \"NULLABLE\"),\n",
    "                bigquery.SchemaField(\"fullyQualifiedName\", \"STRING\", \"NULLABLE\"),\n",
    "                bigquery.SchemaField(\n",
    "                    \"entrySource\", \"RECORD\", \"NULLABLE\",\n",
    "                    fields=[\n",
    "                        bigquery.SchemaField(\"resource\", \"STRING\", \"NULLABLE\"),\n",
    "                        bigquery.SchemaField(\"system\", \"STRING\", \"NULLABLE\"),\n",
    "                        bigquery.SchemaField(\"platform\", \"STRING\", \"NULLABLE\"),\n",
    "                        bigquery.SchemaField(\"displayName\", \"STRING\", \"NULLABLE\"),\n",
    "                        bigquery.SchemaField(\"description\", \"STRING\", \"NULLABLE\"),\n",
    "                        bigquery.SchemaField(\"labels\", \"JSON\", \"NULLABLE\"),\n",
    "                        bigquery.SchemaField(\n",
    "                            \"ancestors\", \"RECORD\", \"REPEATED\",\n",
    "                            fields=[\n",
    "                                bigquery.SchemaField(\"name\", \"STRING\", \"NULLABLE\"),\n",
    "                                bigquery.SchemaField(\"type\", \"STRING\", \"NULLABLE\"),\n",
    "                            ],\n",
    "                        ),\n",
    "                        bigquery.SchemaField(\"createTime\", \"STRING\", \"NULLABLE\"),\n",
    "                        bigquery.SchemaField(\"updateTime\", \"STRING\", \"NULLABLE\"),\n",
    "                        bigquery.SchemaField(\"location\", \"STRING\", \"NULLABLE\"),\n",
    "                    ],\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Configure external data source with Hive partitioning\n",
    "    external_config = bigquery.ExternalConfig(\"NEWLINE_DELIMITED_JSON\")\n",
    "    gcs_uri = f\"gs://{SOURCE_BUCKET_NAME}/*\"\n",
    "    external_config.source_uris = [gcs_uri]\n",
    "\n",
    "    hive_partitioning_options = bigquery.HivePartitioningOptions()\n",
    "    hive_partitioning_options.mode = \"AUTO\"\n",
    "    hive_partitioning_options.source_uri_prefix = f\"gs://{SOURCE_BUCKET_NAME}/\"\n",
    "    external_config.hive_partitioning = hive_partitioning_options\n",
    "\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table.external_data_configuration = external_config\n",
    "\n",
    "    # Create or replace the table (idempotent)\n",
    "    try:\n",
    "        # Check if table exists\n",
    "        existing_table = bq_client.get_table(table_ref)\n",
    "        # Table exists, update it\n",
    "        existing_table.schema = schema\n",
    "        existing_table.external_data_configuration = external_config\n",
    "        updated_table = bq_client.update_table(\n",
    "            existing_table,\n",
    "            [\"schema\", \"external_data_configuration\"]\n",
    "        )\n",
    "        print(\n",
    "            f\"Updated existing external table: \"\n",
    "            f\"{updated_table.project}.{updated_table.dataset_id}.{updated_table.table_id}\"\n",
    "        )\n",
    "    except NotFound:\n",
    "        # Table doesn't exist, create it\n",
    "        created_table = bq_client.create_table(table)\n",
    "        print(\n",
    "            f\"Created new external table: \"\n",
    "            f\"{created_table.project}.{created_table.dataset_id}.{created_table.table_id}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def create_aspect_extraction_udf() -> None:\n",
    "    \"\"\"\n",
    "    Creates a persistent UDF to extract aspect information from the nested JSON structure.\n",
    "    This UDF handles the dynamic keys in the aspects JSON.\n",
    "    \"\"\"\n",
    "    udf_sql = f\"\"\"\n",
    "    CREATE OR REPLACE FUNCTION `{PROJECT_ID}.{DATASET_ID}.extract_aspect_types`(aspects_json JSON)\n",
    "    RETURNS STRING\n",
    "    LANGUAGE js AS r\\\"\\\"\\\"\n",
    "      if (!aspects_json) return null;\n",
    "\n",
    "      try {{\n",
    "        const aspectTypes = [];\n",
    "\n",
    "        // Iterate through all keys in the aspects object\n",
    "        for (const aspectId in aspects_json) {{\n",
    "          if (aspects_json.hasOwnProperty(aspectId)) {{\n",
    "            const aspect = aspects_json[aspectId];\n",
    "            if (aspect && aspect.aspectType) {{\n",
    "              aspectTypes.push(aspect.aspectType);\n",
    "            }}\n",
    "          }}\n",
    "        }}\n",
    "\n",
    "        // Return unique, sorted aspect types as comma-separated string\n",
    "        return [...new Set(aspectTypes)].sort().join(', ');\n",
    "      }} catch (e) {{\n",
    "        return null;\n",
    "      }}\n",
    "    \\\"\\\"\\\";\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        bq_client.query(udf_sql).result()\n",
    "        print(f\"Created/updated UDF: {PROJECT_ID}.{DATASET_ID}.extract_aspect_types\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating UDF: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def create_unnested_metadata_view() -> None:\n",
    "    \"\"\"\n",
    "    Creates a view that unnests the metadata export table for easier querying.\n",
    "\n",
    "    The view flattens the nested entry structure and parses aspect JSON to extract\n",
    "    key metadata fields. This makes it easier to query and analyze the metadata\n",
    "    without dealing with complex nested structures.\n",
    "    \"\"\"\n",
    "    view_id = f\"{PROJECT_ID}.{DATASET_ID}.vw_{TABLE_ID}_unnested\"\n",
    "\n",
    "    # Create view SQL that unnests and parses the metadata\n",
    "    view_sql = f\"\"\"\n",
    "    CREATE OR REPLACE VIEW `{view_id}` AS\n",
    "    SELECT\n",
    "      -- Entry identification\n",
    "      entry.name AS entry_name,\n",
    "      entry.entryType AS entry_type,\n",
    "      entry.fullyQualifiedName AS fully_qualified_name,\n",
    "      entry.parentEntry AS parent_entry,\n",
    "\n",
    "      -- Entry metadata (parse timestamps)\n",
    "      PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E*SZ', entry.createTime) AS entry_create_time,\n",
    "      PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E*SZ', entry.updateTime) AS entry_update_time,\n",
    "\n",
    "      -- Entry Source: Resource information\n",
    "      entry.entrySource.resource AS resource,\n",
    "      entry.entrySource.system AS system,\n",
    "      entry.entrySource.platform AS platform,\n",
    "      entry.entrySource.displayName AS display_name,\n",
    "      entry.entrySource.description AS description,\n",
    "      entry.entrySource.location AS resource_location,\n",
    "      PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E*SZ', entry.entrySource.createTime) AS resource_create_time,\n",
    "      PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E*SZ', entry.entrySource.updateTime) AS resource_update_time,\n",
    "\n",
    "      -- Labels (keep as JSON for flexibility)\n",
    "      entry.entrySource.labels AS labels_json,\n",
    "\n",
    "      -- Aspects: Keep full JSON for detailed analysis\n",
    "      entry.aspects AS aspects_json,\n",
    "\n",
    "      -- Extract aspect types using the UDF\n",
    "      `{PROJECT_ID}.{DATASET_ID}.extract_aspect_types`(entry.aspects) AS aspect_types,\n",
    "\n",
    "      -- Ancestor information (unnest the ancestors array)\n",
    "      ancestor.name AS ancestor_name,\n",
    "      ancestor.type AS ancestor_type,\n",
    "\n",
    "      -- Partition columns for efficient filtering\n",
    "      project,\n",
    "      year,\n",
    "      month,\n",
    "      day\n",
    "\n",
    "    FROM\n",
    "      `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n",
    "    LEFT JOIN\n",
    "      UNNEST(entry.entrySource.ancestors) AS ancestor\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        bq_client.query(view_sql).result()\n",
    "        print(f\"Created/updated unnested view: {view_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating view: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Initialize BigQuery client\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Create the external table\n",
    "create_hive_partitioned_external_table()\n",
    "\n",
    "# Create the UDF for aspect extraction\n",
    "create_aspect_extraction_udf()\n",
    "\n",
    "# Create the unnested view\n",
    "create_unnested_metadata_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PjV5lDEK2uYQ",
   "metadata": {
    "id": "PjV5lDEK2uYQ"
   },
   "outputs": [],
   "source": [
    "# Query the unnested view for easier analysis\n",
    "# This view flattens the nested structure making queries simpler\n",
    "\n",
    "query = f\"\"\"\n",
    "-- List the top 10 projects with the most resources\n",
    "SELECT\n",
    "    project,\n",
    "    COUNT(DISTINCT resource) AS unique_resources,\n",
    "    COUNT(DISTINCT entry_name) AS total_entries,\n",
    "    COUNT(DISTINCT entry_type) AS entry_type_count\n",
    "FROM\n",
    "    `{PROJECT_ID}.{DATASET_ID}.vw_{TABLE_ID}_unnested`\n",
    "WHERE\n",
    "    year = EXTRACT(YEAR FROM CURRENT_DATE())\n",
    "GROUP BY\n",
    "    project\n",
    "ORDER BY\n",
    "    unique_resources DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "df = bq_client.query(query).to_dataframe()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f_lvfB8S5KWX",
   "metadata": {
    "id": "f_lvfB8S5KWX"
   },
   "outputs": [],
   "source": [
    "# Analyze aspect types across all entries using the unnested view\n",
    "# The view has already extracted aspect types, making this query much simpler\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    TRIM(aspect_type) AS aspect_type,\n",
    "    COUNT(DISTINCT entry_name) AS entry_count,\n",
    "    COUNT(DISTINCT project) AS project_count,\n",
    "    COUNT(DISTINCT system) AS system_count\n",
    "FROM\n",
    "    `{PROJECT_ID}.{DATASET_ID}.vw_{TABLE_ID}_unnested`,\n",
    "    UNNEST(SPLIT(aspect_types, ', ')) AS aspect_type\n",
    "WHERE\n",
    "    aspect_type IS NOT NULL\n",
    "    AND aspect_type != ''\n",
    "GROUP BY\n",
    "    aspect_type\n",
    "ORDER BY\n",
    "    entry_count DESC;\n",
    "\"\"\"\n",
    "\n",
    "df = bq_client.query(query).to_dataframe()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DugGdczbbvbm",
   "metadata": {
    "id": "DugGdczbbvbm"
   },
   "source": [
    "---\n",
    "\n",
    "## Interesting Questions to Explore in Data Canvas\n",
    "\n",
    "Once your metadata is loaded, open a new Data Canvas and add the view as a data source.  Here are some initial questions you can investigate and visualize:\n",
    "\n",
    "### **Data Governance Dashboard**\n",
    "- **Metadata Coverage**: What percentage of entries lack aspects?\n",
    "- **Freshness Analysis**: Resources that haven't been updated in 12+ months (potential for cleanup)\n",
    "\n",
    "### **Data Estate Insights**\n",
    "- **Growth Trends**: New resources created per month by project/system (line chart)\n",
    "- **Platform Distribution**: Pie chart of resources by platform (BigQuery, GCS, Dataform, etc.)\n",
    "- **Entry Type Breakdown**: Bar chart showing most common entry types across the organization\n",
    "\n",
    "\n",
    "**Note**: The pre-exported dataset does ***not*** contain the data used to create the graphs below; it is a small sub-set of generic data.  Please explore the metadata in your Argolis organization post-export for a richer experience.\n",
    "\n",
    "Some examples:\n",
    "\n",
    "![Entites created by system over time](https://raw.githubusercontent.com/haneyr/bq-academy-datagov-lab/main/media/metadata01.png)\n",
    "\n",
    "\n",
    "![Resources created over time by project](https://raw.githubusercontent.com/haneyr/bq-academy-datagov-lab/main/media/metadata02.png)\n",
    "\n",
    "\n",
    "Try these queries in Data Canvas to get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7l85zpsmqa4",
   "metadata": {
    "id": "7l85zpsmqa4"
   },
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "The following cells will help you clean up resources created by this notebook. Run these only when you're done with the lab and want to remove all created resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wlidhb3i5i",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlidhb3i5i",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1759995180104,
     "user_tz": 240,
     "elapsed": 108552,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "e3c8c68b-878a-4dce-f198-8273024a7260"
   },
   "outputs": [],
   "source": "# Cleanup functions - reuse the existing bq_client created earlier\n\ndef cleanup_all_resources():\n    \"\"\"Cleans up all resources created by this notebook.\"\"\"\n    print(\"Cleaning up resources...\")\n\n    # Delete BigQuery dataset and all contents\n    dataset_id = f\"{PROJECT_ID}.{DATASET_ID}\"\n    try:\n        bq_client.delete_dataset(dataset_id, delete_contents=True, not_found_ok=True)\n        print(f\"Deleted dataset: {dataset_id}\")\n    except Exception as e:\n        print(f\"Error deleting dataset: {e}\")\n\n    # Delete GCS bucket and all contents\n    try:\n        storage_client = storage.Client(project=PROJECT_ID)\n        bucket = storage_client.get_bucket(EXPORT_BUCKET_NAME)\n        \n        # Delete all objects in the bucket using batch deletion\n        blobs = list(bucket.list_blobs())\n        if blobs:\n            print(f\"Deleting {len(blobs)} objects from bucket...\")\n            bucket.delete_blobs(blobs)\n            print(f\"Deleted {len(blobs)} objects\")\n        \n        # Now delete the empty bucket\n        bucket.delete()\n        print(f\"Deleted bucket: {EXPORT_BUCKET_NAME}\")\n    except NotFound:\n        print(f\"Bucket not found: {EXPORT_BUCKET_NAME}\")\n    except Exception as e:\n        print(f\"Error deleting bucket: {e}\")\n\n    print(\"Cleanup complete!\")\n\n\n# Uncomment to delete all resources\n# cleanup_all_resources()"
  }
 ],
 "metadata": {
  "colab": {
   "name": "02 - Export Metadata",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}